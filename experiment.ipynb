{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57f586d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T19:16:52.425339Z",
     "start_time": "2023-03-10T19:16:50.163190Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import torch\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from toolbox  import detach, flatten, tensor\n",
    "from unidimensional import UnidimensionalGUM as GUM,\\\n",
    "                           UnidimensionalHMM as HMM,\\\n",
    "                           UnidimensionalRNN as RNN\n",
    "from unidimensional import get_GUM_from_AB, sample_AB\n",
    "from training import ABTrainingVisitor,               \\\n",
    "                     AggregateTrainingVisitor,        \\\n",
    "                     DefaultTrainingVisitor,          \\\n",
    "                     LossTrainingVisitor,             \\\n",
    "                     TalkativeTrainingVisitor,        \\\n",
    "                     parameters_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f7e8f",
   "metadata": {},
   "source": [
    "# Unknown GUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5c29f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T19:16:52.441384Z",
     "start_time": "2023-03-10T19:16:52.427597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "\t> a     : tensor([[0.3000]])\n",
      "\t> b     : tensor([[0.9000]])\n",
      "\t> c     : tensor([[0.1000]])\n",
      "\t> eta   : tensor([[0.7000]])\n",
      "\t> alpha : tensor([[0.5892]])\n",
      "\t> beta  : tensor([[0.4330]])\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2601]])\n"
     ]
    }
   ],
   "source": [
    "unknown_gum = GUM(\n",
    "    a      = tensor([[.3]]),\n",
    "    b      = tensor([[.9]]),\n",
    "    c      = tensor([[.1]]),\n",
    "    eta_   = flatten(tensor([[.7]])),\n",
    ")\n",
    "\n",
    "print(unknown_gum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff4062",
   "metadata": {},
   "source": [
    "## Observations generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5c887a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T19:16:52.810721Z",
     "start_time": "2023-03-10T19:16:52.444408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([-0.1496]) ; Variance: tensor([1.3113])\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "h, x = unknown_gum.sample(100)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.patch.set_facecolor('xkcd:white')\n",
    "\n",
    "plt.plot(list(map(lambda h: h.numpy().ravel(), h)), color=\"grey\", linestyle=\"dashed\", label=\"h[t]\")\n",
    "plt.plot(list(map(lambda x: x.numpy().ravel(), x)), color=\"C1\", label=\"x[t]\", linewidth=2)\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel(\"Time (t)\", fontsize=20)\n",
    "plt.ylabel(\"Value\"   , fontsize=20)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean:\", torch.mean(torch.cat(x, dim=0), dim=0), \"; Variance:\", torch.var(torch.cat(x, dim=0), dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad46b1f6",
   "metadata": {},
   "source": [
    "## Derivation of the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8c2718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T19:16:52.925626Z",
     "start_time": "2023-03-10T19:16:52.817579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative log-likelihood of generated observations : 1.5800213813781738\n",
      "Negative log-likelihood of a constant serie       : 484.1977233886719\n"
     ]
    }
   ],
   "source": [
    "# the generated suite of observation by the model itself\n",
    "print(\"Negative log-likelihood of generated observations : {0}\".format(\n",
    "    detach(unknown_gum.negative_log_likelihood(x))))\n",
    "\n",
    "# an unlikely suite (constant, equal to 42)\n",
    "print(\"Negative log-likelihood of a constant serie       : {0}\".format(\n",
    "    detach(unknown_gum.negative_log_likelihood([tensor([[42]]) for _ in range(len(x))]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58709a6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T19:18:42.411840Z",
     "start_time": "2023-03-10T19:16:52.928480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 1.385165810585022; Std: 0.024082845076918602\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for _ in range(100):\n",
    "    h, x = unknown_gum.sample(1000)\n",
    "    l.append(unknown_gum.negative_log_likelihood(x).numpy()[0, 0])\n",
    "    \n",
    "print(\"Average loss: {0}; Std: {1}\".format(np.mean(l), np.std(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a10d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T19:45:54.312231Z",
     "start_time": "2023-03-10T19:18:42.413569Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h, x = unknown_gum.sample(1000)\n",
    "\n",
    "X = []\n",
    "\n",
    "n = 100\n",
    "for i in range(0, n+1):\n",
    "    X.append([])\n",
    "    \n",
    "    B = -(i/n * 2 - 1)\n",
    "    for j in range(0, n+1):\n",
    "        A = (j/n * 2 - 1)\n",
    "        \n",
    "        if (A-1)/2 <= B and B <= (A+1)/2:\n",
    "            gum = get_GUM_from_AB(A, B)\n",
    "        \n",
    "            X[-1].append(gum.negative_log_likelihood(x).numpy()[0, 0])\n",
    "        else:\n",
    "            X[-1].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5fe2df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T19:45:54.511382Z",
     "start_time": "2023-03-10T19:45:54.314400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bound=2\n",
    "X_ = np.minimum(np.array(X), bound*np.ones(np.shape(X)))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "fig.patch.set_facecolor('xkcd:white')\n",
    "ax  = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "plt.imshow(X_)\n",
    "ax.plot(\n",
    "    [(1 + unknown_gum.A.detach().numpy()[0, 0]) * n/2 ], \n",
    "    [-(unknown_gum.B.detach().numpy()[0, 0] - 1) * n/2  ],\n",
    "    marker=\"x\", markersize=10, label=\"Unknown generator\", color=\"red\", linestyle=\"None\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "993378e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-11T14:30:10.800803Z",
     "start_time": "2023-03-11T14:30:10.788976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3821148872375488"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_[np.nonzero(X_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f64fd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T14:11:01.751145Z",
     "start_time": "2023-03-07T14:11:01.745787Z"
    }
   },
   "source": [
    "# Parameters estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ada97c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T19:45:54.526363Z",
     "start_time": "2023-03-10T19:45:54.514945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from (A=-0.3592488636516451, B=-0.6741789922303709).\n",
      "\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1000]], requires_grad=True)\n",
      "\t> b     : tensor([[0.9000]], requires_grad=True)\n",
      "\t> c     : tensor([[0.7000]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.3000]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2308]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7570]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7300]]), B=tensor([[0.6543]])\n"
     ]
    }
   ],
   "source": [
    "A, B = sample_AB()\n",
    "print(\"Starting from (A={0}, B={1}).\\n\".format(A, B))\n",
    "\n",
    "gum = get_GUM_from_AB(A, B, requires_grad=True)\n",
    "\n",
    "gum = GUM(\n",
    "    a      = tensor([[.1]]),\n",
    "    b      = tensor([[.9]]),\n",
    "    c      = tensor([[.7]]),\n",
    "    eta_   = tensor([[.3]]),\n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "# gum = HMM(\n",
    "#     a      = tensor([[.1]]),\n",
    "#     b      = tensor([[.9]]),\n",
    "#     eta_   = tensor([[.7]]),\n",
    "#     requires_grad=True\n",
    "# )\n",
    "\n",
    "# gum = RNN(\n",
    "#     a      = tensor([[.1]]),\n",
    "#     b      = tensor([[.9]]),\n",
    "#     c      = tensor([[.7]]),\n",
    "#     requires_grad=True\n",
    "# )\n",
    "\n",
    "print(gum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0014c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T16:06:39.048598Z",
     "start_time": "2023-03-10T16:06:39.043687Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN: expected B=tensor([[-0.0980]]), computed B=tensor([[0.0567]])\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN: expected B={0}, computed B={1}\".format(gum.A * (2 * gum.A**2 - 1), gum.B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c630f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T21:17:00.369541Z",
     "start_time": "2023-03-10T19:45:54.528430Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0995]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8995]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6995]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2995]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2303]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7577]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7287]]), B=tensor([[0.6533]])\n",
      "Loss: 1.5168919563293457\n",
      "\n",
      "> Iteration 2/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0990]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8990]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6990]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2297]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7583]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7274]]), B=tensor([[0.6523]])\n",
      "Loss: 1.6316419839859009\n",
      "\n",
      "> Iteration 3/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0985]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8985]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6985]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2292]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7590]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7261]]), B=tensor([[0.6514]])\n",
      "Loss: 1.4718819856643677\n",
      "\n",
      "> Iteration 4/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0980]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8980]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6980]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7597]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7249]]), B=tensor([[0.6504]])\n",
      "Loss: 1.5768555402755737\n",
      "\n",
      "> Iteration 5/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0976]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8975]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6975]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7603]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7236]]), B=tensor([[0.6494]])\n",
      "Loss: 1.5549135208129883\n",
      "\n",
      "> Iteration 6/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0971]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8970]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6970]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7610]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7223]]), B=tensor([[0.6484]])\n",
      "Loss: 1.571690320968628\n",
      "\n",
      "> Iteration 7/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0966]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8965]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6965]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2271]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7617]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7210]]), B=tensor([[0.6475]])\n",
      "Loss: 1.551574468612671\n",
      "\n",
      "> Iteration 8/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0961]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8960]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6960]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7623]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7197]]), B=tensor([[0.6465]])\n",
      "Loss: 1.5958354473114014\n",
      "\n",
      "> Iteration 9/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0956]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8955]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6955]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2955]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2261]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7630]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7184]]), B=tensor([[0.6455]])\n",
      "Loss: 1.6238981485366821\n",
      "\n",
      "> Iteration 10/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0951]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8950]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6950]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2256]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7637]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7172]]), B=tensor([[0.6445]])\n",
      "Loss: 1.5158251523971558\n",
      "\n",
      "> Iteration 11/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0946]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8945]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6945]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2251]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7643]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7159]]), B=tensor([[0.6436]])\n",
      "Loss: 1.5362211465835571\n",
      "\n",
      "> Iteration 12/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0941]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8940]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6940]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2940]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7650]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7146]]), B=tensor([[0.6426]])\n",
      "Loss: 1.6162267923355103\n",
      "\n",
      "> Iteration 13/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0937]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8935]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6935]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2241]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7656]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7134]]), B=tensor([[0.6416]])\n",
      "Loss: 1.5156550407409668\n",
      "\n",
      "> Iteration 14/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0932]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8930]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6930]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2931]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2236]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7663]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7121]]), B=tensor([[0.6407]])\n",
      "Loss: 1.5752959251403809\n",
      "\n",
      "> Iteration 15/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0927]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8925]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6925]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2926]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2231]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7669]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7108]]), B=tensor([[0.6397]])\n",
      "Loss: 1.556158423423767\n",
      "\n",
      "> Iteration 16/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0922]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8921]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6921]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2226]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7676]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7096]]), B=tensor([[0.6388]])\n",
      "Loss: 1.479100227355957\n",
      "\n",
      "> Iteration 17/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0917]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8916]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6916]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2916]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2221]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7682]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7083]]), B=tensor([[0.6379]])\n",
      "Loss: 1.5154610872268677\n",
      "\n",
      "> Iteration 18/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0913]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8911]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6911]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2911]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2216]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7688]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7071]]), B=tensor([[0.6369]])\n",
      "Loss: 1.5697087049484253\n",
      "\n",
      "> Iteration 19/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0908]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8906]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6906]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2907]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7695]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7058]]), B=tensor([[0.6360]])\n",
      "Loss: 1.556229829788208\n",
      "\n",
      "> Iteration 20/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0903]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8901]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6901]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2902]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7701]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7046]]), B=tensor([[0.6350]])\n",
      "Loss: 1.5002304315567017\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 21/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0898]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8896]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6896]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2897]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7707]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7033]]), B=tensor([[0.6341]])\n",
      "Loss: 1.560874104499817\n",
      "\n",
      "> Iteration 22/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0893]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8892]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6892]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2893]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2197]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7713]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7021]]), B=tensor([[0.6332]])\n",
      "Loss: 1.5036693811416626\n",
      "\n",
      "> Iteration 23/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0888]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8887]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6887]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2888]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2192]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7719]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.7009]]), B=tensor([[0.6323]])\n",
      "Loss: 1.5379780530929565\n",
      "\n",
      "> Iteration 24/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0884]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8882]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6882]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2883]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2187]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7725]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6996]]), B=tensor([[0.6314]])\n",
      "Loss: 1.5223925113677979\n",
      "\n",
      "> Iteration 25/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0879]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8877]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6877]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2879]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7731]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6984]]), B=tensor([[0.6305]])\n",
      "Loss: 1.514571189880371\n",
      "\n",
      "> Iteration 26/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0874]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8873]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6873]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2874]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2177]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7737]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6972]]), B=tensor([[0.6296]])\n",
      "Loss: 1.5218141078948975\n",
      "\n",
      "> Iteration 27/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0870]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8868]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6868]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2870]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2173]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7743]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6960]]), B=tensor([[0.6287]])\n",
      "Loss: 1.4988679885864258\n",
      "\n",
      "> Iteration 28/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0865]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8863]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6863]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2865]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2168]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7749]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6948]]), B=tensor([[0.6278]])\n",
      "Loss: 1.4946688413619995\n",
      "\n",
      "> Iteration 29/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0861]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8859]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6859]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2861]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7755]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6936]]), B=tensor([[0.6269]])\n",
      "Loss: 1.548306941986084\n",
      "\n",
      "> Iteration 30/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0856]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8854]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6854]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2856]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7761]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6925]]), B=tensor([[0.6260]])\n",
      "Loss: 1.5697628259658813\n",
      "\n",
      "> Iteration 31/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0852]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8849]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6849]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2852]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7767]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6913]]), B=tensor([[0.6251]])\n",
      "Loss: 1.5705852508544922\n",
      "\n",
      "> Iteration 32/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0847]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8845]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6844]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2847]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7773]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6901]]), B=tensor([[0.6242]])\n",
      "Loss: 1.5613839626312256\n",
      "\n",
      "> Iteration 33/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0843]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8840]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6840]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2843]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7778]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6889]]), B=tensor([[0.6233]])\n",
      "Loss: 1.5158809423446655\n",
      "\n",
      "> Iteration 34/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0838]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8835]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6835]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2838]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7784]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6877]]), B=tensor([[0.6225]])\n",
      "Loss: 1.5253098011016846\n",
      "\n",
      "> Iteration 35/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0834]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8831]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6830]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2834]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7790]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6866]]), B=tensor([[0.6216]])\n",
      "Loss: 1.5975830554962158\n",
      "\n",
      "> Iteration 36/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0830]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8826]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6825]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2829]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7796]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6854]]), B=tensor([[0.6207]])\n",
      "Loss: 1.5103410482406616\n",
      "\n",
      "> Iteration 37/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0826]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8821]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6821]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2825]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2127]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7802]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6842]]), B=tensor([[0.6198]])\n",
      "Loss: 1.5923856496810913\n",
      "\n",
      "> Iteration 38/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0821]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8816]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6816]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2821]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2123]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7808]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6831]]), B=tensor([[0.6189]])\n",
      "Loss: 1.5458201169967651\n",
      "\n",
      "> Iteration 39/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0817]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8812]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6811]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2816]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7813]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6819]]), B=tensor([[0.6181]])\n",
      "Loss: 1.5521273612976074\n",
      "\n",
      "> Iteration 40/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0813]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8807]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6807]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2812]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2114]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7819]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6808]]), B=tensor([[0.6172]])\n",
      "Loss: 1.4382998943328857\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 41/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0809]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8803]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6802]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2807]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2110]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7825]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6796]]), B=tensor([[0.6163]])\n",
      "Loss: 1.4862509965896606\n",
      "\n",
      "> Iteration 42/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0805]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8798]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6797]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2803]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2105]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7830]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6785]]), B=tensor([[0.6155]])\n",
      "Loss: 1.5617940425872803\n",
      "\n",
      "> Iteration 43/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0801]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8793]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6793]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2799]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2101]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7836]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6774]]), B=tensor([[0.6147]])\n",
      "Loss: 1.548957109451294\n",
      "\n",
      "> Iteration 44/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0797]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8789]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6788]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2795]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2097]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7841]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6763]]), B=tensor([[0.6138]])\n",
      "Loss: 1.5021793842315674\n",
      "\n",
      "> Iteration 45/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0793]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8784]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6784]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2790]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2093]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7847]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6752]]), B=tensor([[0.6130]])\n",
      "Loss: 1.5529427528381348\n",
      "\n",
      "> Iteration 46/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0789]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8780]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6779]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2786]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2088]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7852]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6741]]), B=tensor([[0.6121]])\n",
      "Loss: 1.526545763015747\n",
      "\n",
      "> Iteration 47/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0785]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8775]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6775]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2782]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2084]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7858]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6730]]), B=tensor([[0.6113]])\n",
      "Loss: 1.5096133947372437\n",
      "\n",
      "> Iteration 48/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0781]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8771]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6770]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2778]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2080]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7863]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6719]]), B=tensor([[0.6105]])\n",
      "Loss: 1.531885027885437\n",
      "\n",
      "> Iteration 49/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0777]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8766]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6765]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2774]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2076]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7868]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6708]]), B=tensor([[0.6097]])\n",
      "Loss: 1.5677810907363892\n",
      "\n",
      "> Iteration 50/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0774]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8762]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6761]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2770]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2072]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7874]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6697]]), B=tensor([[0.6088]])\n",
      "Loss: 1.560420036315918\n",
      "\n",
      "> Iteration 51/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0770]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8757]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6756]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2766]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2068]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7879]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6687]]), B=tensor([[0.6080]])\n",
      "Loss: 1.5248730182647705\n",
      "\n",
      "> Iteration 52/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0766]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8753]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6752]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2762]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2063]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7884]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6676]]), B=tensor([[0.6072]])\n",
      "Loss: 1.4910733699798584\n",
      "\n",
      "> Iteration 53/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0762]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8749]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6748]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2758]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2059]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7889]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6665]]), B=tensor([[0.6064]])\n",
      "Loss: 1.4886633157730103\n",
      "\n",
      "> Iteration 54/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0758]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8744]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6743]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2754]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2055]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7894]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6654]]), B=tensor([[0.6056]])\n",
      "Loss: 1.53841233253479\n",
      "\n",
      "> Iteration 55/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0754]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8740]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6739]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2750]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2051]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7899]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6643]]), B=tensor([[0.6048]])\n",
      "Loss: 1.4535548686981201\n",
      "\n",
      "> Iteration 56/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0750]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8736]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6734]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2746]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2047]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7905]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6633]]), B=tensor([[0.6040]])\n",
      "Loss: 1.5498744249343872\n",
      "\n",
      "> Iteration 57/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0746]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8731]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6730]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2742]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2043]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7910]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6623]]), B=tensor([[0.6032]])\n",
      "Loss: 1.4640007019042969\n",
      "\n",
      "> Iteration 58/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0743]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8727]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6726]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2738]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2039]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7915]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6612]]), B=tensor([[0.6024]])\n",
      "Loss: 1.5472999811172485\n",
      "\n",
      "> Iteration 59/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0739]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8723]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6721]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2734]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2035]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7919]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6602]]), B=tensor([[0.6017]])\n",
      "Loss: 1.5023311376571655\n",
      "\n",
      "> Iteration 60/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0736]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8718]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6717]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2731]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2031]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7924]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6592]]), B=tensor([[0.6009]])\n",
      "Loss: 1.490797996520996\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 61/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0733]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8714]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6713]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2727]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2027]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7929]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6582]]), B=tensor([[0.6001]])\n",
      "Loss: 1.566809892654419\n",
      "\n",
      "> Iteration 62/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0729]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8710]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6708]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2723]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2024]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7934]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6572]]), B=tensor([[0.5994]])\n",
      "Loss: 1.5016340017318726\n",
      "\n",
      "> Iteration 63/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0726]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8706]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6704]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2719]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2020]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7939]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6562]]), B=tensor([[0.5986]])\n",
      "Loss: 1.4912420511245728\n",
      "\n",
      "> Iteration 64/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0722]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8702]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6700]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2716]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2016]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7944]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6552]]), B=tensor([[0.5979]])\n",
      "Loss: 1.465072751045227\n",
      "\n",
      "> Iteration 65/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0719]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8697]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6696]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2712]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2012]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7948]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6543]]), B=tensor([[0.5971]])\n",
      "Loss: 1.5293152332305908\n",
      "\n",
      "> Iteration 66/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0716]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8693]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6692]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2709]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2009]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7953]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6533]]), B=tensor([[0.5964]])\n",
      "Loss: 1.430672287940979\n",
      "\n",
      "> Iteration 67/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0712]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8689]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6687]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2705]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2005]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7958]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6523]]), B=tensor([[0.5956]])\n",
      "Loss: 1.4307374954223633\n",
      "\n",
      "> Iteration 68/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0709]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8685]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6683]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2702]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.2001]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7962]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6514]]), B=tensor([[0.5949]])\n",
      "Loss: 1.5250848531723022\n",
      "\n",
      "> Iteration 69/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0706]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8681]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6679]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2698]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1997]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7967]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6504]]), B=tensor([[0.5942]])\n",
      "Loss: 1.5725075006484985\n",
      "\n",
      "> Iteration 70/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0703]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8677]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6675]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2694]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1994]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7971]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6495]]), B=tensor([[0.5935]])\n",
      "Loss: 1.4909037351608276\n",
      "\n",
      "> Iteration 71/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0699]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8673]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6671]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2691]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1990]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7976]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6485]]), B=tensor([[0.5927]])\n",
      "Loss: 1.500008463859558\n",
      "\n",
      "> Iteration 72/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0696]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8669]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6667]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2688]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1986]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7980]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6476]]), B=tensor([[0.5920]])\n",
      "Loss: 1.4822757244110107\n",
      "\n",
      "> Iteration 73/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0693]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8665]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6663]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2684]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1983]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7985]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6466]]), B=tensor([[0.5913]])\n",
      "Loss: 1.5384173393249512\n",
      "\n",
      "> Iteration 74/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0690]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8661]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6659]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2681]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1979]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7989]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6457]]), B=tensor([[0.5906]])\n",
      "Loss: 1.508010745048523\n",
      "\n",
      "> Iteration 75/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0686]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8657]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6655]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2677]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1975]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7994]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6447]]), B=tensor([[0.5898]])\n",
      "Loss: 1.4365755319595337\n",
      "\n",
      "> Iteration 76/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0683]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8653]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6650]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2674]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1972]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.7998]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6437]]), B=tensor([[0.5891]])\n",
      "Loss: 1.488038420677185\n",
      "\n",
      "> Iteration 77/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0679]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8649]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6646]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2671]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1968]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8002]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6428]]), B=tensor([[0.5884]])\n",
      "Loss: 1.5261188745498657\n",
      "\n",
      "> Iteration 78/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0676]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8645]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6642]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2667]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1964]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8006]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6418]]), B=tensor([[0.5877]])\n",
      "Loss: 1.4639676809310913\n",
      "\n",
      "> Iteration 79/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0673]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8641]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6638]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2664]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1960]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8011]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6409]]), B=tensor([[0.5870]])\n",
      "Loss: 1.5153024196624756\n",
      "\n",
      "> Iteration 80/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0669]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8637]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6634]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2661]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1957]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8015]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6400]]), B=tensor([[0.5863]])\n",
      "Loss: 1.5082693099975586\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 81/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0666]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8633]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6630]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2658]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1953]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8019]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6390]]), B=tensor([[0.5856]])\n",
      "Loss: 1.5234699249267578\n",
      "\n",
      "> Iteration 82/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0663]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8629]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6626]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2654]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1949]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8024]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6381]]), B=tensor([[0.5849]])\n",
      "Loss: 1.4595305919647217\n",
      "\n",
      "> Iteration 83/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0660]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8625]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6622]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2651]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1946]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8028]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6372]]), B=tensor([[0.5842]])\n",
      "Loss: 1.5353633165359497\n",
      "\n",
      "> Iteration 84/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0658]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8621]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6618]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2648]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1943]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8032]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6363]]), B=tensor([[0.5835]])\n",
      "Loss: 1.4184812307357788\n",
      "\n",
      "> Iteration 85/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0655]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8617]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6614]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2644]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1939]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8036]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6354]]), B=tensor([[0.5828]])\n",
      "Loss: 1.5326085090637207\n",
      "\n",
      "> Iteration 86/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0652]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8613]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6610]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2641]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1935]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8041]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6345]]), B=tensor([[0.5821]])\n",
      "Loss: 1.5549118518829346\n",
      "\n",
      "> Iteration 87/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0649]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8609]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6606]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2638]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1932]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8045]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6337]]), B=tensor([[0.5814]])\n",
      "Loss: 1.4251788854599\n",
      "\n",
      "> Iteration 88/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0646]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8606]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6602]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2635]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1929]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8049]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6328]]), B=tensor([[0.5808]])\n",
      "Loss: 1.4974762201309204\n",
      "\n",
      "> Iteration 89/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0643]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8602]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6598]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2632]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1925]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8053]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6319]]), B=tensor([[0.5801]])\n",
      "Loss: 1.5519483089447021\n",
      "\n",
      "> Iteration 90/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0641]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8598]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6594]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2628]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1922]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8057]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6310]]), B=tensor([[0.5794]])\n",
      "Loss: 1.4900591373443604\n",
      "\n",
      "> Iteration 91/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0638]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8594]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6590]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2625]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1918]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8061]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6302]]), B=tensor([[0.5787]])\n",
      "Loss: 1.5031912326812744\n",
      "\n",
      "> Iteration 92/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0636]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8590]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6586]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2622]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1915]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8065]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6293]]), B=tensor([[0.5780]])\n",
      "Loss: 1.449461579322815\n",
      "\n",
      "> Iteration 93/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0633]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8586]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6582]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2619]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1912]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8069]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6285]]), B=tensor([[0.5774]])\n",
      "Loss: 1.492152214050293\n",
      "\n",
      "> Iteration 94/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0630]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8582]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6578]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2616]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1908]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8073]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6276]]), B=tensor([[0.5767]])\n",
      "Loss: 1.4710994958877563\n",
      "\n",
      "> Iteration 95/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0628]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8578]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6574]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2613]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1905]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8077]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6267]]), B=tensor([[0.5760]])\n",
      "Loss: 1.5195057392120361\n",
      "\n",
      "> Iteration 96/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0625]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8574]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6570]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2610]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1901]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8081]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6259]]), B=tensor([[0.5754]])\n",
      "Loss: 1.462200403213501\n",
      "\n",
      "> Iteration 97/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0623]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8571]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6566]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2607]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1898]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8085]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6250]]), B=tensor([[0.5747]])\n",
      "Loss: 1.4880222082138062\n",
      "\n",
      "> Iteration 98/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0620]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8567]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6563]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2604]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1895]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8089]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6242]]), B=tensor([[0.5741]])\n",
      "Loss: 1.4672549962997437\n",
      "\n",
      "> Iteration 99/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0617]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8563]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6559]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2601]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1891]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8093]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6234]]), B=tensor([[0.5734]])\n",
      "Loss: 1.4775644540786743\n",
      "\n",
      "> Iteration 100/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0615]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8559]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6555]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2598]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1888]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8097]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6225]]), B=tensor([[0.5728]])\n",
      "Loss: 1.4530110359191895\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 101/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0612]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8556]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6551]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2595]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1885]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8101]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6217]]), B=tensor([[0.5721]])\n",
      "Loss: 1.4820797443389893\n",
      "\n",
      "> Iteration 102/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0609]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8552]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6547]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2592]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1881]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8104]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6208]]), B=tensor([[0.5715]])\n",
      "Loss: 1.4677373170852661\n",
      "\n",
      "> Iteration 103/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0606]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8548]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6544]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2589]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1878]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8108]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6200]]), B=tensor([[0.5709]])\n",
      "Loss: 1.4478641748428345\n",
      "\n",
      "> Iteration 104/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0604]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8545]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6540]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2586]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1875]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8112]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6192]]), B=tensor([[0.5702]])\n",
      "Loss: 1.4829307794570923\n",
      "\n",
      "> Iteration 105/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0601]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8541]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6536]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2584]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1871]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8115]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6183]]), B=tensor([[0.5696]])\n",
      "Loss: 1.5195739269256592\n",
      "\n",
      "> Iteration 106/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0598]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8537]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6532]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2581]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1868]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8119]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6175]]), B=tensor([[0.5689]])\n",
      "Loss: 1.4765777587890625\n",
      "\n",
      "> Iteration 107/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0595]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8534]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6529]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2578]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1864]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8123]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6167]]), B=tensor([[0.5683]])\n",
      "Loss: 1.3988457918167114\n",
      "\n",
      "> Iteration 108/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0592]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8530]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6525]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2575]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1861]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8126]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6158]]), B=tensor([[0.5677]])\n",
      "Loss: 1.467735767364502\n",
      "\n",
      "> Iteration 109/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0590]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8527]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6521]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2573]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1858]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8130]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6150]]), B=tensor([[0.5671]])\n",
      "Loss: 1.417708158493042\n",
      "\n",
      "> Iteration 110/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0587]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8523]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6518]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2570]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1855]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8133]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6142]]), B=tensor([[0.5665]])\n",
      "Loss: 1.4474196434020996\n",
      "\n",
      "> Iteration 111/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0584]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8520]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6514]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2568]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1851]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8136]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6134]]), B=tensor([[0.5659]])\n",
      "Loss: 1.4501652717590332\n",
      "\n",
      "> Iteration 112/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0581]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8516]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6511]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2565]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1848]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8140]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6126]]), B=tensor([[0.5653]])\n",
      "Loss: 1.4605995416641235\n",
      "\n",
      "> Iteration 113/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0579]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8513]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6507]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1845]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8143]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6118]]), B=tensor([[0.5647]])\n",
      "Loss: 1.4968056678771973\n",
      "\n",
      "> Iteration 114/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0576]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8509]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6504]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1842]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8146]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6110]]), B=tensor([[0.5641]])\n",
      "Loss: 1.5030936002731323\n",
      "\n",
      "> Iteration 115/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0573]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8506]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6500]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1838]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6102]]), B=tensor([[0.5635]])\n",
      "Loss: 1.4687262773513794\n",
      "\n",
      "> Iteration 116/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0570]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8502]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6496]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2555]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1835]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8153]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6094]]), B=tensor([[0.5629]])\n",
      "Loss: 1.465579628944397\n",
      "\n",
      "> Iteration 117/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0568]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8499]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6493]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2552]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1832]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8157]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6086]]), B=tensor([[0.5623]])\n",
      "Loss: 1.4262832403182983\n",
      "\n",
      "> Iteration 118/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0565]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8495]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6489]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2550]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1828]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8160]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6078]]), B=tensor([[0.5617]])\n",
      "Loss: 1.4797273874282837\n",
      "\n",
      "> Iteration 119/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0562]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8492]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6486]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2547]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1825]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8163]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6070]]), B=tensor([[0.5611]])\n",
      "Loss: 1.4373842477798462\n",
      "\n",
      "> Iteration 120/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0559]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8489]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6482]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2545]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1822]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8166]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6062]]), B=tensor([[0.5605]])\n",
      "Loss: 1.4919370412826538\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 121/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0557]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8485]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6479]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1819]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8169]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6054]]), B=tensor([[0.5599]])\n",
      "Loss: 1.498649001121521\n",
      "\n",
      "> Iteration 122/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0554]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8482]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6475]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1815]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6046]]), B=tensor([[0.5593]])\n",
      "Loss: 1.49861741065979\n",
      "\n",
      "> Iteration 123/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0551]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8478]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6472]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2538]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1812]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8176]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6038]]), B=tensor([[0.5588]])\n",
      "Loss: 1.4630372524261475\n",
      "\n",
      "> Iteration 124/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0548]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8475]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6468]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2535]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1809]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8179]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6030]]), B=tensor([[0.5582]])\n",
      "Loss: 1.514067530632019\n",
      "\n",
      "> Iteration 125/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0545]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8472]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6465]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2533]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1805]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6022]]), B=tensor([[0.5576]])\n",
      "Loss: 1.4603321552276611\n",
      "\n",
      "> Iteration 126/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0542]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8468]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6461]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1802]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8186]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6014]]), B=tensor([[0.5570]])\n",
      "Loss: 1.4976708889007568\n",
      "\n",
      "> Iteration 127/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0540]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8465]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6457]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2528]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1799]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8189]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.6006]]), B=tensor([[0.5564]])\n",
      "Loss: 1.4856510162353516\n",
      "\n",
      "> Iteration 128/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0537]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8461]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6454]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1795]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8192]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5998]]), B=tensor([[0.5558]])\n",
      "Loss: 1.4683486223220825\n",
      "\n",
      "> Iteration 129/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0534]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8458]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6450]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1792]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8195]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5990]]), B=tensor([[0.5552]])\n",
      "Loss: 1.457616925239563\n",
      "\n",
      "> Iteration 130/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0532]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8454]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6447]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1789]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8198]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5982]]), B=tensor([[0.5546]])\n",
      "Loss: 1.4549967050552368\n",
      "\n",
      "> Iteration 131/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0529]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8451]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6443]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1785]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8201]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5974]]), B=tensor([[0.5540]])\n",
      "Loss: 1.4612921476364136\n",
      "\n",
      "> Iteration 132/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0527]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8448]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6440]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1782]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8205]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5967]]), B=tensor([[0.5535]])\n",
      "Loss: 1.4794621467590332\n",
      "\n",
      "> Iteration 133/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0524]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8444]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6436]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1779]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8208]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5959]]), B=tensor([[0.5529]])\n",
      "Loss: 1.3784888982772827\n",
      "\n",
      "> Iteration 134/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0522]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8441]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6433]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2511]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1776]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8211]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5952]]), B=tensor([[0.5523]])\n",
      "Loss: 1.4694695472717285\n",
      "\n",
      "> Iteration 135/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0520]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8437]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6429]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2509]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1773]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8214]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5944]]), B=tensor([[0.5518]])\n",
      "Loss: 1.4958966970443726\n",
      "\n",
      "> Iteration 136/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0517]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8434]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6426]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2507]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1770]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8217]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5937]]), B=tensor([[0.5512]])\n",
      "Loss: 1.4507079124450684\n",
      "\n",
      "> Iteration 137/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0515]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8431]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6422]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2505]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1767]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8220]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5930]]), B=tensor([[0.5506]])\n",
      "Loss: 1.4306786060333252\n",
      "\n",
      "> Iteration 138/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0513]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8427]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6419]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2502]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1764]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8223]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5922]]), B=tensor([[0.5501]])\n",
      "Loss: 1.4850350618362427\n",
      "\n",
      "> Iteration 139/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0511]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8424]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6416]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2500]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1760]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8226]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5915]]), B=tensor([[0.5495]])\n",
      "Loss: 1.4781603813171387\n",
      "\n",
      "> Iteration 140/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0508]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8421]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6412]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2498]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1757]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8229]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5908]]), B=tensor([[0.5489]])\n",
      "Loss: 1.4247941970825195\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 141/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0506]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8417]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6409]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2495]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1754]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8232]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5901]]), B=tensor([[0.5484]])\n",
      "Loss: 1.4975284337997437\n",
      "\n",
      "> Iteration 142/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0504]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8414]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6405]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2493]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1751]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8235]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5894]]), B=tensor([[0.5478]])\n",
      "Loss: 1.4341199398040771\n",
      "\n",
      "> Iteration 143/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0502]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8411]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6402]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2491]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1748]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8238]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5886]]), B=tensor([[0.5473]])\n",
      "Loss: 1.4058823585510254\n",
      "\n",
      "> Iteration 144/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0500]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8407]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6398]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2489]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1745]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8241]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5879]]), B=tensor([[0.5467]])\n",
      "Loss: 1.4474140405654907\n",
      "\n",
      "> Iteration 145/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0498]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8404]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6395]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2487]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1742]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8244]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5872]]), B=tensor([[0.5462]])\n",
      "Loss: 1.4688307046890259\n",
      "\n",
      "> Iteration 146/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0496]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8401]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6392]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2485]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1739]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8246]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5866]]), B=tensor([[0.5457]])\n",
      "Loss: 1.4030364751815796\n",
      "\n",
      "> Iteration 147/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0494]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8398]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6388]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2482]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1736]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8249]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5859]]), B=tensor([[0.5451]])\n",
      "Loss: 1.4878181219100952\n",
      "\n",
      "> Iteration 148/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0492]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8394]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6385]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2480]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1733]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8252]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5852]]), B=tensor([[0.5446]])\n",
      "Loss: 1.4531902074813843\n",
      "\n",
      "> Iteration 149/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0490]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8391]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6382]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2478]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1730]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8255]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5845]]), B=tensor([[0.5440]])\n",
      "Loss: 1.4956811666488647\n",
      "\n",
      "> Iteration 150/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0488]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8388]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6378]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2476]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1727]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8258]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5838]]), B=tensor([[0.5435]])\n",
      "Loss: 1.4596158266067505\n",
      "\n",
      "> Iteration 151/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0487]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8385]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6375]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2474]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1724]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8261]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5832]]), B=tensor([[0.5430]])\n",
      "Loss: 1.4571294784545898\n",
      "\n",
      "> Iteration 152/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0485]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8381]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6371]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2472]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1721]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8264]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5825]]), B=tensor([[0.5424]])\n",
      "Loss: 1.4958090782165527\n",
      "\n",
      "> Iteration 153/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0483]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8378]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6368]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2470]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1718]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8267]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5818]]), B=tensor([[0.5419]])\n",
      "Loss: 1.464759349822998\n",
      "\n",
      "> Iteration 154/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0481]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8375]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6365]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2467]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1716]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8269]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5811]]), B=tensor([[0.5413]])\n",
      "Loss: 1.4296759366989136\n",
      "\n",
      "> Iteration 155/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0479]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8372]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6361]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2465]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1713]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8272]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5805]]), B=tensor([[0.5408]])\n",
      "Loss: 1.4221128225326538\n",
      "\n",
      "> Iteration 156/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0478]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8368]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6358]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2463]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1710]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8275]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5798]]), B=tensor([[0.5403]])\n",
      "Loss: 1.4594998359680176\n",
      "\n",
      "> Iteration 157/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0476]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8365]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6354]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2461]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1707]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8278]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5792]]), B=tensor([[0.5397]])\n",
      "Loss: 1.5235629081726074\n",
      "\n",
      "> Iteration 158/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0474]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8362]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6351]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2459]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1704]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8281]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5785]]), B=tensor([[0.5392]])\n",
      "Loss: 1.4563043117523193\n",
      "\n",
      "> Iteration 159/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0473]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8358]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6348]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2457]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1701]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8283]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5778]]), B=tensor([[0.5387]])\n",
      "Loss: 1.470478892326355\n",
      "\n",
      "> Iteration 160/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0471]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8355]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6344]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2455]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1698]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8286]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5772]]), B=tensor([[0.5381]])\n",
      "Loss: 1.482824683189392\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 161/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0469]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8352]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6341]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2453]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1695]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8289]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5765]]), B=tensor([[0.5376]])\n",
      "Loss: 1.4952751398086548\n",
      "\n",
      "> Iteration 162/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0468]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8349]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6337]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2451]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1692]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8292]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5758]]), B=tensor([[0.5371]])\n",
      "Loss: 1.4313408136367798\n",
      "\n",
      "> Iteration 163/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0466]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8345]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6334]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2449]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1689]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8295]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5752]]), B=tensor([[0.5365]])\n",
      "Loss: 1.4896459579467773\n",
      "\n",
      "> Iteration 164/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0464]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8342]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6330]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2446]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1686]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8298]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5745]]), B=tensor([[0.5360]])\n",
      "Loss: 1.4574882984161377\n",
      "\n",
      "> Iteration 165/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0463]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8339]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6327]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2444]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1683]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8300]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5739]]), B=tensor([[0.5355]])\n",
      "Loss: 1.4921928644180298\n",
      "\n",
      "> Iteration 166/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0461]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8335]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6324]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2442]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1681]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8303]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5733]]), B=tensor([[0.5349]])\n",
      "Loss: 1.4795341491699219\n",
      "\n",
      "> Iteration 167/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0460]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8332]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6320]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2440]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1678]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8306]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5726]]), B=tensor([[0.5344]])\n",
      "Loss: 1.4238054752349854\n",
      "\n",
      "> Iteration 168/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0459]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8329]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6317]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2438]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1675]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8309]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5720]]), B=tensor([[0.5339]])\n",
      "Loss: 1.3688822984695435\n",
      "\n",
      "> Iteration 169/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0457]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8326]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6314]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2436]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1672]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8311]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5714]]), B=tensor([[0.5334]])\n",
      "Loss: 1.498936414718628\n",
      "\n",
      "> Iteration 170/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0455]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8323]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6310]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2434]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1669]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8314]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5707]]), B=tensor([[0.5329]])\n",
      "Loss: 1.413155436515808\n",
      "\n",
      "> Iteration 171/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0454]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8320]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2432]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1666]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8316]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5701]]), B=tensor([[0.5324]])\n",
      "Loss: 1.4493696689605713\n",
      "\n",
      "> Iteration 172/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0452]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8316]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2431]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1664]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8319]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5695]]), B=tensor([[0.5319]])\n",
      "Loss: 1.4487944841384888\n",
      "\n",
      "> Iteration 173/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0450]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8313]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2429]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1661]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8322]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5688]]), B=tensor([[0.5313]])\n",
      "Loss: 1.5553144216537476\n",
      "\n",
      "> Iteration 174/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0449]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8310]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2427]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1658]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8324]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5682]]), B=tensor([[0.5308]])\n",
      "Loss: 1.4191415309906006\n",
      "\n",
      "> Iteration 175/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0447]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2425]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1655]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8327]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5676]]), B=tensor([[0.5303]])\n",
      "Loss: 1.4806867837905884\n",
      "\n",
      "> Iteration 176/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0446]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2423]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1652]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8329]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5670]]), B=tensor([[0.5298]])\n",
      "Loss: 1.437113642692566\n",
      "\n",
      "> Iteration 177/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0444]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6287]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2421]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1649]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8332]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5663]]), B=tensor([[0.5293]])\n",
      "Loss: 1.4848676919937134\n",
      "\n",
      "> Iteration 178/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0443]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6284]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2419]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1647]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8335]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5657]]), B=tensor([[0.5288]])\n",
      "Loss: 1.4881436824798584\n",
      "\n",
      "> Iteration 179/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0441]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6281]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2417]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1644]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8337]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5651]]), B=tensor([[0.5283]])\n",
      "Loss: 1.4082725048065186\n",
      "\n",
      "> Iteration 180/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0440]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6278]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2415]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1641]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8340]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5645]]), B=tensor([[0.5278]])\n",
      "Loss: 1.4625799655914307\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 181/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0438]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8288]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6274]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2413]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1638]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8342]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5638]]), B=tensor([[0.5273]])\n",
      "Loss: 1.443163275718689\n",
      "\n",
      "> Iteration 182/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0437]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8285]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6271]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2411]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1635]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8345]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5632]]), B=tensor([[0.5268]])\n",
      "Loss: 1.4322954416275024\n",
      "\n",
      "> Iteration 183/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0435]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8282]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6268]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2410]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1633]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8347]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5626]]), B=tensor([[0.5263]])\n",
      "Loss: 1.4411306381225586\n",
      "\n",
      "> Iteration 184/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0434]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8279]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6265]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2408]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1630]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8350]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5620]]), B=tensor([[0.5258]])\n",
      "Loss: 1.4599182605743408\n",
      "\n",
      "> Iteration 185/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0432]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8276]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6262]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2406]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1627]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8352]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5614]]), B=tensor([[0.5253]])\n",
      "Loss: 1.494521141052246\n",
      "\n",
      "> Iteration 186/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0430]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8273]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6258]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2404]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1624]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8355]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5607]]), B=tensor([[0.5248]])\n",
      "Loss: 1.4638276100158691\n",
      "\n",
      "> Iteration 187/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0428]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8269]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6255]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2402]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1621]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8357]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5601]]), B=tensor([[0.5243]])\n",
      "Loss: 1.4670418500900269\n",
      "\n",
      "> Iteration 188/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8266]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6252]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2400]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1618]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8360]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5595]]), B=tensor([[0.5238]])\n",
      "Loss: 1.4374576807022095\n",
      "\n",
      "> Iteration 189/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0425]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8263]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6249]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2399]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1615]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8362]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5589]]), B=tensor([[0.5233]])\n",
      "Loss: 1.4219599962234497\n",
      "\n",
      "> Iteration 190/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0424]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8260]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6245]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2397]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1613]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8365]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5582]]), B=tensor([[0.5228]])\n",
      "Loss: 1.4598957300186157\n",
      "\n",
      "> Iteration 191/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0422]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8257]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6242]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2395]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1610]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8367]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5576]]), B=tensor([[0.5223]])\n",
      "Loss: 1.4458502531051636\n",
      "\n",
      "> Iteration 192/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0420]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8254]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6239]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2393]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1607]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8369]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5570]]), B=tensor([[0.5218]])\n",
      "Loss: 1.4026176929473877\n",
      "\n",
      "> Iteration 193/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0418]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8251]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6236]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2392]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1604]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8371]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5564]]), B=tensor([[0.5214]])\n",
      "Loss: 1.3805791139602661\n",
      "\n",
      "> Iteration 194/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0416]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8249]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6233]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2390]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1601]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8374]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5558]]), B=tensor([[0.5209]])\n",
      "Loss: 1.4370912313461304\n",
      "\n",
      "> Iteration 195/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0415]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8246]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6230]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2389]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1599]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5552]]), B=tensor([[0.5204]])\n",
      "Loss: 1.4413083791732788\n",
      "\n",
      "> Iteration 196/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0413]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8243]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6227]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2387]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1596]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8378]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5546]]), B=tensor([[0.5200]])\n",
      "Loss: 1.4331440925598145\n",
      "\n",
      "> Iteration 197/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0411]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8240]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6224]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2385]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1593]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5540]]), B=tensor([[0.5195]])\n",
      "Loss: 1.449299931526184\n",
      "\n",
      "> Iteration 198/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0410]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8237]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6221]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2384]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1591]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8383]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5534]]), B=tensor([[0.5191]])\n",
      "Loss: 1.43031907081604\n",
      "\n",
      "> Iteration 199/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0408]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8234]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6218]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2382]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1588]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5528]]), B=tensor([[0.5186]])\n",
      "Loss: 1.3853075504302979\n",
      "\n",
      "> Iteration 200/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0406]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8231]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6215]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2381]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1585]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5522]]), B=tensor([[0.5182]])\n",
      "Loss: 1.4372460842132568\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 201/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0404]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8229]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6212]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2379]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1582]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5516]]), B=tensor([[0.5177]])\n",
      "Loss: 1.4488624334335327\n",
      "\n",
      "> Iteration 202/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0403]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8226]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6209]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2378]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1580]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8391]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5510]]), B=tensor([[0.5173]])\n",
      "Loss: 1.3882840871810913\n",
      "\n",
      "> Iteration 203/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0401]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8223]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6206]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2376]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1577]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5505]]), B=tensor([[0.5168]])\n",
      "Loss: 1.4518725872039795\n",
      "\n",
      "> Iteration 204/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0400]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8220]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6204]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2375]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1574]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5499]]), B=tensor([[0.5164]])\n",
      "Loss: 1.3769347667694092\n",
      "\n",
      "> Iteration 205/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0398]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8218]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6201]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2373]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1572]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5494]]), B=tensor([[0.5159]])\n",
      "Loss: 1.4673694372177124\n",
      "\n",
      "> Iteration 206/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0397]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8215]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6198]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2372]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1569]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5488]]), B=tensor([[0.5155]])\n",
      "Loss: 1.4007127285003662\n",
      "\n",
      "> Iteration 207/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0395]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8212]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6195]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2370]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1567]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5483]]), B=tensor([[0.5150]])\n",
      "Loss: 1.5051298141479492\n",
      "\n",
      "> Iteration 208/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0394]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8209]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6192]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2368]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1564]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5477]]), B=tensor([[0.5146]])\n",
      "Loss: 1.4613056182861328\n",
      "\n",
      "> Iteration 209/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0393]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8206]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6189]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2367]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1562]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8406]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5472]]), B=tensor([[0.5142]])\n",
      "Loss: 1.382083773612976\n",
      "\n",
      "> Iteration 210/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0392]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8204]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6186]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2365]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1559]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8408]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5466]]), B=tensor([[0.5137]])\n",
      "Loss: 1.4987270832061768\n",
      "\n",
      "> Iteration 211/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0391]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8201]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6183]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2364]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1556]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8410]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5461]]), B=tensor([[0.5133]])\n",
      "Loss: 1.4053009748458862\n",
      "\n",
      "> Iteration 212/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0389]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8198]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6180]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2362]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1554]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8412]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5456]]), B=tensor([[0.5128]])\n",
      "Loss: 1.4425513744354248\n",
      "\n",
      "> Iteration 213/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0388]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8195]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6177]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2361]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1551]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8414]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5450]]), B=tensor([[0.5124]])\n",
      "Loss: 1.4144566059112549\n",
      "\n",
      "> Iteration 214/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0387]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8192]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6174]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2359]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1549]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8416]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5445]]), B=tensor([[0.5120]])\n",
      "Loss: 1.440369725227356\n",
      "\n",
      "> Iteration 215/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0386]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8190]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6172]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2358]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1546]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8419]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5440]]), B=tensor([[0.5115]])\n",
      "Loss: 1.4012738466262817\n",
      "\n",
      "> Iteration 216/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0384]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8187]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6169]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2356]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1544]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8421]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5435]]), B=tensor([[0.5111]])\n",
      "Loss: 1.4058332443237305\n",
      "\n",
      "> Iteration 217/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0383]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8184]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6166]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2355]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1541]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8423]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5429]]), B=tensor([[0.5107]])\n",
      "Loss: 1.443217158317566\n",
      "\n",
      "> Iteration 218/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8182]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6163]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2354]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1539]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8425]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5424]]), B=tensor([[0.5102]])\n",
      "Loss: 1.4375910758972168\n",
      "\n",
      "> Iteration 219/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0380]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8179]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6160]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2352]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1536]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8427]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5418]]), B=tensor([[0.5098]])\n",
      "Loss: 1.4673055410385132\n",
      "\n",
      "> Iteration 220/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8176]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6157]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2351]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1533]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8429]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5413]]), B=tensor([[0.5094]])\n",
      "Loss: 1.4097609519958496\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 221/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0377]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8173]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6154]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2349]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1531]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8431]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5407]]), B=tensor([[0.5089]])\n",
      "Loss: 1.5668500661849976\n",
      "\n",
      "> Iteration 222/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0375]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8170]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6151]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2348]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1528]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8433]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5401]]), B=tensor([[0.5085]])\n",
      "Loss: 1.4250531196594238\n",
      "\n",
      "> Iteration 223/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8168]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6148]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2346]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1525]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8435]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5396]]), B=tensor([[0.5080]])\n",
      "Loss: 1.448271632194519\n",
      "\n",
      "> Iteration 224/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0373]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8165]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6145]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2345]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1523]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8437]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5390]]), B=tensor([[0.5076]])\n",
      "Loss: 1.4863842725753784\n",
      "\n",
      "> Iteration 225/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0371]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8162]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6142]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2343]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1520]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8439]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5385]]), B=tensor([[0.5071]])\n",
      "Loss: 1.459812879562378\n",
      "\n",
      "> Iteration 226/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8159]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6139]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2342]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1517]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8441]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5379]]), B=tensor([[0.5067]])\n",
      "Loss: 1.4520655870437622\n",
      "\n",
      "> Iteration 227/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8156]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6136]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2340]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1515]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8443]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5374]]), B=tensor([[0.5062]])\n",
      "Loss: 1.4187194108963013\n",
      "\n",
      "> Iteration 228/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8154]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6133]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2339]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1512]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8445]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5369]]), B=tensor([[0.5058]])\n",
      "Loss: 1.3578121662139893\n",
      "\n",
      "> Iteration 229/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0367]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8151]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6131]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2338]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1510]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8447]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5364]]), B=tensor([[0.5054]])\n",
      "Loss: 1.4383344650268555\n",
      "\n",
      "> Iteration 230/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0366]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8148]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6128]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2336]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1507]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8449]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5359]]), B=tensor([[0.5050]])\n",
      "Loss: 1.4001660346984863\n",
      "\n",
      "> Iteration 231/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0365]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8145]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6125]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2335]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1505]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8451]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5354]]), B=tensor([[0.5045]])\n",
      "Loss: 1.435402274131775\n",
      "\n",
      "> Iteration 232/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0363]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8143]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6122]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2333]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1502]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8453]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5348]]), B=tensor([[0.5041]])\n",
      "Loss: 1.4049257040023804\n",
      "\n",
      "> Iteration 233/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0362]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8140]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6119]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2332]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1500]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8455]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5343]]), B=tensor([[0.5037]])\n",
      "Loss: 1.4289283752441406\n",
      "\n",
      "> Iteration 234/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0361]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8137]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6116]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2331]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1497]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8457]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5338]]), B=tensor([[0.5033]])\n",
      "Loss: 1.4525604248046875\n",
      "\n",
      "> Iteration 235/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0360]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8135]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6114]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2329]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1495]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8459]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5333]]), B=tensor([[0.5029]])\n",
      "Loss: 1.4235684871673584\n",
      "\n",
      "> Iteration 236/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0359]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8132]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6111]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2328]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1492]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8461]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5328]]), B=tensor([[0.5025]])\n",
      "Loss: 1.3830509185791016\n",
      "\n",
      "> Iteration 237/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0358]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8129]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6108]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2327]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1490]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8462]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5323]]), B=tensor([[0.5020]])\n",
      "Loss: 1.3971589803695679\n",
      "\n",
      "> Iteration 238/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0357]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8127]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6105]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2325]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1487]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8464]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5319]]), B=tensor([[0.5016]])\n",
      "Loss: 1.4460359811782837\n",
      "\n",
      "> Iteration 239/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0356]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8124]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6103]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2324]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1485]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8466]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5314]]), B=tensor([[0.5012]])\n",
      "Loss: 1.4257750511169434\n",
      "\n",
      "> Iteration 240/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0355]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8121]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6100]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2323]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1482]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8468]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5309]]), B=tensor([[0.5008]])\n",
      "Loss: 1.4445308446884155\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 241/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0353]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8119]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6097]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2322]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1480]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8470]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5303]]), B=tensor([[0.5004]])\n",
      "Loss: 1.469541311264038\n",
      "\n",
      "> Iteration 242/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0352]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8116]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6094]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2320]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1477]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8472]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5298]]), B=tensor([[0.5000]])\n",
      "Loss: 1.44517982006073\n",
      "\n",
      "> Iteration 243/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0351]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8113]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6091]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2319]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1475]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8474]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5293]]), B=tensor([[0.4996]])\n",
      "Loss: 1.4211028814315796\n",
      "\n",
      "> Iteration 244/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0350]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8111]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6088]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2318]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1472]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8475]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5288]]), B=tensor([[0.4992]])\n",
      "Loss: 1.4534986019134521\n",
      "\n",
      "> Iteration 245/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0349]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8108]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6086]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2316]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1470]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8477]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5283]]), B=tensor([[0.4987]])\n",
      "Loss: 1.4298276901245117\n",
      "\n",
      "> Iteration 246/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0347]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8105]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2315]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1467]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8479]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5278]]), B=tensor([[0.4983]])\n",
      "Loss: 1.4564259052276611\n",
      "\n",
      "> Iteration 247/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0346]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8103]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2314]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1465]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8481]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5273]]), B=tensor([[0.4979]])\n",
      "Loss: 1.4087589979171753\n",
      "\n",
      "> Iteration 248/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0346]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8100]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2312]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1462]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8483]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5268]]), B=tensor([[0.4975]])\n",
      "Loss: 1.4920523166656494\n",
      "\n",
      "> Iteration 249/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0344]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8097]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2311]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1460]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8485]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5263]]), B=tensor([[0.4971]])\n",
      "Loss: 1.4744312763214111\n",
      "\n",
      "> Iteration 250/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8094]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2310]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1457]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8487]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5258]]), B=tensor([[0.4966]])\n",
      "Loss: 1.4374114274978638\n",
      "\n",
      "> Iteration 251/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0342]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8092]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2308]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1455]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8489]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5253]]), B=tensor([[0.4962]])\n",
      "Loss: 1.4571515321731567\n",
      "\n",
      "> Iteration 252/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0342]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8089]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2307]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1452]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8490]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5248]]), B=tensor([[0.4958]])\n",
      "Loss: 1.4418448209762573\n",
      "\n",
      "> Iteration 253/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0341]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8086]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6063]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2306]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1450]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8492]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5243]]), B=tensor([[0.4954]])\n",
      "Loss: 1.3755481243133545\n",
      "\n",
      "> Iteration 254/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0340]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8084]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6060]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2304]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1447]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8494]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5239]]), B=tensor([[0.4950]])\n",
      "Loss: 1.407196044921875\n",
      "\n",
      "> Iteration 255/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0339]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8081]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6057]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2303]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1445]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8496]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5234]]), B=tensor([[0.4946]])\n",
      "Loss: 1.4372055530548096\n",
      "\n",
      "> Iteration 256/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0339]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8078]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6054]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2302]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1443]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8498]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5230]]), B=tensor([[0.4942]])\n",
      "Loss: 1.415174961090088\n",
      "\n",
      "> Iteration 257/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0338]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8076]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6052]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2301]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1440]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8499]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5225]]), B=tensor([[0.4938]])\n",
      "Loss: 1.3646095991134644\n",
      "\n",
      "> Iteration 258/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0337]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8073]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6049]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2300]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1438]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8501]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5221]]), B=tensor([[0.4934]])\n",
      "Loss: 1.4336448907852173\n",
      "\n",
      "> Iteration 259/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0336]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8071]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6046]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2298]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1436]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8503]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5216]]), B=tensor([[0.4930]])\n",
      "Loss: 1.4449653625488281\n",
      "\n",
      "> Iteration 260/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0335]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8068]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6044]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2297]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1433]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8505]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5211]]), B=tensor([[0.4926]])\n",
      "Loss: 1.4215565919876099\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 261/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0334]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8066]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6041]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2296]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1431]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8506]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5207]]), B=tensor([[0.4923]])\n",
      "Loss: 1.4046071767807007\n",
      "\n",
      "> Iteration 262/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0333]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8063]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6038]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2295]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1428]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8508]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5202]]), B=tensor([[0.4919]])\n",
      "Loss: 1.4459832906723022\n",
      "\n",
      "> Iteration 263/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0332]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8061]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6036]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2294]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1426]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8510]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5197]]), B=tensor([[0.4915]])\n",
      "Loss: 1.4286772012710571\n",
      "\n",
      "> Iteration 264/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8058]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6033]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2292]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1424]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8511]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5192]]), B=tensor([[0.4911]])\n",
      "Loss: 1.4526859521865845\n",
      "\n",
      "> Iteration 265/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0329]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8056]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6030]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2291]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1421]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8513]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5187]]), B=tensor([[0.4907]])\n",
      "Loss: 1.388636827468872\n",
      "\n",
      "> Iteration 266/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0328]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8053]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6028]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2290]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1419]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8515]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5182]]), B=tensor([[0.4903]])\n",
      "Loss: 1.3897823095321655\n",
      "\n",
      "> Iteration 267/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0327]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8051]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6025]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2289]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1416]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8516]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5178]]), B=tensor([[0.4899]])\n",
      "Loss: 1.3930524587631226\n",
      "\n",
      "> Iteration 268/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8048]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6023]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2288]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1414]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8518]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5173]]), B=tensor([[0.4896]])\n",
      "Loss: 1.4716119766235352\n",
      "\n",
      "> Iteration 269/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8046]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6020]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2287]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1412]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8520]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5168]]), B=tensor([[0.4892]])\n",
      "Loss: 1.438537359237671\n",
      "\n",
      "> Iteration 270/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8043]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6017]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2286]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1409]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8521]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5164]]), B=tensor([[0.4888]])\n",
      "Loss: 1.4473966360092163\n",
      "\n",
      "> Iteration 271/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8041]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6015]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2285]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1407]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8523]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5160]]), B=tensor([[0.4884]])\n",
      "Loss: 1.449662446975708\n",
      "\n",
      "> Iteration 272/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0322]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8038]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6012]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2283]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1405]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8525]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5155]]), B=tensor([[0.4880]])\n",
      "Loss: 1.4771928787231445\n",
      "\n",
      "> Iteration 273/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0321]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8036]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6010]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2282]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1402]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8526]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5150]]), B=tensor([[0.4877]])\n",
      "Loss: 1.444326400756836\n",
      "\n",
      "> Iteration 274/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0320]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8033]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6007]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2281]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1400]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8528]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5146]]), B=tensor([[0.4873]])\n",
      "Loss: 1.4566928148269653\n",
      "\n",
      "> Iteration 275/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0319]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8031]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6004]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2280]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1397]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8530]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5141]]), B=tensor([[0.4869]])\n",
      "Loss: 1.4382908344268799\n",
      "\n",
      "> Iteration 276/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0318]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8028]], requires_grad=True)\n",
      "\t> c     : tensor([[0.6001]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2279]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1395]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8531]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5136]]), B=tensor([[0.4865]])\n",
      "Loss: 1.4464651346206665\n",
      "\n",
      "> Iteration 277/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0317]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8026]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5999]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2278]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1393]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8533]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5132]]), B=tensor([[0.4861]])\n",
      "Loss: 1.3988380432128906\n",
      "\n",
      "> Iteration 278/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0316]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8023]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5996]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2277]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1390]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8534]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5127]]), B=tensor([[0.4857]])\n",
      "Loss: 1.4417622089385986\n",
      "\n",
      "> Iteration 279/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0315]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8021]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5993]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2275]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1388]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8536]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5123]]), B=tensor([[0.4853]])\n",
      "Loss: 1.4185231924057007\n",
      "\n",
      "> Iteration 280/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0315]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8018]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5991]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2274]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1386]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8538]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5118]]), B=tensor([[0.4850]])\n",
      "Loss: 1.4252269268035889\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 281/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0314]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8016]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5988]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2273]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1383]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8539]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5114]]), B=tensor([[0.4846]])\n",
      "Loss: 1.464805006980896\n",
      "\n",
      "> Iteration 282/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8013]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5986]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2272]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1381]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8541]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5109]]), B=tensor([[0.4842]])\n",
      "Loss: 1.4093172550201416\n",
      "\n",
      "> Iteration 283/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0312]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8011]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5983]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2271]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1379]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8543]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5105]]), B=tensor([[0.4838]])\n",
      "Loss: 1.414210557937622\n",
      "\n",
      "> Iteration 284/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0311]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8008]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5980]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2270]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1376]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8544]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5101]]), B=tensor([[0.4835]])\n",
      "Loss: 1.4552280902862549\n",
      "\n",
      "> Iteration 285/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0311]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8006]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5978]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2269]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1374]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8546]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5096]]), B=tensor([[0.4831]])\n",
      "Loss: 1.4344576597213745\n",
      "\n",
      "> Iteration 286/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0310]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8003]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5975]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2268]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1372]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8547]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5092]]), B=tensor([[0.4827]])\n",
      "Loss: 1.4060475826263428\n",
      "\n",
      "> Iteration 287/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0309]], requires_grad=True)\n",
      "\t> b     : tensor([[0.8001]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5972]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2267]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1369]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8549]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5087]]), B=tensor([[0.4823]])\n",
      "Loss: 1.41841721534729\n",
      "\n",
      "> Iteration 288/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0308]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7998]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5970]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2266]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1367]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8551]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5083]]), B=tensor([[0.4820]])\n",
      "Loss: 1.4189581871032715\n",
      "\n",
      "> Iteration 289/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0307]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7996]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5967]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2265]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1365]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8552]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5079]]), B=tensor([[0.4816]])\n",
      "Loss: 1.4066057205200195\n",
      "\n",
      "> Iteration 290/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0307]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7994]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5965]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2264]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1363]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8554]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5075]]), B=tensor([[0.4812]])\n",
      "Loss: 1.4412873983383179\n",
      "\n",
      "> Iteration 291/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0306]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7991]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5962]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2263]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1360]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8555]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5071]]), B=tensor([[0.4809]])\n",
      "Loss: 1.3543928861618042\n",
      "\n",
      "> Iteration 292/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0305]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7989]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5960]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2261]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1358]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8557]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5066]]), B=tensor([[0.4805]])\n",
      "Loss: 1.4628936052322388\n",
      "\n",
      "> Iteration 293/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0305]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7987]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5957]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2260]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1356]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8558]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5063]]), B=tensor([[0.4802]])\n",
      "Loss: 1.3651375770568848\n",
      "\n",
      "> Iteration 294/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0304]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7984]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5955]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2259]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1354]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8560]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5059]]), B=tensor([[0.4798]])\n",
      "Loss: 1.447608232498169\n",
      "\n",
      "> Iteration 295/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0303]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7982]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5952]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2258]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1352]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8561]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5054]]), B=tensor([[0.4795]])\n",
      "Loss: 1.461307406425476\n",
      "\n",
      "> Iteration 296/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0302]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7979]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5950]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2257]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1349]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8563]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5050]]), B=tensor([[0.4791]])\n",
      "Loss: 1.3997949361801147\n",
      "\n",
      "> Iteration 297/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0302]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7977]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5947]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2256]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1347]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8564]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5046]]), B=tensor([[0.4787]])\n",
      "Loss: 1.4049805402755737\n",
      "\n",
      "> Iteration 298/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0301]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7975]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5945]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2255]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1345]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8566]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5042]]), B=tensor([[0.4784]])\n",
      "Loss: 1.431924819946289\n",
      "\n",
      "> Iteration 299/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0300]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7972]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5942]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2254]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1343]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8567]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5038]]), B=tensor([[0.4780]])\n",
      "Loss: 1.3548650741577148\n",
      "\n",
      "> Iteration 300/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0299]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7970]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5940]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2253]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1341]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8569]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5034]]), B=tensor([[0.4777]])\n",
      "Loss: 1.4192893505096436\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 301/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0299]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7968]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5937]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2252]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1338]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8570]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5030]]), B=tensor([[0.4774]])\n",
      "Loss: 1.4193485975265503\n",
      "\n",
      "> Iteration 302/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0298]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7966]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5935]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2251]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1336]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8571]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5026]]), B=tensor([[0.4770]])\n",
      "Loss: 1.4372851848602295\n",
      "\n",
      "> Iteration 303/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0298]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7963]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5932]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2250]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1334]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8573]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5022]]), B=tensor([[0.4767]])\n",
      "Loss: 1.428502082824707\n",
      "\n",
      "> Iteration 304/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0297]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7961]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5930]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2249]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1332]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8574]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5018]]), B=tensor([[0.4763]])\n",
      "Loss: 1.4395807981491089\n",
      "\n",
      "> Iteration 305/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0297]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7958]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5927]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2248]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1330]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8576]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5014]]), B=tensor([[0.4760]])\n",
      "Loss: 1.4272507429122925\n",
      "\n",
      "> Iteration 306/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0296]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7956]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5925]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2247]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1328]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8577]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5010]]), B=tensor([[0.4756]])\n",
      "Loss: 1.4069623947143555\n",
      "\n",
      "> Iteration 307/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0296]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7954]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5922]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2247]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1326]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8579]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5006]]), B=tensor([[0.4753]])\n",
      "Loss: 1.3842275142669678\n",
      "\n",
      "> Iteration 308/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0296]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7951]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5920]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2246]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1323]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8580]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.5003]]), B=tensor([[0.4749]])\n",
      "Loss: 1.401573896408081\n",
      "\n",
      "> Iteration 309/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7949]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5918]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2245]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1321]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8582]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4999]]), B=tensor([[0.4746]])\n",
      "Loss: 1.374467372894287\n",
      "\n",
      "> Iteration 310/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7947]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5915]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2244]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1319]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8583]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4996]]), B=tensor([[0.4743]])\n",
      "Loss: 1.387392520904541\n",
      "\n",
      "> Iteration 311/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7945]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5913]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2243]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1317]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8584]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4992]]), B=tensor([[0.4739]])\n",
      "Loss: 1.4076118469238281\n",
      "\n",
      "> Iteration 312/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0294]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7943]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5910]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2242]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1315]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8586]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4989]]), B=tensor([[0.4736]])\n",
      "Loss: 1.422019362449646\n",
      "\n",
      "> Iteration 313/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0294]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7940]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5908]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2241]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1313]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8587]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4985]]), B=tensor([[0.4733]])\n",
      "Loss: 1.4581722021102905\n",
      "\n",
      "> Iteration 314/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0293]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7938]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5906]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2240]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1311]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8589]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4981]]), B=tensor([[0.4729]])\n",
      "Loss: 1.4456121921539307\n",
      "\n",
      "> Iteration 315/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0292]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7936]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5903]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2239]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1309]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8590]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4977]]), B=tensor([[0.4726]])\n",
      "Loss: 1.3928676843643188\n",
      "\n",
      "> Iteration 316/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0291]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7933]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5901]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2238]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1307]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8591]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4972]]), B=tensor([[0.4722]])\n",
      "Loss: 1.4518353939056396\n",
      "\n",
      "> Iteration 317/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7931]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5898]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2237]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1305]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8593]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4968]]), B=tensor([[0.4719]])\n",
      "Loss: 1.3902702331542969\n",
      "\n",
      "> Iteration 318/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7929]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5896]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2236]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1303]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8594]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4965]]), B=tensor([[0.4716]])\n",
      "Loss: 1.401067852973938\n",
      "\n",
      "> Iteration 319/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0289]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7927]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5894]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2235]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1300]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8595]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4960]]), B=tensor([[0.4712]])\n",
      "Loss: 1.4557749032974243\n",
      "\n",
      "> Iteration 320/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0288]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7924]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5891]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2234]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1298]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8597]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4956]]), B=tensor([[0.4709]])\n",
      "Loss: 1.4312586784362793\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 321/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0287]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7922]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5889]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2233]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1296]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8598]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4952]]), B=tensor([[0.4705]])\n",
      "Loss: 1.4375267028808594\n",
      "\n",
      "> Iteration 322/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0286]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7920]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5886]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2233]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1294]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8600]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4948]]), B=tensor([[0.4702]])\n",
      "Loss: 1.4064656496047974\n",
      "\n",
      "> Iteration 323/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0285]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7918]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5884]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2232]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1291]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8601]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4944]]), B=tensor([[0.4698]])\n",
      "Loss: 1.4602426290512085\n",
      "\n",
      "> Iteration 324/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0284]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7915]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5881]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2231]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1289]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8603]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4940]]), B=tensor([[0.4695]])\n",
      "Loss: 1.4133152961730957\n",
      "\n",
      "> Iteration 325/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0283]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7913]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5879]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2230]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8604]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4935]]), B=tensor([[0.4691]])\n",
      "Loss: 1.394044041633606\n",
      "\n",
      "> Iteration 326/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0282]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7910]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5876]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2229]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8605]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4931]]), B=tensor([[0.4688]])\n",
      "Loss: 1.4337860345840454\n",
      "\n",
      "> Iteration 327/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0281]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7908]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5874]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2228]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8607]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4926]]), B=tensor([[0.4684]])\n",
      "Loss: 1.4201915264129639\n",
      "\n",
      "> Iteration 328/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0280]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7906]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5871]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2227]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8608]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4922]]), B=tensor([[0.4681]])\n",
      "Loss: 1.436984658241272\n",
      "\n",
      "> Iteration 329/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0279]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7903]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5869]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2226]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8610]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4917]]), B=tensor([[0.4677]])\n",
      "Loss: 1.4220935106277466\n",
      "\n",
      "> Iteration 330/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0278]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7901]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5866]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2225]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8611]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4913]]), B=tensor([[0.4674]])\n",
      "Loss: 1.4263091087341309\n",
      "\n",
      "> Iteration 331/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0277]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7899]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5864]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2224]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8612]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4909]]), B=tensor([[0.4670]])\n",
      "Loss: 1.3827589750289917\n",
      "\n",
      "> Iteration 332/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0276]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7896]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5861]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2223]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1271]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8614]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4904]]), B=tensor([[0.4667]])\n",
      "Loss: 1.463293194770813\n",
      "\n",
      "> Iteration 333/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0275]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7894]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5859]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2222]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8615]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4900]]), B=tensor([[0.4663]])\n",
      "Loss: 1.4025427103042603\n",
      "\n",
      "> Iteration 334/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0274]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7892]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5856]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2221]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8617]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4896]]), B=tensor([[0.4660]])\n",
      "Loss: 1.415828824043274\n",
      "\n",
      "> Iteration 335/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0274]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7889]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5854]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2221]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8618]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4892]]), B=tensor([[0.4656]])\n",
      "Loss: 1.4122158288955688\n",
      "\n",
      "> Iteration 336/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0273]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7887]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5851]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2220]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8619]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4888]]), B=tensor([[0.4653]])\n",
      "Loss: 1.4435757398605347\n",
      "\n",
      "> Iteration 337/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0272]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7885]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5849]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2219]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1260]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8621]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4884]]), B=tensor([[0.4649]])\n",
      "Loss: 1.3878222703933716\n",
      "\n",
      "> Iteration 338/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0271]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7882]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5846]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2218]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1257]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8622]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4880]]), B=tensor([[0.4646]])\n",
      "Loss: 1.432163119316101\n",
      "\n",
      "> Iteration 339/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0270]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7880]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5844]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2217]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1255]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8623]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4875]]), B=tensor([[0.4642]])\n",
      "Loss: 1.4336353540420532\n",
      "\n",
      "> Iteration 340/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0269]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7878]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5841]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2216]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1253]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8625]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4871]]), B=tensor([[0.4639]])\n",
      "Loss: 1.4435079097747803\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 341/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0268]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7875]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5839]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2215]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1250]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8626]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4867]]), B=tensor([[0.4635]])\n",
      "Loss: 1.4081881046295166\n",
      "\n",
      "> Iteration 342/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0267]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7873]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5836]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2214]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1248]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8628]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4862]]), B=tensor([[0.4632]])\n",
      "Loss: 1.4343558549880981\n",
      "\n",
      "> Iteration 343/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0266]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7871]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5834]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2213]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8629]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4858]]), B=tensor([[0.4628]])\n",
      "Loss: 1.4458765983581543\n",
      "\n",
      "> Iteration 344/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0265]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7868]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5831]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2212]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1243]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8630]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4854]]), B=tensor([[0.4625]])\n",
      "Loss: 1.39190673828125\n",
      "\n",
      "> Iteration 345/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0264]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7866]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5829]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2212]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1241]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8632]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4849]]), B=tensor([[0.4621]])\n",
      "Loss: 1.4198002815246582\n",
      "\n",
      "> Iteration 346/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0263]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7864]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5826]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2211]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1239]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8633]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4845]]), B=tensor([[0.4618]])\n",
      "Loss: 1.4334219694137573\n",
      "\n",
      "> Iteration 347/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0262]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7861]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5824]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2210]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1236]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8634]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4841]]), B=tensor([[0.4614]])\n",
      "Loss: 1.3800920248031616\n",
      "\n",
      "> Iteration 348/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0261]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7859]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5822]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2209]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1234]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8636]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4836]]), B=tensor([[0.4611]])\n",
      "Loss: 1.4597760438919067\n",
      "\n",
      "> Iteration 349/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0260]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7857]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5819]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2208]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1232]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8637]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4832]]), B=tensor([[0.4607]])\n",
      "Loss: 1.3970082998275757\n",
      "\n",
      "> Iteration 350/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0258]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7855]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5817]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2207]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1230]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8638]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4827]]), B=tensor([[0.4604]])\n",
      "Loss: 1.4087860584259033\n",
      "\n",
      "> Iteration 351/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0257]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7853]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5814]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2207]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8639]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4823]]), B=tensor([[0.4601]])\n",
      "Loss: 1.3882133960723877\n",
      "\n",
      "> Iteration 352/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0256]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7850]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5812]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2206]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1225]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8641]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4819]]), B=tensor([[0.4598]])\n",
      "Loss: 1.4489657878875732\n",
      "\n",
      "> Iteration 353/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0255]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7848]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5810]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2205]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1223]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8642]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4815]]), B=tensor([[0.4594]])\n",
      "Loss: 1.387323260307312\n",
      "\n",
      "> Iteration 354/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0254]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7846]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5807]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2204]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1221]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8643]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4811]]), B=tensor([[0.4591]])\n",
      "Loss: 1.450414776802063\n",
      "\n",
      "> Iteration 355/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7844]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5805]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2204]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1219]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8644]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4807]]), B=tensor([[0.4588]])\n",
      "Loss: 1.4261759519577026\n",
      "\n",
      "> Iteration 356/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7842]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5803]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2203]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1216]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8646]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4802]]), B=tensor([[0.4584]])\n",
      "Loss: 1.4034252166748047\n",
      "\n",
      "> Iteration 357/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0251]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7839]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5800]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2202]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1214]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8647]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4798]]), B=tensor([[0.4581]])\n",
      "Loss: 1.4370514154434204\n",
      "\n",
      "> Iteration 358/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0250]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7837]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5798]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2201]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8648]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4794]]), B=tensor([[0.4578]])\n",
      "Loss: 1.4296045303344727\n",
      "\n",
      "> Iteration 359/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0249]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7835]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5795]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2200]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8649]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4790]]), B=tensor([[0.4574]])\n",
      "Loss: 1.4237335920333862\n",
      "\n",
      "> Iteration 360/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0248]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7833]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5793]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2200]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1207]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8651]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4786]]), B=tensor([[0.4571]])\n",
      "Loss: 1.3693758249282837\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 361/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0247]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7830]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5791]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2199]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1205]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8652]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4782]]), B=tensor([[0.4568]])\n",
      "Loss: 1.428202748298645\n",
      "\n",
      "> Iteration 362/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0246]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7828]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5788]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2198]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1203]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8653]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4778]]), B=tensor([[0.4564]])\n",
      "Loss: 1.4204776287078857\n",
      "\n",
      "> Iteration 363/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0245]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7826]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5786]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2197]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8654]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4773]]), B=tensor([[0.4561]])\n",
      "Loss: 1.4563047885894775\n",
      "\n",
      "> Iteration 364/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0244]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7824]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5784]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2196]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1198]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8656]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4769]]), B=tensor([[0.4558]])\n",
      "Loss: 1.4000955820083618\n",
      "\n",
      "> Iteration 365/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0243]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7822]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5781]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2196]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1196]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8657]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4765]]), B=tensor([[0.4554]])\n",
      "Loss: 1.415189504623413\n",
      "\n",
      "> Iteration 366/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0242]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7819]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5779]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2195]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1194]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8658]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4761]]), B=tensor([[0.4551]])\n",
      "Loss: 1.3894518613815308\n",
      "\n",
      "> Iteration 367/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7817]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5777]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2194]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1192]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8659]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4757]]), B=tensor([[0.4548]])\n",
      "Loss: 1.4049081802368164\n",
      "\n",
      "> Iteration 368/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7815]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5774]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2193]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1190]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8660]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4753]]), B=tensor([[0.4545]])\n",
      "Loss: 1.375534176826477\n",
      "\n",
      "> Iteration 369/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0240]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7813]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5772]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2193]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1188]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8661]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4750]]), B=tensor([[0.4542]])\n",
      "Loss: 1.4145610332489014\n",
      "\n",
      "> Iteration 370/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0239]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7811]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5770]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2192]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1186]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8663]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4746]]), B=tensor([[0.4539]])\n",
      "Loss: 1.4247369766235352\n",
      "\n",
      "> Iteration 371/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0239]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7809]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5768]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2191]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8664]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4743]]), B=tensor([[0.4536]])\n",
      "Loss: 1.369728684425354\n",
      "\n",
      "> Iteration 372/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0238]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7807]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5765]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2191]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8665]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4739]]), B=tensor([[0.4533]])\n",
      "Loss: 1.3955676555633545\n",
      "\n",
      "> Iteration 373/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7805]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5763]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2190]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8666]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4736]]), B=tensor([[0.4530]])\n",
      "Loss: 1.3816256523132324\n",
      "\n",
      "> Iteration 374/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7803]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5761]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2189]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1178]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8667]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4732]]), B=tensor([[0.4527]])\n",
      "Loss: 1.4022372961044312\n",
      "\n",
      "> Iteration 375/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7801]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5759]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2189]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1176]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8668]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4728]]), B=tensor([[0.4524]])\n",
      "Loss: 1.4369412660598755\n",
      "\n",
      "> Iteration 376/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7799]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5757]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2188]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8669]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4725]]), B=tensor([[0.4521]])\n",
      "Loss: 1.3946577310562134\n",
      "\n",
      "> Iteration 377/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0234]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7797]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5755]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2187]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8670]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4721]]), B=tensor([[0.4518]])\n",
      "Loss: 1.3962081670761108\n",
      "\n",
      "> Iteration 378/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0233]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7795]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5752]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2187]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8671]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4717]]), B=tensor([[0.4515]])\n",
      "Loss: 1.406787633895874\n",
      "\n",
      "> Iteration 379/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0233]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7793]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5750]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2186]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1167]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8673]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4714]]), B=tensor([[0.4512]])\n",
      "Loss: 1.4440702199935913\n",
      "\n",
      "> Iteration 380/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0232]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7791]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5748]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2185]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1165]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8674]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4710]]), B=tensor([[0.4509]])\n",
      "Loss: 1.3874925374984741\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 381/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0231]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7789]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5746]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2185]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8675]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4706]]), B=tensor([[0.4506]])\n",
      "Loss: 1.428080439567566\n",
      "\n",
      "> Iteration 382/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7786]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5744]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2184]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8676]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4703]]), B=tensor([[0.4503]])\n",
      "Loss: 1.3925584554672241\n",
      "\n",
      "> Iteration 383/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0229]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7784]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5741]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2183]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8677]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4699]]), B=tensor([[0.4500]])\n",
      "Loss: 1.4302237033843994\n",
      "\n",
      "> Iteration 384/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0228]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7782]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5739]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2182]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8678]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4694]]), B=tensor([[0.4496]])\n",
      "Loss: 1.427596092224121\n",
      "\n",
      "> Iteration 385/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0227]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7780]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5737]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2182]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8679]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4690]]), B=tensor([[0.4493]])\n",
      "Loss: 1.3980673551559448\n",
      "\n",
      "> Iteration 386/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0226]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7778]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5735]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2181]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8680]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4687]]), B=tensor([[0.4490]])\n",
      "Loss: 1.3647373914718628\n",
      "\n",
      "> Iteration 387/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0225]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7776]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5732]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2180]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8682]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4683]]), B=tensor([[0.4487]])\n",
      "Loss: 1.3726282119750977\n",
      "\n",
      "> Iteration 388/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0224]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7774]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5730]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2180]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8683]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4679]]), B=tensor([[0.4484]])\n",
      "Loss: 1.410705804824829\n",
      "\n",
      "> Iteration 389/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0223]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7772]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5728]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2179]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8684]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4675]]), B=tensor([[0.4481]])\n",
      "Loss: 1.3856161832809448\n",
      "\n",
      "> Iteration 390/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0223]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7770]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5726]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2179]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8685]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4672]]), B=tensor([[0.4478]])\n",
      "Loss: 1.4265170097351074\n",
      "\n",
      "> Iteration 391/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0222]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7768]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5724]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2178]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8686]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4669]]), B=tensor([[0.4476]])\n",
      "Loss: 1.448899269104004\n",
      "\n",
      "> Iteration 392/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0222]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7766]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5722]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2177]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8687]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4665]]), B=tensor([[0.4473]])\n",
      "Loss: 1.4227142333984375\n",
      "\n",
      "> Iteration 393/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0221]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7764]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5720]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2177]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8688]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4662]]), B=tensor([[0.4470]])\n",
      "Loss: 1.447102665901184\n",
      "\n",
      "> Iteration 394/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0220]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7762]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5717]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2176]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8689]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4658]]), B=tensor([[0.4467]])\n",
      "Loss: 1.3923587799072266\n",
      "\n",
      "> Iteration 395/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0219]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7760]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5715]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2175]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8690]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4654]]), B=tensor([[0.4464]])\n",
      "Loss: 1.458173394203186\n",
      "\n",
      "> Iteration 396/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0218]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7758]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5713]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2175]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8691]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4650]]), B=tensor([[0.4461]])\n",
      "Loss: 1.4318716526031494\n",
      "\n",
      "> Iteration 397/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7756]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5711]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2174]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8692]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4646]]), B=tensor([[0.4458]])\n",
      "Loss: 1.4302963018417358\n",
      "\n",
      "> Iteration 398/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0216]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7754]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5708]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2173]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1128]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8693]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4642]]), B=tensor([[0.4454]])\n",
      "Loss: 1.4036426544189453\n",
      "\n",
      "> Iteration 399/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0215]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7752]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5706]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2173]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1126]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8694]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4638]]), B=tensor([[0.4451]])\n",
      "Loss: 1.3841667175292969\n",
      "\n",
      "> Iteration 400/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0214]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7750]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5704]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2172]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1124]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8695]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4635]]), B=tensor([[0.4448]])\n",
      "Loss: 1.4257138967514038\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 401/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0214]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7748]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5702]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2172]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1122]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8696]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4631]]), B=tensor([[0.4446]])\n",
      "Loss: 1.4152566194534302\n",
      "\n",
      "> Iteration 402/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0213]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7746]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5700]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2171]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1119]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8697]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4628]]), B=tensor([[0.4443]])\n",
      "Loss: 1.4014652967453003\n",
      "\n",
      "> Iteration 403/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0212]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7744]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5698]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2170]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8699]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4624]]), B=tensor([[0.4440]])\n",
      "Loss: 1.3415641784667969\n",
      "\n",
      "> Iteration 404/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0212]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7742]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5695]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2170]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1116]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8700]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4621]]), B=tensor([[0.4437]])\n",
      "Loss: 1.4325305223464966\n",
      "\n",
      "> Iteration 405/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0211]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7740]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5693]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2169]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1114]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8701]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4618]]), B=tensor([[0.4434]])\n",
      "Loss: 1.3787015676498413\n",
      "\n",
      "> Iteration 406/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0211]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7738]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5691]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2169]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1112]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8702]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4614]]), B=tensor([[0.4431]])\n",
      "Loss: 1.4484776258468628\n",
      "\n",
      "> Iteration 407/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0210]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7736]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5689]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2168]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1110]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8703]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4611]]), B=tensor([[0.4428]])\n",
      "Loss: 1.3679766654968262\n",
      "\n",
      "> Iteration 408/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0209]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7734]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5687]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2167]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1108]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8704]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4608]]), B=tensor([[0.4425]])\n",
      "Loss: 1.3657596111297607\n",
      "\n",
      "> Iteration 409/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0209]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7732]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5685]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2167]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1106]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8705]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4604]]), B=tensor([[0.4423]])\n",
      "Loss: 1.4281779527664185\n",
      "\n",
      "> Iteration 410/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0208]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7730]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5683]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2166]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1104]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8706]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4601]]), B=tensor([[0.4420]])\n",
      "Loss: 1.387289047241211\n",
      "\n",
      "> Iteration 411/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0207]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7728]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5681]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2166]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1102]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8707]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4597]]), B=tensor([[0.4417]])\n",
      "Loss: 1.4535974264144897\n",
      "\n",
      "> Iteration 412/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0207]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7726]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5679]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2165]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1100]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8708]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4594]]), B=tensor([[0.4414]])\n",
      "Loss: 1.3770798444747925\n",
      "\n",
      "> Iteration 413/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7724]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5677]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2165]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1098]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8709]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4591]]), B=tensor([[0.4411]])\n",
      "Loss: 1.3919237852096558\n",
      "\n",
      "> Iteration 414/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7722]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5675]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2164]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1096]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8710]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4588]]), B=tensor([[0.4409]])\n",
      "Loss: 1.4063023328781128\n",
      "\n",
      "> Iteration 415/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7720]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5673]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2163]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1094]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8710]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4585]]), B=tensor([[0.4406]])\n",
      "Loss: 1.3698110580444336\n",
      "\n",
      "> Iteration 416/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7719]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5671]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2163]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1092]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8711]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4582]]), B=tensor([[0.4403]])\n",
      "Loss: 1.3874878883361816\n",
      "\n",
      "> Iteration 417/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7717]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5669]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2162]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1090]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8712]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4579]]), B=tensor([[0.4401]])\n",
      "Loss: 1.4161025285720825\n",
      "\n",
      "> Iteration 418/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7715]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5667]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2162]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1089]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8713]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4576]]), B=tensor([[0.4398]])\n",
      "Loss: 1.4285290241241455\n",
      "\n",
      "> Iteration 419/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0203]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7713]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5664]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2161]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1087]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8714]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4572]]), B=tensor([[0.4395]])\n",
      "Loss: 1.4213722944259644\n",
      "\n",
      "> Iteration 420/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0203]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7711]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5662]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2161]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1085]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8715]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4569]]), B=tensor([[0.4392]])\n",
      "Loss: 1.4784945249557495\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 421/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0203]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7709]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5660]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2160]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1083]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8716]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4566]]), B=tensor([[0.4389]])\n",
      "Loss: 1.3747360706329346\n",
      "\n",
      "> Iteration 422/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0202]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7707]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5658]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2159]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1081]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8717]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4563]]), B=tensor([[0.4387]])\n",
      "Loss: 1.3867710828781128\n",
      "\n",
      "> Iteration 423/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0202]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7705]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5656]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2159]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1079]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8718]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4560]]), B=tensor([[0.4384]])\n",
      "Loss: 1.3814202547073364\n",
      "\n",
      "> Iteration 424/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0202]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7703]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5654]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2158]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1077]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8719]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4557]]), B=tensor([[0.4381]])\n",
      "Loss: 1.4585256576538086\n",
      "\n",
      "> Iteration 425/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0201]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7701]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5652]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2158]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1075]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8720]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4553]]), B=tensor([[0.4378]])\n",
      "Loss: 1.4012702703475952\n",
      "\n",
      "> Iteration 426/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0201]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7699]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5649]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2157]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1073]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8721]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4550]]), B=tensor([[0.4375]])\n",
      "Loss: 1.3368984460830688\n",
      "\n",
      "> Iteration 427/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0200]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7697]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5647]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2157]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1071]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8722]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4547]]), B=tensor([[0.4372]])\n",
      "Loss: 1.420212984085083\n",
      "\n",
      "> Iteration 428/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0200]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7695]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5645]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2156]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1069]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8723]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4544]]), B=tensor([[0.4370]])\n",
      "Loss: 1.3511155843734741\n",
      "\n",
      "> Iteration 429/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0199]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7694]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5643]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2156]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1067]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8724]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4541]]), B=tensor([[0.4367]])\n",
      "Loss: 1.4073538780212402\n",
      "\n",
      "> Iteration 430/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0198]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7692]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5641]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2155]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1065]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8725]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4537]]), B=tensor([[0.4365]])\n",
      "Loss: 1.3718057870864868\n",
      "\n",
      "> Iteration 431/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0197]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7690]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5640]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2155]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1064]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8726]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4534]]), B=tensor([[0.4362]])\n",
      "Loss: 1.364666223526001\n",
      "\n",
      "> Iteration 432/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0197]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7688]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5638]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2154]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1062]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8727]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4531]]), B=tensor([[0.4359]])\n",
      "Loss: 1.4081997871398926\n",
      "\n",
      "> Iteration 433/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0196]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7686]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5636]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2154]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1060]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8728]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4528]]), B=tensor([[0.4357]])\n",
      "Loss: 1.3668395280838013\n",
      "\n",
      "> Iteration 434/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0195]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7685]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5634]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2153]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1058]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8729]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4525]]), B=tensor([[0.4354]])\n",
      "Loss: 1.4076294898986816\n",
      "\n",
      "> Iteration 435/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0194]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7683]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5632]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2153]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1056]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8729]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4521]]), B=tensor([[0.4352]])\n",
      "Loss: 1.3947356939315796\n",
      "\n",
      "> Iteration 436/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0193]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7681]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5630]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2152]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1054]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8730]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4518]]), B=tensor([[0.4349]])\n",
      "Loss: 1.4462701082229614\n",
      "\n",
      "> Iteration 437/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7679]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5628]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2152]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1052]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8731]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4514]]), B=tensor([[0.4346]])\n",
      "Loss: 1.3870573043823242\n",
      "\n",
      "> Iteration 438/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7677]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5626]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2151]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1050]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8732]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4511]]), B=tensor([[0.4343]])\n",
      "Loss: 1.468088150024414\n",
      "\n",
      "> Iteration 439/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0190]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7675]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5624]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2151]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1048]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8733]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4507]]), B=tensor([[0.4341]])\n",
      "Loss: 1.4060688018798828\n",
      "\n",
      "> Iteration 440/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7674]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5622]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2150]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1046]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8734]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4503]]), B=tensor([[0.4338]])\n",
      "Loss: 1.4227664470672607\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 441/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0188]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7672]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5620]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2150]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1044]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8735]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4499]]), B=tensor([[0.4335]])\n",
      "Loss: 1.430668592453003\n",
      "\n",
      "> Iteration 442/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0187]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7670]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5618]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2149]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1042]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8736]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4496]]), B=tensor([[0.4332]])\n",
      "Loss: 1.406990647315979\n",
      "\n",
      "> Iteration 443/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0186]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7668]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5615]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2149]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1040]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8737]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4492]]), B=tensor([[0.4329]])\n",
      "Loss: 1.4124209880828857\n",
      "\n",
      "> Iteration 444/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0186]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7666]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5613]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2148]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1038]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8738]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4489]]), B=tensor([[0.4326]])\n",
      "Loss: 1.4510586261749268\n",
      "\n",
      "> Iteration 445/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0185]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7664]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5611]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2148]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1036]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8739]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4485]]), B=tensor([[0.4323]])\n",
      "Loss: 1.446423053741455\n",
      "\n",
      "> Iteration 446/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0184]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7662]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5609]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2147]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1034]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8740]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4482]]), B=tensor([[0.4321]])\n",
      "Loss: 1.403638482093811\n",
      "\n",
      "> Iteration 447/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0184]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7660]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5607]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2146]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1032]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8741]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4478]]), B=tensor([[0.4317]])\n",
      "Loss: 1.4648826122283936\n",
      "\n",
      "> Iteration 448/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0183]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7657]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5604]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2146]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1029]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8742]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4475]]), B=tensor([[0.4315]])\n",
      "Loss: 1.3983864784240723\n",
      "\n",
      "> Iteration 449/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0183]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7655]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5602]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2145]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1028]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8743]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4472]]), B=tensor([[0.4312]])\n",
      "Loss: 1.3636975288391113\n",
      "\n",
      "> Iteration 450/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0183]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7653]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5600]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2145]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1025]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8744]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4469]]), B=tensor([[0.4309]])\n",
      "Loss: 1.4100843667984009\n",
      "\n",
      "> Iteration 451/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0182]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7651]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5598]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2144]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1023]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8745]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4465]]), B=tensor([[0.4306]])\n",
      "Loss: 1.474798560142517\n",
      "\n",
      "> Iteration 452/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0182]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7649]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5595]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2144]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1021]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8746]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4462]]), B=tensor([[0.4303]])\n",
      "Loss: 1.4234024286270142\n",
      "\n",
      "> Iteration 453/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7647]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5593]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2143]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1019]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8747]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4458]]), B=tensor([[0.4300]])\n",
      "Loss: 1.410231590270996\n",
      "\n",
      "> Iteration 454/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7645]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5591]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2143]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1017]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8748]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4455]]), B=tensor([[0.4297]])\n",
      "Loss: 1.3893632888793945\n",
      "\n",
      "> Iteration 455/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7643]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5589]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2142]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1015]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8749]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4452]]), B=tensor([[0.4294]])\n",
      "Loss: 1.4274150133132935\n",
      "\n",
      "> Iteration 456/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7641]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5587]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2142]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1013]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8750]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4448]]), B=tensor([[0.4291]])\n",
      "Loss: 1.4183318614959717\n",
      "\n",
      "> Iteration 457/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7639]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5584]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2141]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1011]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8751]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4445]]), B=tensor([[0.4288]])\n",
      "Loss: 1.4374639987945557\n",
      "\n",
      "> Iteration 458/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7637]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5582]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2141]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1009]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8752]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4441]]), B=tensor([[0.4285]])\n",
      "Loss: 1.3802688121795654\n",
      "\n",
      "> Iteration 459/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7635]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5580]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2140]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1007]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8752]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4437]]), B=tensor([[0.4283]])\n",
      "Loss: 1.38288152217865\n",
      "\n",
      "> Iteration 460/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0176]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7633]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5578]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2140]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1005]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8753]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4434]]), B=tensor([[0.4280]])\n",
      "Loss: 1.4122215509414673\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 461/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0176]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7631]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5576]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2139]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1003]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8754]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4431]]), B=tensor([[0.4277]])\n",
      "Loss: 1.4243505001068115\n",
      "\n",
      "> Iteration 462/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0175]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7629]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5574]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2139]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.1001]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8755]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4428]]), B=tensor([[0.4274]])\n",
      "Loss: 1.4310933351516724\n",
      "\n",
      "> Iteration 463/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0175]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7627]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5572]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2138]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0999]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8756]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4424]]), B=tensor([[0.4271]])\n",
      "Loss: 1.4406929016113281\n",
      "\n",
      "> Iteration 464/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0174]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7625]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5569]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2138]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0997]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8757]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4421]]), B=tensor([[0.4268]])\n",
      "Loss: 1.4151980876922607\n",
      "\n",
      "> Iteration 465/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0174]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7623]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5567]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2137]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0994]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8758]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4418]]), B=tensor([[0.4266]])\n",
      "Loss: 1.4134738445281982\n",
      "\n",
      "> Iteration 466/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0173]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7621]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5565]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2137]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0993]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8759]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4415]]), B=tensor([[0.4263]])\n",
      "Loss: 1.378377914428711\n",
      "\n",
      "> Iteration 467/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0173]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7619]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5563]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2136]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0991]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8760]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4411]]), B=tensor([[0.4260]])\n",
      "Loss: 1.4183813333511353\n",
      "\n",
      "> Iteration 468/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0172]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7617]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5561]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2136]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0989]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8761]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4408]]), B=tensor([[0.4257]])\n",
      "Loss: 1.442794919013977\n",
      "\n",
      "> Iteration 469/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0171]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7616]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5559]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2135]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0987]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8762]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4405]]), B=tensor([[0.4255]])\n",
      "Loss: 1.3175560235977173\n",
      "\n",
      "> Iteration 470/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0171]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7614]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5557]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2135]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0985]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8763]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4402]]), B=tensor([[0.4252]])\n",
      "Loss: 1.366466760635376\n",
      "\n",
      "> Iteration 471/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0170]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7612]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5555]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2134]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0983]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8763]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4399]]), B=tensor([[0.4250]])\n",
      "Loss: 1.3862098455429077\n",
      "\n",
      "> Iteration 472/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0169]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7610]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5553]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2134]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0981]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8764]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4395]]), B=tensor([[0.4247]])\n",
      "Loss: 1.4145821332931519\n",
      "\n",
      "> Iteration 473/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0168]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7608]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5551]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2133]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0979]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8765]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4392]]), B=tensor([[0.4244]])\n",
      "Loss: 1.4172067642211914\n",
      "\n",
      "> Iteration 474/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0168]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7607]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5549]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2133]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0977]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8766]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4389]]), B=tensor([[0.4242]])\n",
      "Loss: 1.3751509189605713\n",
      "\n",
      "> Iteration 475/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0167]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7605]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5547]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2133]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0975]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8767]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4386]]), B=tensor([[0.4239]])\n",
      "Loss: 1.4006134271621704\n",
      "\n",
      "> Iteration 476/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0166]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7603]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5545]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2132]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0974]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8767]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4383]]), B=tensor([[0.4237]])\n",
      "Loss: 1.400953769683838\n",
      "\n",
      "> Iteration 477/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0166]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7602]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5544]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2132]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0972]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8768]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4380]]), B=tensor([[0.4234]])\n",
      "Loss: 1.3506423234939575\n",
      "\n",
      "> Iteration 478/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0165]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7600]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5542]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2131]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0970]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8769]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4377]]), B=tensor([[0.4232]])\n",
      "Loss: 1.444076657295227\n",
      "\n",
      "> Iteration 479/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0164]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7598]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5540]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2131]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0968]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8770]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4374]]), B=tensor([[0.4229]])\n",
      "Loss: 1.420042872428894\n",
      "\n",
      "> Iteration 480/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0164]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7596]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5538]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2131]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0966]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8771]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4371]]), B=tensor([[0.4227]])\n",
      "Loss: 1.4413866996765137\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 481/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0163]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7594]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5536]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2130]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0964]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8771]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4368]]), B=tensor([[0.4224]])\n",
      "Loss: 1.4297559261322021\n",
      "\n",
      "> Iteration 482/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0163]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7593]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5534]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2130]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0962]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8772]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4364]]), B=tensor([[0.4222]])\n",
      "Loss: 1.4027162790298462\n",
      "\n",
      "> Iteration 483/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0162]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7591]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5532]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2129]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0961]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8773]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4361]]), B=tensor([[0.4219]])\n",
      "Loss: 1.3934541940689087\n",
      "\n",
      "> Iteration 484/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0162]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7589]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5530]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2129]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0959]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8774]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4359]]), B=tensor([[0.4217]])\n",
      "Loss: 1.3793154954910278\n",
      "\n",
      "> Iteration 485/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0162]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7587]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5528]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2128]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0957]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8775]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4356]]), B=tensor([[0.4214]])\n",
      "Loss: 1.3634674549102783\n",
      "\n",
      "> Iteration 486/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7585]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5526]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2128]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0955]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8776]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4353]]), B=tensor([[0.4212]])\n",
      "Loss: 1.3968617916107178\n",
      "\n",
      "> Iteration 487/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7584]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5524]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2128]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0953]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8776]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4350]]), B=tensor([[0.4209]])\n",
      "Loss: 1.419235110282898\n",
      "\n",
      "> Iteration 488/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7582]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5522]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2127]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0951]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8777]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4347]]), B=tensor([[0.4207]])\n",
      "Loss: 1.4026046991348267\n",
      "\n",
      "> Iteration 489/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7580]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5520]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2127]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0950]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8778]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4345]]), B=tensor([[0.4204]])\n",
      "Loss: 1.382741928100586\n",
      "\n",
      "> Iteration 490/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7578]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5518]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2126]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0948]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8779]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4342]]), B=tensor([[0.4202]])\n",
      "Loss: 1.3761802911758423\n",
      "\n",
      "> Iteration 491/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7577]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5517]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2126]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0946]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8780]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4339]]), B=tensor([[0.4199]])\n",
      "Loss: 1.3793964385986328\n",
      "\n",
      "> Iteration 492/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7575]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5515]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2126]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0944]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8780]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4337]]), B=tensor([[0.4197]])\n",
      "Loss: 1.4395866394042969\n",
      "\n",
      "> Iteration 493/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7573]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5513]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2125]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0943]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8781]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4334]]), B=tensor([[0.4194]])\n",
      "Loss: 1.4354403018951416\n",
      "\n",
      "> Iteration 494/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7571]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5511]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2125]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0941]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8782]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4331]]), B=tensor([[0.4192]])\n",
      "Loss: 1.3661885261535645\n",
      "\n",
      "> Iteration 495/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7570]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5509]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2124]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0939]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8783]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4328]]), B=tensor([[0.4189]])\n",
      "Loss: 1.40206778049469\n",
      "\n",
      "> Iteration 496/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7568]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5507]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2124]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0937]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8784]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4326]]), B=tensor([[0.4187]])\n",
      "Loss: 1.4104273319244385\n",
      "\n",
      "> Iteration 497/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7566]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5505]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2124]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0936]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8784]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4323]]), B=tensor([[0.4185]])\n",
      "Loss: 1.3454149961471558\n",
      "\n",
      "> Iteration 498/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7565]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5503]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2123]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0934]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8785]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4320]]), B=tensor([[0.4182]])\n",
      "Loss: 1.3773454427719116\n",
      "\n",
      "> Iteration 499/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7563]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5502]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2123]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0932]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8786]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4318]]), B=tensor([[0.4180]])\n",
      "Loss: 1.3726506233215332\n",
      "\n",
      "> Iteration 500/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7561]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5500]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2122]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0931]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8786]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4316]]), B=tensor([[0.4178]])\n",
      "Loss: 1.3627690076828003\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 501/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7560]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5498]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2122]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0929]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8787]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4314]]), B=tensor([[0.4176]])\n",
      "Loss: 1.4033117294311523\n",
      "\n",
      "> Iteration 502/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7558]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5496]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2122]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0928]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8788]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4312]]), B=tensor([[0.4173]])\n",
      "Loss: 1.4117668867111206\n",
      "\n",
      "> Iteration 503/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7557]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5495]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2121]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0926]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8789]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4309]]), B=tensor([[0.4171]])\n",
      "Loss: 1.4048835039138794\n",
      "\n",
      "> Iteration 504/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7555]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5493]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2121]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0924]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8789]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4307]]), B=tensor([[0.4169]])\n",
      "Loss: 1.4096730947494507\n",
      "\n",
      "> Iteration 505/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7553]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5491]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2121]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0923]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8790]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4305]]), B=tensor([[0.4167]])\n",
      "Loss: 1.3746248483657837\n",
      "\n",
      "> Iteration 506/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7552]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5489]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2120]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0921]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8791]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4302]]), B=tensor([[0.4164]])\n",
      "Loss: 1.4471073150634766\n",
      "\n",
      "> Iteration 507/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7550]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5488]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2120]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0919]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8792]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4300]]), B=tensor([[0.4162]])\n",
      "Loss: 1.4677785634994507\n",
      "\n",
      "> Iteration 508/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7548]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5486]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2120]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0918]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8792]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4297]]), B=tensor([[0.4160]])\n",
      "Loss: 1.4205776453018188\n",
      "\n",
      "> Iteration 509/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7547]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5484]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2119]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0916]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8793]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4295]]), B=tensor([[0.4157]])\n",
      "Loss: 1.366429090499878\n",
      "\n",
      "> Iteration 510/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7545]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5482]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2119]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0914]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8794]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4292]]), B=tensor([[0.4155]])\n",
      "Loss: 1.3673183917999268\n",
      "\n",
      "> Iteration 511/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7543]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5480]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2118]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0913]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8795]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4290]]), B=tensor([[0.4153]])\n",
      "Loss: 1.3829307556152344\n",
      "\n",
      "> Iteration 512/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7542]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5478]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2118]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0911]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8795]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4287]]), B=tensor([[0.4150]])\n",
      "Loss: 1.4331419467926025\n",
      "\n",
      "> Iteration 513/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0155]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7540]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5477]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2118]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0909]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8796]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4285]]), B=tensor([[0.4148]])\n",
      "Loss: 1.4186497926712036\n",
      "\n",
      "> Iteration 514/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0155]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7538]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5475]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2117]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0908]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8797]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4282]]), B=tensor([[0.4146]])\n",
      "Loss: 1.4263546466827393\n",
      "\n",
      "> Iteration 515/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0155]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7537]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5473]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2117]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0906]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8798]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4280]]), B=tensor([[0.4143]])\n",
      "Loss: 1.3862762451171875\n",
      "\n",
      "> Iteration 516/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0155]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7535]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5471]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2116]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0904]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8798]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4277]]), B=tensor([[0.4141]])\n",
      "Loss: 1.4106031656265259\n",
      "\n",
      "> Iteration 517/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0155]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7533]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5469]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2116]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0902]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8799]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4275]]), B=tensor([[0.4138]])\n",
      "Loss: 1.3819305896759033\n",
      "\n",
      "> Iteration 518/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0154]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7531]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5467]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2116]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0901]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8800]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4272]]), B=tensor([[0.4136]])\n",
      "Loss: 1.4090256690979004\n",
      "\n",
      "> Iteration 519/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0154]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7529]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5465]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2115]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0899]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8801]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4269]]), B=tensor([[0.4133]])\n",
      "Loss: 1.4138754606246948\n",
      "\n",
      "> Iteration 520/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0154]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7528]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5463]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2115]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0897]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8802]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4266]]), B=tensor([[0.4131]])\n",
      "Loss: 1.4181104898452759\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 521/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0154]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7526]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5461]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2115]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0895]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8802]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4264]]), B=tensor([[0.4129]])\n",
      "Loss: 1.3838095664978027\n",
      "\n",
      "> Iteration 522/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0153]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7524]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5460]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2114]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0894]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8803]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4261]]), B=tensor([[0.4126]])\n",
      "Loss: 1.3988151550292969\n",
      "\n",
      "> Iteration 523/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0153]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7523]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5458]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2114]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0892]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8804]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4258]]), B=tensor([[0.4124]])\n",
      "Loss: 1.3784079551696777\n",
      "\n",
      "> Iteration 524/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0152]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7521]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5456]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2114]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0890]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8804]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4256]]), B=tensor([[0.4122]])\n",
      "Loss: 1.3665964603424072\n",
      "\n",
      "> Iteration 525/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0152]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7520]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5454]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2113]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0889]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8805]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4254]]), B=tensor([[0.4120]])\n",
      "Loss: 1.4163129329681396\n",
      "\n",
      "> Iteration 526/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0152]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7518]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5453]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2113]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0887]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8806]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4251]]), B=tensor([[0.4117]])\n",
      "Loss: 1.3508270978927612\n",
      "\n",
      "> Iteration 527/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0152]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7516]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5451]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2112]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0885]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8807]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4249]]), B=tensor([[0.4115]])\n",
      "Loss: 1.4236501455307007\n",
      "\n",
      "> Iteration 528/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7515]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5449]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2112]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0884]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8807]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4246]]), B=tensor([[0.4113]])\n",
      "Loss: 1.398463249206543\n",
      "\n",
      "> Iteration 529/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7513]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5447]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2112]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0882]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8808]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4244]]), B=tensor([[0.4111]])\n",
      "Loss: 1.3519997596740723\n",
      "\n",
      "> Iteration 530/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7512]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5446]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2111]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0880]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8809]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4241]]), B=tensor([[0.4108]])\n",
      "Loss: 1.3749525547027588\n",
      "\n",
      "> Iteration 531/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7510]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5444]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2111]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0879]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8809]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4238]]), B=tensor([[0.4106]])\n",
      "Loss: 1.4012264013290405\n",
      "\n",
      "> Iteration 532/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7508]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5442]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2111]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0877]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8810]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4236]]), B=tensor([[0.4104]])\n",
      "Loss: 1.4298458099365234\n",
      "\n",
      "> Iteration 533/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7507]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5440]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2110]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0876]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8811]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4234]]), B=tensor([[0.4102]])\n",
      "Loss: 1.408074140548706\n",
      "\n",
      "> Iteration 534/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7505]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5439]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2110]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0874]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8811]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4231]]), B=tensor([[0.4100]])\n",
      "Loss: 1.374100923538208\n",
      "\n",
      "> Iteration 535/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7504]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5437]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2110]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0872]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8812]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4229]]), B=tensor([[0.4097]])\n",
      "Loss: 1.4222753047943115\n",
      "\n",
      "> Iteration 536/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7502]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5435]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2109]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0871]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8813]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4226]]), B=tensor([[0.4095]])\n",
      "Loss: 1.396438479423523\n",
      "\n",
      "> Iteration 537/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7500]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5433]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2109]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0869]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8814]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4224]]), B=tensor([[0.4093]])\n",
      "Loss: 1.3998740911483765\n",
      "\n",
      "> Iteration 538/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7499]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5432]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2109]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0867]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8814]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4221]]), B=tensor([[0.4091]])\n",
      "Loss: 1.4566528797149658\n",
      "\n",
      "> Iteration 539/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7497]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5430]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2108]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0866]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8815]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4219]]), B=tensor([[0.4088]])\n",
      "Loss: 1.3981820344924927\n",
      "\n",
      "> Iteration 540/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7495]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5428]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2108]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0864]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8816]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4216]]), B=tensor([[0.4086]])\n",
      "Loss: 1.378239631652832\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 541/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0147]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7494]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5426]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2108]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0862]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8816]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4213]]), B=tensor([[0.4084]])\n",
      "Loss: 1.429809331893921\n",
      "\n",
      "> Iteration 542/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0147]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7492]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5424]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2107]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0860]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8817]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4210]]), B=tensor([[0.4081]])\n",
      "Loss: 1.4346461296081543\n",
      "\n",
      "> Iteration 543/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7490]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5422]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2107]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0859]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8818]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4208]]), B=tensor([[0.4079]])\n",
      "Loss: 1.3722364902496338\n",
      "\n",
      "> Iteration 544/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7489]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5421]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2107]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0857]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8819]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4205]]), B=tensor([[0.4077]])\n",
      "Loss: 1.3714617490768433\n",
      "\n",
      "> Iteration 545/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0145]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7487]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5419]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2106]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0855]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8819]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4203]]), B=tensor([[0.4074]])\n",
      "Loss: 1.4013971090316772\n",
      "\n",
      "> Iteration 546/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0145]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7486]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5417]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2106]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0854]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8820]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4200]]), B=tensor([[0.4072]])\n",
      "Loss: 1.419870376586914\n",
      "\n",
      "> Iteration 547/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0144]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7484]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5415]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2106]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0852]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8821]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4197]]), B=tensor([[0.4070]])\n",
      "Loss: 1.3975636959075928\n",
      "\n",
      "> Iteration 548/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0144]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7482]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5414]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2105]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0850]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8821]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4195]]), B=tensor([[0.4068]])\n",
      "Loss: 1.3915232419967651\n",
      "\n",
      "> Iteration 549/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0144]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7481]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5412]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2105]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0849]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8822]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4192]]), B=tensor([[0.4065]])\n",
      "Loss: 1.4055951833724976\n",
      "\n",
      "> Iteration 550/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0143]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7479]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5410]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2105]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0847]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8823]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4190]]), B=tensor([[0.4063]])\n",
      "Loss: 1.3841452598571777\n",
      "\n",
      "> Iteration 551/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0143]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7478]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5408]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2104]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0845]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8823]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4187]]), B=tensor([[0.4061]])\n",
      "Loss: 1.4060004949569702\n",
      "\n",
      "> Iteration 552/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0142]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7476]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5407]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2104]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0844]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8824]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4185]]), B=tensor([[0.4059]])\n",
      "Loss: 1.414673924446106\n",
      "\n",
      "> Iteration 553/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0142]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7475]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5405]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2104]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0842]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8825]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4182]]), B=tensor([[0.4057]])\n",
      "Loss: 1.3212406635284424\n",
      "\n",
      "> Iteration 554/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0142]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7473]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5404]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2103]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0841]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8825]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4180]]), B=tensor([[0.4055]])\n",
      "Loss: 1.4051401615142822\n",
      "\n",
      "> Iteration 555/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0141]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7472]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5402]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2103]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0839]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8826]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4177]]), B=tensor([[0.4053]])\n",
      "Loss: 1.4057376384735107\n",
      "\n",
      "> Iteration 556/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0140]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7470]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5400]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2103]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0838]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8826]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4175]]), B=tensor([[0.4051]])\n",
      "Loss: 1.3695125579833984\n",
      "\n",
      "> Iteration 557/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0140]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7469]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5399]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2103]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0836]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8827]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4172]]), B=tensor([[0.4049]])\n",
      "Loss: 1.3867132663726807\n",
      "\n",
      "> Iteration 558/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0139]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7467]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5397]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2102]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0835]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8828]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4169]]), B=tensor([[0.4046]])\n",
      "Loss: 1.3994314670562744\n",
      "\n",
      "> Iteration 559/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0139]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7466]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5395]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2102]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0833]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8828]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4167]]), B=tensor([[0.4044]])\n",
      "Loss: 1.376090407371521\n",
      "\n",
      "> Iteration 560/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0138]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7464]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5394]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2102]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0831]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8829]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4164]]), B=tensor([[0.4042]])\n",
      "Loss: 1.413594126701355\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 561/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0138]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7463]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5392]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2101]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0830]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8830]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4162]]), B=tensor([[0.4040]])\n",
      "Loss: 1.3846766948699951\n",
      "\n",
      "> Iteration 562/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0138]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7461]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5390]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2101]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0828]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8830]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4160]]), B=tensor([[0.4038]])\n",
      "Loss: 1.4002431631088257\n",
      "\n",
      "> Iteration 563/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0137]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7460]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5389]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2101]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0827]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8831]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4157]]), B=tensor([[0.4036]])\n",
      "Loss: 1.412246584892273\n",
      "\n",
      "> Iteration 564/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0137]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7458]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5387]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2100]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0825]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8832]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4155]]), B=tensor([[0.4034]])\n",
      "Loss: 1.3828234672546387\n",
      "\n",
      "> Iteration 565/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0137]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7457]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5385]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2100]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0824]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8832]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4152]]), B=tensor([[0.4032]])\n",
      "Loss: 1.3587286472320557\n",
      "\n",
      "> Iteration 566/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0137]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7455]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5384]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2100]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0822]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8833]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4150]]), B=tensor([[0.4030]])\n",
      "Loss: 1.4158153533935547\n",
      "\n",
      "> Iteration 567/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0136]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7454]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5382]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2100]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0820]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8834]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4148]]), B=tensor([[0.4028]])\n",
      "Loss: 1.4020593166351318\n",
      "\n",
      "> Iteration 568/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0136]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7452]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5380]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2099]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0819]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8834]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4146]]), B=tensor([[0.4025]])\n",
      "Loss: 1.3787407875061035\n",
      "\n",
      "> Iteration 569/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0136]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7451]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5379]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2099]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0817]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8835]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4144]]), B=tensor([[0.4023]])\n",
      "Loss: 1.3819581270217896\n",
      "\n",
      "> Iteration 570/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0136]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7449]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5377]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2099]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0816]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8835]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4142]]), B=tensor([[0.4021]])\n",
      "Loss: 1.3777414560317993\n",
      "\n",
      "> Iteration 571/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0136]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7448]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5375]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2098]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0814]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8836]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4139]]), B=tensor([[0.4019]])\n",
      "Loss: 1.392270803451538\n",
      "\n",
      "> Iteration 572/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0136]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7446]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5374]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2098]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0813]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8837]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4137]]), B=tensor([[0.4017]])\n",
      "Loss: 1.4202378988265991\n",
      "\n",
      "> Iteration 573/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7444]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5372]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2098]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0811]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8837]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4134]]), B=tensor([[0.4015]])\n",
      "Loss: 1.408676266670227\n",
      "\n",
      "> Iteration 574/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7443]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5370]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2097]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0809]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8838]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4132]]), B=tensor([[0.4013]])\n",
      "Loss: 1.3727139234542847\n",
      "\n",
      "> Iteration 575/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7441]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5368]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2097]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0808]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8839]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4130]]), B=tensor([[0.4011]])\n",
      "Loss: 1.358710527420044\n",
      "\n",
      "> Iteration 576/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7440]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5367]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2097]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0806]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8839]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4128]]), B=tensor([[0.4009]])\n",
      "Loss: 1.3573119640350342\n",
      "\n",
      "> Iteration 577/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7438]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5365]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2097]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0805]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8840]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4126]]), B=tensor([[0.4007]])\n",
      "Loss: 1.3852912187576294\n",
      "\n",
      "> Iteration 578/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7437]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5364]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2096]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0804]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8841]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4124]]), B=tensor([[0.4005]])\n",
      "Loss: 1.362803339958191\n",
      "\n",
      "> Iteration 579/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7436]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5362]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2096]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0802]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8841]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4122]]), B=tensor([[0.4003]])\n",
      "Loss: 1.4005159139633179\n",
      "\n",
      "> Iteration 580/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7434]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5360]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2096]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0801]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8842]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4119]]), B=tensor([[0.4001]])\n",
      "Loss: 1.4361119270324707\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 581/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7433]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5359]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2095]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0799]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8842]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4117]]), B=tensor([[0.3998]])\n",
      "Loss: 1.3837838172912598\n",
      "\n",
      "> Iteration 582/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7431]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5357]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2095]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0797]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8843]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4115]]), B=tensor([[0.3996]])\n",
      "Loss: 1.3979257345199585\n",
      "\n",
      "> Iteration 583/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7430]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5356]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2095]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0796]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8844]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4113]]), B=tensor([[0.3994]])\n",
      "Loss: 1.3689738512039185\n",
      "\n",
      "> Iteration 584/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7428]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5354]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2095]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0794]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8844]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4110]]), B=tensor([[0.3992]])\n",
      "Loss: 1.38571035861969\n",
      "\n",
      "> Iteration 585/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0133]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7427]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5352]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2094]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0793]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8845]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4108]]), B=tensor([[0.3990]])\n",
      "Loss: 1.4369220733642578\n",
      "\n",
      "> Iteration 586/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7425]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5351]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2094]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0791]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8846]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4106]]), B=tensor([[0.3988]])\n",
      "Loss: 1.4070301055908203\n",
      "\n",
      "> Iteration 587/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7424]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5349]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2094]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0790]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8846]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4105]]), B=tensor([[0.3986]])\n",
      "Loss: 1.407985806465149\n",
      "\n",
      "> Iteration 588/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7422]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5347]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2093]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0789]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8847]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4103]]), B=tensor([[0.3984]])\n",
      "Loss: 1.3588758707046509\n",
      "\n",
      "> Iteration 589/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7421]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5346]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2093]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0787]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8847]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4101]]), B=tensor([[0.3982]])\n",
      "Loss: 1.3808194398880005\n",
      "\n",
      "> Iteration 590/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7419]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5344]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2093]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0786]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8848]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4099]]), B=tensor([[0.3981]])\n",
      "Loss: 1.3418073654174805\n",
      "\n",
      "> Iteration 591/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7418]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5343]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2093]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0785]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8848]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4098]]), B=tensor([[0.3979]])\n",
      "Loss: 1.4110370874404907\n",
      "\n",
      "> Iteration 592/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7417]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5341]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2092]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0783]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8849]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4096]]), B=tensor([[0.3977]])\n",
      "Loss: 1.3915917873382568\n",
      "\n",
      "> Iteration 593/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7415]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5340]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2092]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0782]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8850]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4094]]), B=tensor([[0.3975]])\n",
      "Loss: 1.3871819972991943\n",
      "\n",
      "> Iteration 594/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7414]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5338]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2092]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0780]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8850]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4092]]), B=tensor([[0.3973]])\n",
      "Loss: 1.397760033607483\n",
      "\n",
      "> Iteration 595/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7412]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5337]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2092]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0779]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8851]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4090]]), B=tensor([[0.3971]])\n",
      "Loss: 1.382455587387085\n",
      "\n",
      "> Iteration 596/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7411]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5335]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2091]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0777]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8851]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4087]]), B=tensor([[0.3969]])\n",
      "Loss: 1.40985107421875\n",
      "\n",
      "> Iteration 597/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7410]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5333]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2091]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0776]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8852]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4086]]), B=tensor([[0.3967]])\n",
      "Loss: 1.4205883741378784\n",
      "\n",
      "> Iteration 598/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7408]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5332]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2091]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0775]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8853]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4084]]), B=tensor([[0.3965]])\n",
      "Loss: 1.3817071914672852\n",
      "\n",
      "> Iteration 599/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7407]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5330]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2090]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0773]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8853]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4082]]), B=tensor([[0.3963]])\n",
      "Loss: 1.3655312061309814\n",
      "\n",
      "> Iteration 600/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7405]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5329]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2090]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0772]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8854]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4080]]), B=tensor([[0.3961]])\n",
      "Loss: 1.457107424736023\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 601/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0133]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7404]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5327]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2090]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0770]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8854]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4077]]), B=tensor([[0.3959]])\n",
      "Loss: 1.3775161504745483\n",
      "\n",
      "> Iteration 602/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0133]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7402]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5325]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2090]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0768]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8855]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4075]]), B=tensor([[0.3957]])\n",
      "Loss: 1.4136384725570679\n",
      "\n",
      "> Iteration 603/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0133]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7401]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5324]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2089]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0767]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8856]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4072]]), B=tensor([[0.3955]])\n",
      "Loss: 1.3432196378707886\n",
      "\n",
      "> Iteration 604/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0132]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7399]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5322]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2089]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0765]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8856]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4070]]), B=tensor([[0.3953]])\n",
      "Loss: 1.40909743309021\n",
      "\n",
      "> Iteration 605/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0132]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7397]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5320]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2089]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0764]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8857]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4067]]), B=tensor([[0.3951]])\n",
      "Loss: 1.4183930158615112\n",
      "\n",
      "> Iteration 606/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7396]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5318]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2088]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0762]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8858]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4065]]), B=tensor([[0.3948]])\n",
      "Loss: 1.4069981575012207\n",
      "\n",
      "> Iteration 607/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7394]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5317]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2088]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0760]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8858]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4063]]), B=tensor([[0.3946]])\n",
      "Loss: 1.4047186374664307\n",
      "\n",
      "> Iteration 608/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7393]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5315]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2088]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0759]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8859]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4060]]), B=tensor([[0.3944]])\n",
      "Loss: 1.428611397743225\n",
      "\n",
      "> Iteration 609/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7391]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5313]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2088]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0757]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8860]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4058]]), B=tensor([[0.3942]])\n",
      "Loss: 1.3903076648712158\n",
      "\n",
      "> Iteration 610/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7390]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5311]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2087]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0756]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8860]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4056]]), B=tensor([[0.3940]])\n",
      "Loss: 1.3963760137557983\n",
      "\n",
      "> Iteration 611/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7388]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5310]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2087]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0754]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8861]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4054]]), B=tensor([[0.3938]])\n",
      "Loss: 1.4204717874526978\n",
      "\n",
      "> Iteration 612/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7386]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5308]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2087]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0753]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8862]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4052]]), B=tensor([[0.3936]])\n",
      "Loss: 1.3754680156707764\n",
      "\n",
      "> Iteration 613/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7385]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5306]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2086]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0751]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8862]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4049]]), B=tensor([[0.3933]])\n",
      "Loss: 1.4022552967071533\n",
      "\n",
      "> Iteration 614/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7383]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5305]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2086]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0749]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8863]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4047]]), B=tensor([[0.3931]])\n",
      "Loss: 1.403274655342102\n",
      "\n",
      "> Iteration 615/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7382]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5303]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2086]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0748]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8863]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4045]]), B=tensor([[0.3929]])\n",
      "Loss: 1.397666573524475\n",
      "\n",
      "> Iteration 616/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7380]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5301]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2086]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0746]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8864]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4043]]), B=tensor([[0.3927]])\n",
      "Loss: 1.4212126731872559\n",
      "\n",
      "> Iteration 617/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5299]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2085]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0745]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8865]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4041]]), B=tensor([[0.3925]])\n",
      "Loss: 1.400569200515747\n",
      "\n",
      "> Iteration 618/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7377]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5298]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2085]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0743]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8865]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4038]]), B=tensor([[0.3923]])\n",
      "Loss: 1.4195830821990967\n",
      "\n",
      "> Iteration 619/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7376]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5296]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2085]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0742]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8866]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4036]]), B=tensor([[0.3921]])\n",
      "Loss: 1.3842906951904297\n",
      "\n",
      "> Iteration 620/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7374]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5294]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2084]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0740]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8867]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4034]]), B=tensor([[0.3919]])\n",
      "Loss: 1.3622721433639526\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 621/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7372]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5293]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2084]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0739]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8867]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4032]]), B=tensor([[0.3917]])\n",
      "Loss: 1.4343563318252563\n",
      "\n",
      "> Iteration 622/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7371]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5291]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2084]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0737]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8868]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4030]]), B=tensor([[0.3914]])\n",
      "Loss: 1.384483814239502\n",
      "\n",
      "> Iteration 623/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7369]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5289]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2084]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0735]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8869]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4027]]), B=tensor([[0.3912]])\n",
      "Loss: 1.3950873613357544\n",
      "\n",
      "> Iteration 624/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0129]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7368]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5287]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2083]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0734]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8869]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4025]]), B=tensor([[0.3910]])\n",
      "Loss: 1.4227678775787354\n",
      "\n",
      "> Iteration 625/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0129]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7366]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5285]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2083]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0732]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8870]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4022]]), B=tensor([[0.3908]])\n",
      "Loss: 1.423060655593872\n",
      "\n",
      "> Iteration 626/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0129]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7364]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5284]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2083]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0730]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8870]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4020]]), B=tensor([[0.3906]])\n",
      "Loss: 1.3755391836166382\n",
      "\n",
      "> Iteration 627/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0129]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7363]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5282]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2082]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0729]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8871]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4018]]), B=tensor([[0.3904]])\n",
      "Loss: 1.4006022214889526\n",
      "\n",
      "> Iteration 628/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0128]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7361]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5280]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2082]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0727]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8872]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4015]]), B=tensor([[0.3901]])\n",
      "Loss: 1.4402244091033936\n",
      "\n",
      "> Iteration 629/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0128]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7360]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5279]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2082]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0726]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8872]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4013]]), B=tensor([[0.3899]])\n",
      "Loss: 1.3606051206588745\n",
      "\n",
      "> Iteration 630/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0128]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7358]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5277]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2082]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0724]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8873]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4011]]), B=tensor([[0.3897]])\n",
      "Loss: 1.400422215461731\n",
      "\n",
      "> Iteration 631/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0128]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7357]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5275]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2081]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0723]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8874]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4009]]), B=tensor([[0.3895]])\n",
      "Loss: 1.4009653329849243\n",
      "\n",
      "> Iteration 632/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0128]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7355]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5274]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2081]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0721]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8874]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4006]]), B=tensor([[0.3893]])\n",
      "Loss: 1.415019154548645\n",
      "\n",
      "> Iteration 633/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0127]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7354]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5272]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2081]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0719]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8875]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4004]]), B=tensor([[0.3891]])\n",
      "Loss: 1.3550759553909302\n",
      "\n",
      "> Iteration 634/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0127]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7352]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5270]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2080]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0718]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8875]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4002]]), B=tensor([[0.3889]])\n",
      "Loss: 1.3803783655166626\n",
      "\n",
      "> Iteration 635/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0127]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7351]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5269]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2080]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0716]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8876]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.3887]])\n",
      "Loss: 1.3863223791122437\n",
      "\n",
      "> Iteration 636/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0126]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7349]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5267]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2080]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0715]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8877]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3997]]), B=tensor([[0.3885]])\n",
      "Loss: 1.403935194015503\n",
      "\n",
      "> Iteration 637/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0126]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7348]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5265]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2080]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0713]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8877]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3995]]), B=tensor([[0.3883]])\n",
      "Loss: 1.3935304880142212\n",
      "\n",
      "> Iteration 638/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0126]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7346]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5264]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2079]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0712]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8878]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3993]]), B=tensor([[0.3881]])\n",
      "Loss: 1.4141706228256226\n",
      "\n",
      "> Iteration 639/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0125]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7345]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5262]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2079]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0710]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8878]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3990]]), B=tensor([[0.3879]])\n",
      "Loss: 1.3835090398788452\n",
      "\n",
      "> Iteration 640/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0125]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7344]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5261]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2079]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0709]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8879]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3988]]), B=tensor([[0.3877]])\n",
      "Loss: 1.3789188861846924\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 641/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0125]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7342]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5259]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2079]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0707]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8879]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3986]]), B=tensor([[0.3875]])\n",
      "Loss: 1.3575594425201416\n",
      "\n",
      "> Iteration 642/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0124]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7341]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5257]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2078]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0706]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8880]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3984]]), B=tensor([[0.3873]])\n",
      "Loss: 1.3651303052902222\n",
      "\n",
      "> Iteration 643/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0124]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7339]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5256]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2078]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0705]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8881]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3982]]), B=tensor([[0.3871]])\n",
      "Loss: 1.3848158121109009\n",
      "\n",
      "> Iteration 644/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0124]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7338]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5254]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2078]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0703]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8881]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3979]]), B=tensor([[0.3869]])\n",
      "Loss: 1.3867456912994385\n",
      "\n",
      "> Iteration 645/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0124]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7336]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5253]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2078]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0702]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8882]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3977]]), B=tensor([[0.3867]])\n",
      "Loss: 1.3569811582565308\n",
      "\n",
      "> Iteration 646/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0123]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7335]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5251]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2077]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0700]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8882]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3975]]), B=tensor([[0.3865]])\n",
      "Loss: 1.3902342319488525\n",
      "\n",
      "> Iteration 647/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0123]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7333]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5249]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2077]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0699]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8883]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3973]]), B=tensor([[0.3863]])\n",
      "Loss: 1.403089165687561\n",
      "\n",
      "> Iteration 648/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0123]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7332]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5248]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2077]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0697]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8884]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3971]]), B=tensor([[0.3861]])\n",
      "Loss: 1.3921574354171753\n",
      "\n",
      "> Iteration 649/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0123]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7331]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5246]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2077]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0696]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8884]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3969]]), B=tensor([[0.3859]])\n",
      "Loss: 1.4420745372772217\n",
      "\n",
      "> Iteration 650/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0123]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7329]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5244]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2076]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0694]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8885]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3966]]), B=tensor([[0.3857]])\n",
      "Loss: 1.4046422243118286\n",
      "\n",
      "> Iteration 651/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7327]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5243]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2076]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0692]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8885]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3964]]), B=tensor([[0.3855]])\n",
      "Loss: 1.396179437637329\n",
      "\n",
      "> Iteration 652/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7326]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5241]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2076]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0691]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8886]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3962]]), B=tensor([[0.3853]])\n",
      "Loss: 1.4116872549057007\n",
      "\n",
      "> Iteration 653/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7324]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5239]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2076]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0689]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8887]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3960]]), B=tensor([[0.3851]])\n",
      "Loss: 1.3905706405639648\n",
      "\n",
      "> Iteration 654/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7323]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5238]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2075]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0688]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8887]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3958]]), B=tensor([[0.3849]])\n",
      "Loss: 1.4092001914978027\n",
      "\n",
      "> Iteration 655/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7321]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5236]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2075]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0686]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8888]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3956]]), B=tensor([[0.3847]])\n",
      "Loss: 1.4181209802627563\n",
      "\n",
      "> Iteration 656/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7320]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5234]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2075]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0685]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8888]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3953]]), B=tensor([[0.3845]])\n",
      "Loss: 1.3909043073654175\n",
      "\n",
      "> Iteration 657/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7318]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5233]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2074]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0683]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8889]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3951]]), B=tensor([[0.3843]])\n",
      "Loss: 1.3871581554412842\n",
      "\n",
      "> Iteration 658/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7317]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5231]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2074]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0682]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8890]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3950]]), B=tensor([[0.3841]])\n",
      "Loss: 1.3793426752090454\n",
      "\n",
      "> Iteration 659/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7315]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5229]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2074]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0680]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8890]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.3839]])\n",
      "Loss: 1.4168556928634644\n",
      "\n",
      "> Iteration 660/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0123]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7314]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5228]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2074]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0679]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8891]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3946]]), B=tensor([[0.3837]])\n",
      "Loss: 1.4352401494979858\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 661/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0123]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7312]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5226]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2073]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0677]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8891]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3944]]), B=tensor([[0.3835]])\n",
      "Loss: 1.4118316173553467\n",
      "\n",
      "> Iteration 662/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0123]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7311]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5224]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2073]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0676]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8892]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3942]]), B=tensor([[0.3833]])\n",
      "Loss: 1.3877900838851929\n",
      "\n",
      "> Iteration 663/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0123]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7309]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5223]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2073]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0675]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8893]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3941]]), B=tensor([[0.3831]])\n",
      "Loss: 1.3872284889221191\n",
      "\n",
      "> Iteration 664/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0124]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7308]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5221]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2073]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0673]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8893]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3939]]), B=tensor([[0.3829]])\n",
      "Loss: 1.362645149230957\n",
      "\n",
      "> Iteration 665/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0124]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5219]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2072]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0672]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8894]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3938]]), B=tensor([[0.3827]])\n",
      "Loss: 1.4070892333984375\n",
      "\n",
      "> Iteration 666/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0125]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5218]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2072]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0671]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8894]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3936]]), B=tensor([[0.3825]])\n",
      "Loss: 1.3660651445388794\n",
      "\n",
      "> Iteration 667/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0125]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5216]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2072]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0669]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8895]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3935]]), B=tensor([[0.3824]])\n",
      "Loss: 1.3346173763275146\n",
      "\n",
      "> Iteration 668/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0125]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5215]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2072]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0668]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8895]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3933]]), B=tensor([[0.3822]])\n",
      "Loss: 1.407713770866394\n",
      "\n",
      "> Iteration 669/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0126]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5213]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2071]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0667]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8896]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3932]]), B=tensor([[0.3820]])\n",
      "Loss: 1.3758260011672974\n",
      "\n",
      "> Iteration 670/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0126]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5212]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2071]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0665]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8896]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3930]]), B=tensor([[0.3818]])\n",
      "Loss: 1.4423768520355225\n",
      "\n",
      "> Iteration 671/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0126]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7298]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5210]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2071]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0664]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8897]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3928]]), B=tensor([[0.3816]])\n",
      "Loss: 1.398495078086853\n",
      "\n",
      "> Iteration 672/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0126]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5209]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2071]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0663]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8898]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3927]]), B=tensor([[0.3814]])\n",
      "Loss: 1.3953627347946167\n",
      "\n",
      "> Iteration 673/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0127]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5207]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2070]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0661]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8898]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3925]]), B=tensor([[0.3813]])\n",
      "Loss: 1.4134562015533447\n",
      "\n",
      "> Iteration 674/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0127]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5206]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2070]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0660]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8899]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3924]]), B=tensor([[0.3811]])\n",
      "Loss: 1.3777637481689453\n",
      "\n",
      "> Iteration 675/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0128]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5204]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2070]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0659]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8899]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3923]]), B=tensor([[0.3809]])\n",
      "Loss: 1.4472694396972656\n",
      "\n",
      "> Iteration 676/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0128]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5203]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2070]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0658]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8900]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3921]]), B=tensor([[0.3807]])\n",
      "Loss: 1.3869373798370361\n",
      "\n",
      "> Iteration 677/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0129]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5201]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2069]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0656]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8900]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3920]]), B=tensor([[0.3806]])\n",
      "Loss: 1.3995815515518188\n",
      "\n",
      "> Iteration 678/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0129]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7288]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5200]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2069]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0655]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8901]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3919]]), B=tensor([[0.3804]])\n",
      "Loss: 1.397841215133667\n",
      "\n",
      "> Iteration 679/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7287]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5198]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2069]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0654]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8901]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3917]]), B=tensor([[0.3802]])\n",
      "Loss: 1.4311378002166748\n",
      "\n",
      "> Iteration 680/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0130]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7286]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5196]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2069]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0652]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8902]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3916]]), B=tensor([[0.3800]])\n",
      "Loss: 1.388683557510376\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 681/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7284]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5195]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2068]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0651]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8903]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3915]]), B=tensor([[0.3798]])\n",
      "Loss: 1.416446566581726\n",
      "\n",
      "> Iteration 682/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7283]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5193]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2068]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0650]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8903]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3914]]), B=tensor([[0.3796]])\n",
      "Loss: 1.3767621517181396\n",
      "\n",
      "> Iteration 683/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0132]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7281]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5192]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2068]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0649]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8904]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3912]]), B=tensor([[0.3795]])\n",
      "Loss: 1.3895270824432373\n",
      "\n",
      "> Iteration 684/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0133]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7280]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5190]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2068]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0647]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8904]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3911]]), B=tensor([[0.3793]])\n",
      "Loss: 1.4097706079483032\n",
      "\n",
      "> Iteration 685/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0133]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7279]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5189]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2067]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0646]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8905]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3910]]), B=tensor([[0.3791]])\n",
      "Loss: 1.3503037691116333\n",
      "\n",
      "> Iteration 686/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7277]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5187]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2067]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0645]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8905]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3909]]), B=tensor([[0.3789]])\n",
      "Loss: 1.4013404846191406\n",
      "\n",
      "> Iteration 687/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7276]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5186]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2067]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0644]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8906]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3907]]), B=tensor([[0.3788]])\n",
      "Loss: 1.370316743850708\n",
      "\n",
      "> Iteration 688/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7275]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5184]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2067]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0642]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8906]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3906]]), B=tensor([[0.3786]])\n",
      "Loss: 1.3511853218078613\n",
      "\n",
      "> Iteration 689/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0135]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7273]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5183]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2066]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0641]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8907]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3905]]), B=tensor([[0.3784]])\n",
      "Loss: 1.3939083814620972\n",
      "\n",
      "> Iteration 690/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0136]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7272]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5181]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2066]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0640]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8907]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3904]]), B=tensor([[0.3783]])\n",
      "Loss: 1.3955671787261963\n",
      "\n",
      "> Iteration 691/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0136]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7271]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5180]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2066]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0639]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8908]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3902]]), B=tensor([[0.3781]])\n",
      "Loss: 1.414239764213562\n",
      "\n",
      "> Iteration 692/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0137]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7269]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5178]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2066]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0638]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8908]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3901]]), B=tensor([[0.3779]])\n",
      "Loss: 1.4097820520401\n",
      "\n",
      "> Iteration 693/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0138]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7268]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5177]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2065]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0637]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8909]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.3778]])\n",
      "Loss: 1.410248875617981\n",
      "\n",
      "> Iteration 694/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0138]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7267]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5176]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2065]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0636]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8909]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.3776]])\n",
      "Loss: 1.3825100660324097\n",
      "\n",
      "> Iteration 695/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0139]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7266]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5174]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2065]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0635]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8910]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3899]]), B=tensor([[0.3775]])\n",
      "Loss: 1.3687162399291992\n",
      "\n",
      "> Iteration 696/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0140]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7264]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5173]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2065]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0633]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8910]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3898]]), B=tensor([[0.3773]])\n",
      "Loss: 1.3863378763198853\n",
      "\n",
      "> Iteration 697/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0140]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7263]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5172]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2064]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0632]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8911]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3897]]), B=tensor([[0.3772]])\n",
      "Loss: 1.3815571069717407\n",
      "\n",
      "> Iteration 698/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0141]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7262]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5170]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2064]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0631]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8911]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3895]]), B=tensor([[0.3770]])\n",
      "Loss: 1.3988685607910156\n",
      "\n",
      "> Iteration 699/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0141]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7261]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5169]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2064]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0630]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8912]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3894]]), B=tensor([[0.3768]])\n",
      "Loss: 1.3987847566604614\n",
      "\n",
      "> Iteration 700/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0141]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7260]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5167]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2064]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0629]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8912]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3893]]), B=tensor([[0.3767]])\n",
      "Loss: 1.391640543937683\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 701/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0142]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7258]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5166]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2063]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0628]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8913]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3891]]), B=tensor([[0.3765]])\n",
      "Loss: 1.4120115041732788\n",
      "\n",
      "> Iteration 702/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0142]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7257]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5165]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2063]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0627]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8914]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3890]]), B=tensor([[0.3763]])\n",
      "Loss: 1.3712021112442017\n",
      "\n",
      "> Iteration 703/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0142]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7256]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5163]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2063]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0625]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8914]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3889]]), B=tensor([[0.3762]])\n",
      "Loss: 1.3924311399459839\n",
      "\n",
      "> Iteration 704/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0143]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7254]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5162]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2063]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0624]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8915]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3887]]), B=tensor([[0.3760]])\n",
      "Loss: 1.4059315919876099\n",
      "\n",
      "> Iteration 705/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0143]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7253]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5160]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2062]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0623]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8915]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3886]]), B=tensor([[0.3758]])\n",
      "Loss: 1.4271844625473022\n",
      "\n",
      "> Iteration 706/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0143]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7252]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5159]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2062]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0622]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8916]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3884]]), B=tensor([[0.3757]])\n",
      "Loss: 1.4182119369506836\n",
      "\n",
      "> Iteration 707/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0143]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7251]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5157]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2062]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0621]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8916]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3883]]), B=tensor([[0.3755]])\n",
      "Loss: 1.37284255027771\n",
      "\n",
      "> Iteration 708/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0143]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7249]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5156]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2062]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0619]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8917]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3881]]), B=tensor([[0.3753]])\n",
      "Loss: 1.3809176683425903\n",
      "\n",
      "> Iteration 709/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0144]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7248]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5155]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2061]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0618]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8917]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3880]]), B=tensor([[0.3752]])\n",
      "Loss: 1.4281542301177979\n",
      "\n",
      "> Iteration 710/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0144]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7247]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5153]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2061]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0617]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8918]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3878]]), B=tensor([[0.3750]])\n",
      "Loss: 1.4693833589553833\n",
      "\n",
      "> Iteration 711/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0144]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7245]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5152]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2061]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0616]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8918]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3876]]), B=tensor([[0.3748]])\n",
      "Loss: 1.3908652067184448\n",
      "\n",
      "> Iteration 712/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0144]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7244]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5150]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2060]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0615]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8919]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3875]]), B=tensor([[0.3746]])\n",
      "Loss: 1.4125471115112305\n",
      "\n",
      "> Iteration 713/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0145]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7243]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5149]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2060]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0613]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8919]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3874]]), B=tensor([[0.3745]])\n",
      "Loss: 1.3757683038711548\n",
      "\n",
      "> Iteration 714/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0145]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7241]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5147]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2060]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0612]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8920]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3872]]), B=tensor([[0.3743]])\n",
      "Loss: 1.4326446056365967\n",
      "\n",
      "> Iteration 715/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0145]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7240]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5146]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2060]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0611]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8920]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3871]]), B=tensor([[0.3741]])\n",
      "Loss: 1.382213830947876\n",
      "\n",
      "> Iteration 716/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0145]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7239]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5144]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2059]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0610]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8921]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3869]]), B=tensor([[0.3740]])\n",
      "Loss: 1.3998386859893799\n",
      "\n",
      "> Iteration 717/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7238]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5143]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2059]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0609]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8921]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3868]]), B=tensor([[0.3738]])\n",
      "Loss: 1.3816378116607666\n",
      "\n",
      "> Iteration 718/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7236]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5142]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2059]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0608]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8922]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3867]]), B=tensor([[0.3736]])\n",
      "Loss: 1.4062857627868652\n",
      "\n",
      "> Iteration 719/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7235]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5140]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2059]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0606]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8922]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3865]]), B=tensor([[0.3735]])\n",
      "Loss: 1.4069187641143799\n",
      "\n",
      "> Iteration 720/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7234]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5139]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2058]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0605]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8923]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3863]]), B=tensor([[0.3733]])\n",
      "Loss: 1.3549185991287231\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 721/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7232]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5137]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2058]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0604]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8923]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3861]]), B=tensor([[0.3731]])\n",
      "Loss: 1.4127390384674072\n",
      "\n",
      "> Iteration 722/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7231]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5136]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2058]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0602]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8924]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3860]]), B=tensor([[0.3729]])\n",
      "Loss: 1.4178208112716675\n",
      "\n",
      "> Iteration 723/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7230]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5134]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2058]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0601]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8925]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3858]]), B=tensor([[0.3728]])\n",
      "Loss: 1.3719526529312134\n",
      "\n",
      "> Iteration 724/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7228]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5133]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2057]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0600]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8925]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3856]]), B=tensor([[0.3726]])\n",
      "Loss: 1.4190815687179565\n",
      "\n",
      "> Iteration 725/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7227]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5131]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2057]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0599]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8926]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3854]]), B=tensor([[0.3724]])\n",
      "Loss: 1.3416987657546997\n",
      "\n",
      "> Iteration 726/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7226]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5130]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2057]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0598]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8926]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3853]]), B=tensor([[0.3722]])\n",
      "Loss: 1.4094911813735962\n",
      "\n",
      "> Iteration 727/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7225]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5129]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2057]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0596]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8927]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3851]]), B=tensor([[0.3721]])\n",
      "Loss: 1.3959211111068726\n",
      "\n",
      "> Iteration 728/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7223]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5127]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2056]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0595]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8927]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3849]]), B=tensor([[0.3719]])\n",
      "Loss: 1.4315623044967651\n",
      "\n",
      "> Iteration 729/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7222]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5126]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2056]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0594]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8928]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3848]]), B=tensor([[0.3717]])\n",
      "Loss: 1.3781625032424927\n",
      "\n",
      "> Iteration 730/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7221]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5124]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2056]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0593]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8928]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3846]]), B=tensor([[0.3716]])\n",
      "Loss: 1.3805922269821167\n",
      "\n",
      "> Iteration 731/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7219]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5123]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2055]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0591]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8929]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3844]]), B=tensor([[0.3714]])\n",
      "Loss: 1.372008204460144\n",
      "\n",
      "> Iteration 732/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7218]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5121]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2055]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0590]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8929]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3843]]), B=tensor([[0.3712]])\n",
      "Loss: 1.3978770971298218\n",
      "\n",
      "> Iteration 733/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7217]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5120]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2055]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0589]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8930]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3841]]), B=tensor([[0.3711]])\n",
      "Loss: 1.4011693000793457\n",
      "\n",
      "> Iteration 734/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7216]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5118]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2055]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0588]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8930]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3839]]), B=tensor([[0.3709]])\n",
      "Loss: 1.4096463918685913\n",
      "\n",
      "> Iteration 735/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7214]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5117]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2054]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0587]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8931]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3838]]), B=tensor([[0.3707]])\n",
      "Loss: 1.3734471797943115\n",
      "\n",
      "> Iteration 736/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7213]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5116]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2054]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0585]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8931]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3836]]), B=tensor([[0.3706]])\n",
      "Loss: 1.34890878200531\n",
      "\n",
      "> Iteration 737/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7212]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5114]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2054]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0584]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8932]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3834]]), B=tensor([[0.3704]])\n",
      "Loss: 1.3903216123580933\n",
      "\n",
      "> Iteration 738/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7211]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5113]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2054]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0583]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8932]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3833]]), B=tensor([[0.3702]])\n",
      "Loss: 1.4300529956817627\n",
      "\n",
      "> Iteration 739/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7209]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5111]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2053]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0582]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8933]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3831]]), B=tensor([[0.3701]])\n",
      "Loss: 1.401178240776062\n",
      "\n",
      "> Iteration 740/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7208]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5110]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2053]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0581]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8933]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3829]]), B=tensor([[0.3699]])\n",
      "Loss: 1.3796963691711426\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 741/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7207]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5109]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2053]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0579]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8934]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3828]]), B=tensor([[0.3697]])\n",
      "Loss: 1.3764195442199707\n",
      "\n",
      "> Iteration 742/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0146]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7206]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5107]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2053]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0578]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8934]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3826]]), B=tensor([[0.3696]])\n",
      "Loss: 1.3440558910369873\n",
      "\n",
      "> Iteration 743/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0147]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7204]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5106]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2052]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0577]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8935]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3825]]), B=tensor([[0.3694]])\n",
      "Loss: 1.359332799911499\n",
      "\n",
      "> Iteration 744/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0147]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7203]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5105]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2052]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0576]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8935]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3824]]), B=tensor([[0.3693]])\n",
      "Loss: 1.3870989084243774\n",
      "\n",
      "> Iteration 745/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7202]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5103]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2052]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0575]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8936]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3823]]), B=tensor([[0.3691]])\n",
      "Loss: 1.3494155406951904\n",
      "\n",
      "> Iteration 746/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7201]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5102]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2052]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0574]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8936]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3822]]), B=tensor([[0.3690]])\n",
      "Loss: 1.377087116241455\n",
      "\n",
      "> Iteration 747/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7200]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5101]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2051]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0573]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8937]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3821]]), B=tensor([[0.3688]])\n",
      "Loss: 1.3972548246383667\n",
      "\n",
      "> Iteration 748/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7199]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5100]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2051]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0572]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8937]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3820]]), B=tensor([[0.3687]])\n",
      "Loss: 1.3559916019439697\n",
      "\n",
      "> Iteration 749/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7197]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5098]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2051]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0571]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8938]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3818]]), B=tensor([[0.3685]])\n",
      "Loss: 1.3704904317855835\n",
      "\n",
      "> Iteration 750/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7196]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5097]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2051]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0570]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8938]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3817]]), B=tensor([[0.3684]])\n",
      "Loss: 1.3986632823944092\n",
      "\n",
      "> Iteration 751/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7195]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5096]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2050]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0569]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8938]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3815]]), B=tensor([[0.3682]])\n",
      "Loss: 1.35964834690094\n",
      "\n",
      "> Iteration 752/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7194]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5095]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2050]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0568]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8939]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3814]]), B=tensor([[0.3681]])\n",
      "Loss: 1.3438540697097778\n",
      "\n",
      "> Iteration 753/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7193]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5093]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2050]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0567]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8939]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3812]]), B=tensor([[0.3679]])\n",
      "Loss: 1.4178550243377686\n",
      "\n",
      "> Iteration 754/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7192]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5092]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2050]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0566]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8940]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3811]]), B=tensor([[0.3678]])\n",
      "Loss: 1.3898429870605469\n",
      "\n",
      "> Iteration 755/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7191]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5091]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2050]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0565]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8940]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3809]]), B=tensor([[0.3676]])\n",
      "Loss: 1.4026063680648804\n",
      "\n",
      "> Iteration 756/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7190]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5090]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2049]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0564]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8941]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3808]]), B=tensor([[0.3675]])\n",
      "Loss: 1.410244107246399\n",
      "\n",
      "> Iteration 757/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7188]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5088]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2049]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0563]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8941]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3806]]), B=tensor([[0.3673]])\n",
      "Loss: 1.4288086891174316\n",
      "\n",
      "> Iteration 758/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7187]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5087]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2049]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0562]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8942]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3805]]), B=tensor([[0.3672]])\n",
      "Loss: 1.4003981351852417\n",
      "\n",
      "> Iteration 759/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7186]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5086]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2049]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0560]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8942]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3803]]), B=tensor([[0.3670]])\n",
      "Loss: 1.4174387454986572\n",
      "\n",
      "> Iteration 760/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7185]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2048]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0559]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8943]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3801]]), B=tensor([[0.3669]])\n",
      "Loss: 1.4018359184265137\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 761/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7183]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2048]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0558]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8943]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3799]]), B=tensor([[0.3667]])\n",
      "Loss: 1.432250738143921\n",
      "\n",
      "> Iteration 762/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7182]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2048]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0557]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8944]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3798]]), B=tensor([[0.3665]])\n",
      "Loss: 1.3804755210876465\n",
      "\n",
      "> Iteration 763/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7181]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2047]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0555]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8944]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3796]]), B=tensor([[0.3663]])\n",
      "Loss: 1.4405455589294434\n",
      "\n",
      "> Iteration 764/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7179]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2047]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0554]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8945]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3794]]), B=tensor([[0.3662]])\n",
      "Loss: 1.387917160987854\n",
      "\n",
      "> Iteration 765/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0148]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7178]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2047]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0553]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8945]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3793]]), B=tensor([[0.3660]])\n",
      "Loss: 1.37549889087677\n",
      "\n",
      "> Iteration 766/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7177]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2047]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0552]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8946]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3791]]), B=tensor([[0.3658]])\n",
      "Loss: 1.3823901414871216\n",
      "\n",
      "> Iteration 767/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7175]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2046]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0551]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8946]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3789]]), B=tensor([[0.3656]])\n",
      "Loss: 1.4194601774215698\n",
      "\n",
      "> Iteration 768/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7174]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2046]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0549]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8947]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3788]]), B=tensor([[0.3655]])\n",
      "Loss: 1.3624943494796753\n",
      "\n",
      "> Iteration 769/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0149]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7173]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2046]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0548]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8947]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3787]]), B=tensor([[0.3653]])\n",
      "Loss: 1.3986667394638062\n",
      "\n",
      "> Iteration 770/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7171]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2046]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0547]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8948]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3785]]), B=tensor([[0.3651]])\n",
      "Loss: 1.4483815431594849\n",
      "\n",
      "> Iteration 771/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7170]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2045]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0546]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8949]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3783]]), B=tensor([[0.3649]])\n",
      "Loss: 1.4104479551315308\n",
      "\n",
      "> Iteration 772/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7169]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2045]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0545]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8949]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3782]]), B=tensor([[0.3648]])\n",
      "Loss: 1.350699543952942\n",
      "\n",
      "> Iteration 773/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7168]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2045]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0544]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8950]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3780]]), B=tensor([[0.3646]])\n",
      "Loss: 1.4141472578048706\n",
      "\n",
      "> Iteration 774/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7166]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5064]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2044]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0542]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8950]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3779]]), B=tensor([[0.3645]])\n",
      "Loss: 1.4033699035644531\n",
      "\n",
      "> Iteration 775/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7165]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5062]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2044]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0541]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8951]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3777]]), B=tensor([[0.3643]])\n",
      "Loss: 1.380280613899231\n",
      "\n",
      "> Iteration 776/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7164]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5061]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2044]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0540]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8951]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3775]]), B=tensor([[0.3641]])\n",
      "Loss: 1.381584644317627\n",
      "\n",
      "> Iteration 777/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7163]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5059]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2044]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0539]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8952]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3774]]), B=tensor([[0.3639]])\n",
      "Loss: 1.3719266653060913\n",
      "\n",
      "> Iteration 778/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7161]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5058]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2043]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0538]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8952]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3772]]), B=tensor([[0.3638]])\n",
      "Loss: 1.439696192741394\n",
      "\n",
      "> Iteration 779/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7160]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5056]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2043]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0536]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8953]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3771]]), B=tensor([[0.3636]])\n",
      "Loss: 1.433497428894043\n",
      "\n",
      "> Iteration 780/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7158]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5055]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2043]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0535]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8953]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3769]]), B=tensor([[0.3634]])\n",
      "Loss: 1.3721599578857422\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 781/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7157]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5053]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2042]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0534]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8954]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3768]]), B=tensor([[0.3632]])\n",
      "Loss: 1.3603886365890503\n",
      "\n",
      "> Iteration 782/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7155]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5051]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2042]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0532]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8954]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3766]]), B=tensor([[0.3630]])\n",
      "Loss: 1.4021356105804443\n",
      "\n",
      "> Iteration 783/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7154]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5050]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2042]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0531]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8955]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3764]]), B=tensor([[0.3629]])\n",
      "Loss: 1.3559982776641846\n",
      "\n",
      "> Iteration 784/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7153]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5049]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2042]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0530]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8955]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3762]]), B=tensor([[0.3627]])\n",
      "Loss: 1.3833918571472168\n",
      "\n",
      "> Iteration 785/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7152]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5047]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2041]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0529]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8956]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3761]]), B=tensor([[0.3625]])\n",
      "Loss: 1.402639627456665\n",
      "\n",
      "> Iteration 786/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7150]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5046]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2041]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0528]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8956]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3759]]), B=tensor([[0.3624]])\n",
      "Loss: 1.3633567094802856\n",
      "\n",
      "> Iteration 787/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0151]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7149]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5044]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2041]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0526]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8957]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3758]]), B=tensor([[0.3622]])\n",
      "Loss: 1.4094581604003906\n",
      "\n",
      "> Iteration 788/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0152]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7148]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5043]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2040]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0525]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8958]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3756]]), B=tensor([[0.3620]])\n",
      "Loss: 1.3561174869537354\n",
      "\n",
      "> Iteration 789/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0152]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7147]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5041]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2040]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0524]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8958]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3755]]), B=tensor([[0.3619]])\n",
      "Loss: 1.3586996793746948\n",
      "\n",
      "> Iteration 790/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0152]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7145]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5040]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2040]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0523]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8958]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3754]]), B=tensor([[0.3617]])\n",
      "Loss: 1.3527714014053345\n",
      "\n",
      "> Iteration 791/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0152]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7144]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5039]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2040]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0522]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8959]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3752]]), B=tensor([[0.3616]])\n",
      "Loss: 1.3813648223876953\n",
      "\n",
      "> Iteration 792/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0153]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7143]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5038]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2039]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0521]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8959]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3751]]), B=tensor([[0.3614]])\n",
      "Loss: 1.396470069885254\n",
      "\n",
      "> Iteration 793/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0153]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7142]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5036]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2039]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0520]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8960]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3750]]), B=tensor([[0.3613]])\n",
      "Loss: 1.3800197839736938\n",
      "\n",
      "> Iteration 794/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0154]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7141]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5035]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2039]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0519]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8960]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3749]]), B=tensor([[0.3611]])\n",
      "Loss: 1.3929699659347534\n",
      "\n",
      "> Iteration 795/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0154]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7140]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5034]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2039]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0518]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8961]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3748]]), B=tensor([[0.3610]])\n",
      "Loss: 1.365208387374878\n",
      "\n",
      "> Iteration 796/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0154]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7138]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5032]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2038]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0517]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8961]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3747]]), B=tensor([[0.3608]])\n",
      "Loss: 1.3706773519515991\n",
      "\n",
      "> Iteration 797/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0155]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7137]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5031]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2038]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0516]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8962]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3745]]), B=tensor([[0.3607]])\n",
      "Loss: 1.3970277309417725\n",
      "\n",
      "> Iteration 798/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0155]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7136]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5030]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2038]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0515]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8962]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3744]]), B=tensor([[0.3605]])\n",
      "Loss: 1.374537467956543\n",
      "\n",
      "> Iteration 799/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0155]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7135]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5028]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2038]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0514]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8963]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3743]]), B=tensor([[0.3604]])\n",
      "Loss: 1.3837759494781494\n",
      "\n",
      "> Iteration 800/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7134]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5027]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2037]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0513]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8963]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3742]]), B=tensor([[0.3602]])\n",
      "Loss: 1.396273136138916\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 801/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7132]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5026]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2037]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0512]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8964]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3740]]), B=tensor([[0.3601]])\n",
      "Loss: 1.407411813735962\n",
      "\n",
      "> Iteration 802/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7131]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5024]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2037]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0511]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8964]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3739]]), B=tensor([[0.3599]])\n",
      "Loss: 1.4128047227859497\n",
      "\n",
      "> Iteration 803/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7130]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5023]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2037]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0509]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8965]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3737]]), B=tensor([[0.3597]])\n",
      "Loss: 1.4152021408081055\n",
      "\n",
      "> Iteration 804/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7129]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5021]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2036]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0508]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8965]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3736]]), B=tensor([[0.3596]])\n",
      "Loss: 1.4077261686325073\n",
      "\n",
      "> Iteration 805/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7127]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5020]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2036]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0507]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8966]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3734]]), B=tensor([[0.3594]])\n",
      "Loss: 1.3700716495513916\n",
      "\n",
      "> Iteration 806/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7126]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5018]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2036]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0506]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8966]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3732]]), B=tensor([[0.3592]])\n",
      "Loss: 1.3904176950454712\n",
      "\n",
      "> Iteration 807/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7125]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5017]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2035]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0505]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8967]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3731]]), B=tensor([[0.3591]])\n",
      "Loss: 1.3782134056091309\n",
      "\n",
      "> Iteration 808/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7123]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5016]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2035]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0504]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8967]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3729]]), B=tensor([[0.3589]])\n",
      "Loss: 1.4025925397872925\n",
      "\n",
      "> Iteration 809/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7122]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5014]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2035]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0502]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8968]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3727]]), B=tensor([[0.3587]])\n",
      "Loss: 1.4142248630523682\n",
      "\n",
      "> Iteration 810/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7121]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5013]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2035]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0501]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8968]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3725]]), B=tensor([[0.3585]])\n",
      "Loss: 1.4163025617599487\n",
      "\n",
      "> Iteration 811/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7119]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5011]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2034]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0500]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8969]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3723]]), B=tensor([[0.3584]])\n",
      "Loss: 1.424010992050171\n",
      "\n",
      "> Iteration 812/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7118]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5009]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2034]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0499]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8969]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3722]]), B=tensor([[0.3582]])\n",
      "Loss: 1.384675145149231\n",
      "\n",
      "> Iteration 813/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7117]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5008]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2034]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0497]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8970]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3720]]), B=tensor([[0.3580]])\n",
      "Loss: 1.3643521070480347\n",
      "\n",
      "> Iteration 814/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7115]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5006]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2033]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0496]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8971]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3719]]), B=tensor([[0.3578]])\n",
      "Loss: 1.3891801834106445\n",
      "\n",
      "> Iteration 815/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7114]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5005]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2033]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0495]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8971]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3717]]), B=tensor([[0.3577]])\n",
      "Loss: 1.3873271942138672\n",
      "\n",
      "> Iteration 816/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7113]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5004]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2033]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0494]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8972]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3716]]), B=tensor([[0.3575]])\n",
      "Loss: 1.3901948928833008\n",
      "\n",
      "> Iteration 817/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7111]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5002]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2033]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0493]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8972]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3714]]), B=tensor([[0.3573]])\n",
      "Loss: 1.3953795433044434\n",
      "\n",
      "> Iteration 818/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7110]], requires_grad=True)\n",
      "\t> c     : tensor([[0.5001]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2032]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0492]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8973]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3713]]), B=tensor([[0.3572]])\n",
      "Loss: 1.3720964193344116\n",
      "\n",
      "> Iteration 819/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7109]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4999]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2032]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0491]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8973]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3711]]), B=tensor([[0.3570]])\n",
      "Loss: 1.3945249319076538\n",
      "\n",
      "> Iteration 820/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7108]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4998]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2032]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0489]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8974]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3710]]), B=tensor([[0.3569]])\n",
      "Loss: 1.4192934036254883\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 821/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7107]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4997]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2032]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0488]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8974]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3708]]), B=tensor([[0.3567]])\n",
      "Loss: 1.4070580005645752\n",
      "\n",
      "> Iteration 822/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7105]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4995]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2031]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0487]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8974]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3707]]), B=tensor([[0.3566]])\n",
      "Loss: 1.411393404006958\n",
      "\n",
      "> Iteration 823/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7104]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4994]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2031]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0486]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8975]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3705]]), B=tensor([[0.3564]])\n",
      "Loss: 1.4043883085250854\n",
      "\n",
      "> Iteration 824/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7103]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4993]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2031]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0485]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8975]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3704]]), B=tensor([[0.3563]])\n",
      "Loss: 1.3711329698562622\n",
      "\n",
      "> Iteration 825/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7102]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4992]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2031]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0484]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8976]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3702]]), B=tensor([[0.3561]])\n",
      "Loss: 1.3911750316619873\n",
      "\n",
      "> Iteration 826/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7101]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4990]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2030]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0483]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8976]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3701]]), B=tensor([[0.3560]])\n",
      "Loss: 1.4131206274032593\n",
      "\n",
      "> Iteration 827/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7100]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4989]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2030]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0482]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8977]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3700]]), B=tensor([[0.3558]])\n",
      "Loss: 1.3768339157104492\n",
      "\n",
      "> Iteration 828/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7099]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4988]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2030]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0481]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8977]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3699]]), B=tensor([[0.3557]])\n",
      "Loss: 1.4171444177627563\n",
      "\n",
      "> Iteration 829/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7098]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4987]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2030]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0480]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8977]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3697]]), B=tensor([[0.3556]])\n",
      "Loss: 1.4044772386550903\n",
      "\n",
      "> Iteration 830/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7097]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4986]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2029]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0480]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8978]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3696]]), B=tensor([[0.3554]])\n",
      "Loss: 1.38809335231781\n",
      "\n",
      "> Iteration 831/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7096]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4985]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2029]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0479]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8978]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3695]]), B=tensor([[0.3553]])\n",
      "Loss: 1.3777178525924683\n",
      "\n",
      "> Iteration 832/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7095]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4983]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2029]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0478]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8979]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3693]]), B=tensor([[0.3552]])\n",
      "Loss: 1.4520930051803589\n",
      "\n",
      "> Iteration 833/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7094]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4982]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2029]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0477]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8979]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3692]]), B=tensor([[0.3550]])\n",
      "Loss: 1.4037657976150513\n",
      "\n",
      "> Iteration 834/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7092]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4981]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2028]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0476]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8980]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3691]]), B=tensor([[0.3549]])\n",
      "Loss: 1.408545732498169\n",
      "\n",
      "> Iteration 835/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7091]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4979]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2028]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0475]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8980]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3690]]), B=tensor([[0.3547]])\n",
      "Loss: 1.37480628490448\n",
      "\n",
      "> Iteration 836/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7090]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4978]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2028]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0474]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8981]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3688]]), B=tensor([[0.3546]])\n",
      "Loss: 1.4040418863296509\n",
      "\n",
      "> Iteration 837/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7089]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4977]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2028]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0473]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8981]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3687]]), B=tensor([[0.3544]])\n",
      "Loss: 1.4449936151504517\n",
      "\n",
      "> Iteration 838/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7088]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4976]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2027]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0472]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8982]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3686]]), B=tensor([[0.3543]])\n",
      "Loss: 1.414160966873169\n",
      "\n",
      "> Iteration 839/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7087]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4974]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2027]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0471]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8982]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3685]]), B=tensor([[0.3541]])\n",
      "Loss: 1.3869128227233887\n",
      "\n",
      "> Iteration 840/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7086]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4973]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2027]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0470]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8982]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3683]]), B=tensor([[0.3540]])\n",
      "Loss: 1.3942105770111084\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 841/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7084]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4972]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2027]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0469]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8983]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3682]]), B=tensor([[0.3538]])\n",
      "Loss: 1.3925831317901611\n",
      "\n",
      "> Iteration 842/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7083]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4970]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2026]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0467]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8983]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3680]]), B=tensor([[0.3537]])\n",
      "Loss: 1.3914183378219604\n",
      "\n",
      "> Iteration 843/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7082]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4969]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2026]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0466]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8984]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3679]]), B=tensor([[0.3535]])\n",
      "Loss: 1.3652256727218628\n",
      "\n",
      "> Iteration 844/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7081]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4968]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2026]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0465]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8984]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3678]]), B=tensor([[0.3534]])\n",
      "Loss: 1.3714367151260376\n",
      "\n",
      "> Iteration 845/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7080]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4967]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2026]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0464]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8985]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3676]]), B=tensor([[0.3532]])\n",
      "Loss: 1.3836175203323364\n",
      "\n",
      "> Iteration 846/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7079]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4965]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2025]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0463]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8985]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3675]]), B=tensor([[0.3531]])\n",
      "Loss: 1.3798173666000366\n",
      "\n",
      "> Iteration 847/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7077]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4964]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2025]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0462]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8986]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3673]]), B=tensor([[0.3529]])\n",
      "Loss: 1.378164291381836\n",
      "\n",
      "> Iteration 848/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7076]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4963]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2025]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0461]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8986]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3672]]), B=tensor([[0.3528]])\n",
      "Loss: 1.4139134883880615\n",
      "\n",
      "> Iteration 849/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7075]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4961]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2025]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0460]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8987]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3670]]), B=tensor([[0.3526]])\n",
      "Loss: 1.3941041231155396\n",
      "\n",
      "> Iteration 850/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7074]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4960]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2024]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0459]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8987]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3668]]), B=tensor([[0.3525]])\n",
      "Loss: 1.3948752880096436\n",
      "\n",
      "> Iteration 851/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7072]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4958]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2024]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0458]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8988]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3667]]), B=tensor([[0.3523]])\n",
      "Loss: 1.4028741121292114\n",
      "\n",
      "> Iteration 852/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7071]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4957]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2024]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0457]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8988]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3665]]), B=tensor([[0.3521]])\n",
      "Loss: 1.3630266189575195\n",
      "\n",
      "> Iteration 853/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7070]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4956]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2023]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0456]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8989]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3664]]), B=tensor([[0.3520]])\n",
      "Loss: 1.3648654222488403\n",
      "\n",
      "> Iteration 854/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7069]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4954]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2023]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0455]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8989]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3662]]), B=tensor([[0.3518]])\n",
      "Loss: 1.3889743089675903\n",
      "\n",
      "> Iteration 855/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7068]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4953]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2023]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0454]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8989]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3661]]), B=tensor([[0.3517]])\n",
      "Loss: 1.436771273612976\n",
      "\n",
      "> Iteration 856/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7067]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4952]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2023]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0453]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8990]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3659]]), B=tensor([[0.3515]])\n",
      "Loss: 1.3945176601409912\n",
      "\n",
      "> Iteration 857/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7066]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4951]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2022]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0451]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8990]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3657]]), B=tensor([[0.3514]])\n",
      "Loss: 1.4591233730316162\n",
      "\n",
      "> Iteration 858/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7064]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4949]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2022]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0450]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8991]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3656]]), B=tensor([[0.3512]])\n",
      "Loss: 1.378527045249939\n",
      "\n",
      "> Iteration 859/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7063]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4948]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2022]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0449]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8991]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3654]]), B=tensor([[0.3511]])\n",
      "Loss: 1.4175933599472046\n",
      "\n",
      "> Iteration 860/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7062]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4947]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2022]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0448]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8992]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3652]]), B=tensor([[0.3509]])\n",
      "Loss: 1.4174623489379883\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 861/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7061]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4945]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2021]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0447]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8992]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3651]]), B=tensor([[0.3508]])\n",
      "Loss: 1.3835618495941162\n",
      "\n",
      "> Iteration 862/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7060]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4944]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2021]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0446]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8993]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3649]]), B=tensor([[0.3506]])\n",
      "Loss: 1.381938099861145\n",
      "\n",
      "> Iteration 863/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7059]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4943]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2021]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0445]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8993]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3648]]), B=tensor([[0.3505]])\n",
      "Loss: 1.3963301181793213\n",
      "\n",
      "> Iteration 864/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7058]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4942]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2021]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0444]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8993]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3646]]), B=tensor([[0.3504]])\n",
      "Loss: 1.3936891555786133\n",
      "\n",
      "> Iteration 865/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7057]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4940]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2020]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0443]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8994]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3645]]), B=tensor([[0.3502]])\n",
      "Loss: 1.3573851585388184\n",
      "\n",
      "> Iteration 866/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7056]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4939]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2020]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0442]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8994]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3643]]), B=tensor([[0.3501]])\n",
      "Loss: 1.381343960762024\n",
      "\n",
      "> Iteration 867/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7055]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4938]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2020]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0441]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8995]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3642]]), B=tensor([[0.3500]])\n",
      "Loss: 1.3949546813964844\n",
      "\n",
      "> Iteration 868/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7054]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4937]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2020]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0440]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8995]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3641]]), B=tensor([[0.3498]])\n",
      "Loss: 1.4032434225082397\n",
      "\n",
      "> Iteration 869/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7053]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4936]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2020]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0440]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8996]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3640]]), B=tensor([[0.3497]])\n",
      "Loss: 1.3596335649490356\n",
      "\n",
      "> Iteration 870/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7052]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4935]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2019]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0439]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8996]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3638]]), B=tensor([[0.3496]])\n",
      "Loss: 1.4096280336380005\n",
      "\n",
      "> Iteration 871/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7050]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4934]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2019]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0438]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8996]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3637]]), B=tensor([[0.3494]])\n",
      "Loss: 1.3854273557662964\n",
      "\n",
      "> Iteration 872/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7049]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4932]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2019]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0437]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8997]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3636]]), B=tensor([[0.3493]])\n",
      "Loss: 1.3675719499588013\n",
      "\n",
      "> Iteration 873/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7048]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4931]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2019]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0436]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8997]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3635]]), B=tensor([[0.3492]])\n",
      "Loss: 1.4454270601272583\n",
      "\n",
      "> Iteration 874/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7047]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4930]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2018]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0435]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8998]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3633]]), B=tensor([[0.3490]])\n",
      "Loss: 1.3821687698364258\n",
      "\n",
      "> Iteration 875/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7046]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4929]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2018]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0434]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8998]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3632]]), B=tensor([[0.3489]])\n",
      "Loss: 1.344625473022461\n",
      "\n",
      "> Iteration 876/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7045]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4928]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2018]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0433]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8998]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3631]]), B=tensor([[0.3488]])\n",
      "Loss: 1.383370280265808\n",
      "\n",
      "> Iteration 877/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7044]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4927]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2018]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0432]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8999]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3630]]), B=tensor([[0.3486]])\n",
      "Loss: 1.4147868156433105\n",
      "\n",
      "> Iteration 878/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7043]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4925]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2017]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0431]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.8999]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3628]]), B=tensor([[0.3485]])\n",
      "Loss: 1.3464760780334473\n",
      "\n",
      "> Iteration 879/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7042]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4924]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2017]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0430]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9000]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3627]]), B=tensor([[0.3484]])\n",
      "Loss: 1.3815302848815918\n",
      "\n",
      "> Iteration 880/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7041]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4923]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2017]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0429]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9000]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3626]]), B=tensor([[0.3482]])\n",
      "Loss: 1.4272419214248657\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 881/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7040]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4922]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2017]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0428]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9000]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3625]]), B=tensor([[0.3481]])\n",
      "Loss: 1.3785085678100586\n",
      "\n",
      "> Iteration 882/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7039]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4921]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2017]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0428]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9001]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3623]]), B=tensor([[0.3479]])\n",
      "Loss: 1.4077603816986084\n",
      "\n",
      "> Iteration 883/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7038]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4919]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2016]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0426]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9001]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3622]]), B=tensor([[0.3478]])\n",
      "Loss: 1.3971219062805176\n",
      "\n",
      "> Iteration 884/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7037]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4918]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2016]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0426]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9002]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3621]]), B=tensor([[0.3477]])\n",
      "Loss: 1.3729472160339355\n",
      "\n",
      "> Iteration 885/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7036]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4917]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2016]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0424]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9002]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3619]]), B=tensor([[0.3475]])\n",
      "Loss: 1.397680640220642\n",
      "\n",
      "> Iteration 886/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7034]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4916]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2016]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0423]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9003]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3617]]), B=tensor([[0.3474]])\n",
      "Loss: 1.381689429283142\n",
      "\n",
      "> Iteration 887/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7033]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4914]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2015]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0422]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9003]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3615]]), B=tensor([[0.3472]])\n",
      "Loss: 1.3966577053070068\n",
      "\n",
      "> Iteration 888/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7032]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4913]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2015]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0421]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9003]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3614]]), B=tensor([[0.3471]])\n",
      "Loss: 1.3794996738433838\n",
      "\n",
      "> Iteration 889/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7031]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4912]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2015]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0420]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9004]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3612]]), B=tensor([[0.3469]])\n",
      "Loss: 1.3748798370361328\n",
      "\n",
      "> Iteration 890/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7030]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4911]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2015]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0419]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9004]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3610]]), B=tensor([[0.3468]])\n",
      "Loss: 1.4221786260604858\n",
      "\n",
      "> Iteration 891/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7029]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4909]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2014]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0418]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9005]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3609]]), B=tensor([[0.3467]])\n",
      "Loss: 1.3381866216659546\n",
      "\n",
      "> Iteration 892/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7028]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4908]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2014]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0417]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9005]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3608]]), B=tensor([[0.3465]])\n",
      "Loss: 1.3882877826690674\n",
      "\n",
      "> Iteration 893/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7027]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4907]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2014]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0416]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9006]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3606]]), B=tensor([[0.3464]])\n",
      "Loss: 1.4327976703643799\n",
      "\n",
      "> Iteration 894/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7026]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4906]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2014]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0415]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9006]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3604]]), B=tensor([[0.3462]])\n",
      "Loss: 1.401705265045166\n",
      "\n",
      "> Iteration 895/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7025]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4904]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2013]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0414]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9006]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3602]]), B=tensor([[0.3461]])\n",
      "Loss: 1.4220502376556396\n",
      "\n",
      "> Iteration 896/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7024]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4903]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2013]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0413]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9007]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3601]]), B=tensor([[0.3459]])\n",
      "Loss: 1.3711464405059814\n",
      "\n",
      "> Iteration 897/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7022]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4902]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2013]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0412]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9007]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3599]]), B=tensor([[0.3458]])\n",
      "Loss: 1.358074426651001\n",
      "\n",
      "> Iteration 898/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7021]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4901]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2013]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0411]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9008]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3598]]), B=tensor([[0.3457]])\n",
      "Loss: 1.3914607763290405\n",
      "\n",
      "> Iteration 899/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7020]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4900]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2012]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0410]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9008]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3597]]), B=tensor([[0.3455]])\n",
      "Loss: 1.3704279661178589\n",
      "\n",
      "> Iteration 900/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7019]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4898]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2012]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0409]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9009]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3595]]), B=tensor([[0.3454]])\n",
      "Loss: 1.3874340057373047\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 901/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7018]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4897]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2012]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0408]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9009]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3593]]), B=tensor([[0.3452]])\n",
      "Loss: 1.3957467079162598\n",
      "\n",
      "> Iteration 902/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7017]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4896]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2012]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0407]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9009]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3592]]), B=tensor([[0.3451]])\n",
      "Loss: 1.350017786026001\n",
      "\n",
      "> Iteration 903/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7016]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4895]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2012]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0406]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9010]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3591]]), B=tensor([[0.3450]])\n",
      "Loss: 1.3463081121444702\n",
      "\n",
      "> Iteration 904/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7015]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4894]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2011]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0406]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9010]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3590]]), B=tensor([[0.3449]])\n",
      "Loss: 1.3531101942062378\n",
      "\n",
      "> Iteration 905/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7014]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4893]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2011]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0405]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9011]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3589]]), B=tensor([[0.3447]])\n",
      "Loss: 1.4045969247817993\n",
      "\n",
      "> Iteration 906/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7013]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4891]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2011]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0404]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9011]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3588]]), B=tensor([[0.3446]])\n",
      "Loss: 1.4161993265151978\n",
      "\n",
      "> Iteration 907/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7012]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4890]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2011]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0403]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9011]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3586]]), B=tensor([[0.3445]])\n",
      "Loss: 1.4070135354995728\n",
      "\n",
      "> Iteration 908/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7011]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4889]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2010]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0402]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9012]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3585]]), B=tensor([[0.3443]])\n",
      "Loss: 1.340224027633667\n",
      "\n",
      "> Iteration 909/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7010]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4888]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2010]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0401]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9012]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3584]]), B=tensor([[0.3442]])\n",
      "Loss: 1.3552238941192627\n",
      "\n",
      "> Iteration 910/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7009]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4887]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2010]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0400]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9012]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3583]]), B=tensor([[0.3441]])\n",
      "Loss: 1.3866970539093018\n",
      "\n",
      "> Iteration 911/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7008]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4886]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2010]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0400]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9013]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3582]]), B=tensor([[0.3440]])\n",
      "Loss: 1.3784822225570679\n",
      "\n",
      "> Iteration 912/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7007]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4885]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2010]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0399]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9013]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3581]]), B=tensor([[0.3439]])\n",
      "Loss: 1.3768303394317627\n",
      "\n",
      "> Iteration 913/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7006]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4884]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2009]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0398]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9014]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3579]]), B=tensor([[0.3437]])\n",
      "Loss: 1.4056239128112793\n",
      "\n",
      "> Iteration 914/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7005]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4883]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2009]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0397]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9014]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3578]]), B=tensor([[0.3436]])\n",
      "Loss: 1.4105228185653687\n",
      "\n",
      "> Iteration 915/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7004]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4882]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2009]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0396]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9014]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3577]]), B=tensor([[0.3435]])\n",
      "Loss: 1.3378171920776367\n",
      "\n",
      "> Iteration 916/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7003]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4881]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2009]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0395]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9015]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3576]]), B=tensor([[0.3434]])\n",
      "Loss: 1.3705408573150635\n",
      "\n",
      "> Iteration 917/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7003]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4880]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2009]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0395]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9015]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3575]]), B=tensor([[0.3433]])\n",
      "Loss: 1.3735997676849365\n",
      "\n",
      "> Iteration 918/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7002]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4878]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2008]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0394]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9015]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3574]]), B=tensor([[0.3431]])\n",
      "Loss: 1.3910951614379883\n",
      "\n",
      "> Iteration 919/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7001]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4877]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2008]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0393]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9016]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3573]]), B=tensor([[0.3430]])\n",
      "Loss: 1.3468698263168335\n",
      "\n",
      "> Iteration 920/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.7000]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4876]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2008]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0392]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9016]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3571]]), B=tensor([[0.3429]])\n",
      "Loss: 1.389367699623108\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 921/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6999]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4875]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2008]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0391]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9017]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3570]]), B=tensor([[0.3428]])\n",
      "Loss: 1.3989579677581787\n",
      "\n",
      "> Iteration 922/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6998]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4874]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2008]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0390]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9017]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3569]]), B=tensor([[0.3427]])\n",
      "Loss: 1.3929834365844727\n",
      "\n",
      "> Iteration 923/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6997]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4873]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2007]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0390]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9017]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3568]]), B=tensor([[0.3425]])\n",
      "Loss: 1.4005341529846191\n",
      "\n",
      "> Iteration 924/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6996]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4872]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2007]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0389]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9018]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3567]]), B=tensor([[0.3424]])\n",
      "Loss: 1.3839706182479858\n",
      "\n",
      "> Iteration 925/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6995]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4871]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2007]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0388]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9018]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3565]]), B=tensor([[0.3423]])\n",
      "Loss: 1.3881072998046875\n",
      "\n",
      "> Iteration 926/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6994]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4870]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2007]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0387]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9018]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3564]]), B=tensor([[0.3422]])\n",
      "Loss: 1.3954415321350098\n",
      "\n",
      "> Iteration 927/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6993]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4869]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2007]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0386]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9019]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3563]]), B=tensor([[0.3421]])\n",
      "Loss: 1.3750838041305542\n",
      "\n",
      "> Iteration 928/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6992]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4868]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2006]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0385]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9019]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3562]]), B=tensor([[0.3419]])\n",
      "Loss: 1.3894842863082886\n",
      "\n",
      "> Iteration 929/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6991]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4867]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2006]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0385]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9019]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3561]]), B=tensor([[0.3418]])\n",
      "Loss: 1.391101598739624\n",
      "\n",
      "> Iteration 930/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6990]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4866]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2006]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0384]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9020]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3559]]), B=tensor([[0.3417]])\n",
      "Loss: 1.3784980773925781\n",
      "\n",
      "> Iteration 931/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6989]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4865]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2006]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0383]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9020]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3558]]), B=tensor([[0.3416]])\n",
      "Loss: 1.3837275505065918\n",
      "\n",
      "> Iteration 932/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6988]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4864]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2006]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0382]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9021]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3556]]), B=tensor([[0.3414]])\n",
      "Loss: 1.3933309316635132\n",
      "\n",
      "> Iteration 933/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6988]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4862]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2005]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0381]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9021]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3555]]), B=tensor([[0.3413]])\n",
      "Loss: 1.3855479955673218\n",
      "\n",
      "> Iteration 934/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6987]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4861]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2005]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0380]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9021]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3553]]), B=tensor([[0.3412]])\n",
      "Loss: 1.386574149131775\n",
      "\n",
      "> Iteration 935/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6986]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4860]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2005]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0379]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9022]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3552]]), B=tensor([[0.3410]])\n",
      "Loss: 1.4080266952514648\n",
      "\n",
      "> Iteration 936/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6985]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4859]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2005]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0378]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9022]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3550]]), B=tensor([[0.3409]])\n",
      "Loss: 1.3924181461334229\n",
      "\n",
      "> Iteration 937/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6983]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4858]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2004]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0377]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9022]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3549]]), B=tensor([[0.3408]])\n",
      "Loss: 1.4190796613693237\n",
      "\n",
      "> Iteration 938/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6982]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4857]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2004]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0376]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9023]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3547]]), B=tensor([[0.3406]])\n",
      "Loss: 1.3931795358657837\n",
      "\n",
      "> Iteration 939/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0156]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6981]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4856]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2004]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0375]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9023]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3546]]), B=tensor([[0.3405]])\n",
      "Loss: 1.3960336446762085\n",
      "\n",
      "> Iteration 940/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6980]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4854]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2004]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0374]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9024]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3545]]), B=tensor([[0.3404]])\n",
      "Loss: 1.4104145765304565\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 941/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6979]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4853]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2004]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0374]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9024]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3544]]), B=tensor([[0.3402]])\n",
      "Loss: 1.3860015869140625\n",
      "\n",
      "> Iteration 942/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6978]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4852]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2003]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0373]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9024]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3543]]), B=tensor([[0.3401]])\n",
      "Loss: 1.3396451473236084\n",
      "\n",
      "> Iteration 943/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6977]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4851]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2003]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0372]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9025]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3542]]), B=tensor([[0.3400]])\n",
      "Loss: 1.3774564266204834\n",
      "\n",
      "> Iteration 944/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6976]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4850]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2003]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0371]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9025]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3541]]), B=tensor([[0.3399]])\n",
      "Loss: 1.4005675315856934\n",
      "\n",
      "> Iteration 945/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6975]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4849]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2003]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0370]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9026]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3540]]), B=tensor([[0.3398]])\n",
      "Loss: 1.3953205347061157\n",
      "\n",
      "> Iteration 946/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6975]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4848]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2002]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0369]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9026]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3538]]), B=tensor([[0.3396]])\n",
      "Loss: 1.3788483142852783\n",
      "\n",
      "> Iteration 947/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0157]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6974]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4847]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2002]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0369]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9026]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3537]]), B=tensor([[0.3395]])\n",
      "Loss: 1.4030572175979614\n",
      "\n",
      "> Iteration 948/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6973]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4846]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2002]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0368]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9027]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3536]]), B=tensor([[0.3394]])\n",
      "Loss: 1.4057066440582275\n",
      "\n",
      "> Iteration 949/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6972]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4844]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2002]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0367]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9027]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3535]]), B=tensor([[0.3393]])\n",
      "Loss: 1.3957788944244385\n",
      "\n",
      "> Iteration 950/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6971]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4843]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2002]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0366]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9027]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3535]]), B=tensor([[0.3391]])\n",
      "Loss: 1.419477939605713\n",
      "\n",
      "> Iteration 951/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6969]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4842]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2001]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0365]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9028]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3533]]), B=tensor([[0.3390]])\n",
      "Loss: 1.4374197721481323\n",
      "\n",
      "> Iteration 952/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0159]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6968]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4841]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2001]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0364]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9028]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3532]]), B=tensor([[0.3389]])\n",
      "Loss: 1.3481950759887695\n",
      "\n",
      "> Iteration 953/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6967]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4840]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2001]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0363]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9029]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3532]]), B=tensor([[0.3387]])\n",
      "Loss: 1.3685405254364014\n",
      "\n",
      "> Iteration 954/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6966]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4839]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2001]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0363]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9029]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3531]]), B=tensor([[0.3386]])\n",
      "Loss: 1.376120686531067\n",
      "\n",
      "> Iteration 955/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6965]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4837]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2000]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0362]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9029]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3530]]), B=tensor([[0.3385]])\n",
      "Loss: 1.423765778541565\n",
      "\n",
      "> Iteration 956/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6964]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4836]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2000]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0361]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9030]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3529]]), B=tensor([[0.3384]])\n",
      "Loss: 1.4142264127731323\n",
      "\n",
      "> Iteration 957/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6963]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4835]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2000]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0360]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9030]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3528]]), B=tensor([[0.3383]])\n",
      "Loss: 1.3733998537063599\n",
      "\n",
      "> Iteration 958/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6962]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4834]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2000]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0359]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9031]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3527]]), B=tensor([[0.3381]])\n",
      "Loss: 1.4049896001815796\n",
      "\n",
      "> Iteration 959/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6961]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4833]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.2000]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0358]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9031]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3525]]), B=tensor([[0.3380]])\n",
      "Loss: 1.3831843137741089\n",
      "\n",
      "> Iteration 960/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6960]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4832]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1999]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0357]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9031]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3524]]), B=tensor([[0.3379]])\n",
      "Loss: 1.3862040042877197\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 961/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6959]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4831]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1999]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0357]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9032]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3523]]), B=tensor([[0.3377]])\n",
      "Loss: 1.377703070640564\n",
      "\n",
      "> Iteration 962/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6958]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4830]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1999]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0356]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9032]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3522]]), B=tensor([[0.3376]])\n",
      "Loss: 1.3791829347610474\n",
      "\n",
      "> Iteration 963/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6957]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4828]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1999]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0355]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9033]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3521]]), B=tensor([[0.3375]])\n",
      "Loss: 1.3839260339736938\n",
      "\n",
      "> Iteration 964/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0161]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6956]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4827]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1998]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0354]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9033]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3520]]), B=tensor([[0.3374]])\n",
      "Loss: 1.4083138704299927\n",
      "\n",
      "> Iteration 965/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0162]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6955]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4826]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1998]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0353]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9033]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3519]]), B=tensor([[0.3372]])\n",
      "Loss: 1.3982990980148315\n",
      "\n",
      "> Iteration 966/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0162]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6954]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4825]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1998]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0352]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9034]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3518]]), B=tensor([[0.3371]])\n",
      "Loss: 1.4084725379943848\n",
      "\n",
      "> Iteration 967/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0162]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6953]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4824]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1998]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0351]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9034]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3516]]), B=tensor([[0.3370]])\n",
      "Loss: 1.3894349336624146\n",
      "\n",
      "> Iteration 968/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0163]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6952]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4823]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1997]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0351]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9035]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3515]]), B=tensor([[0.3369]])\n",
      "Loss: 1.3711291551589966\n",
      "\n",
      "> Iteration 969/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0163]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6951]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4822]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1997]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0350]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9035]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3515]]), B=tensor([[0.3367]])\n",
      "Loss: 1.3517589569091797\n",
      "\n",
      "> Iteration 970/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0163]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6950]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4820]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1997]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0349]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9035]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3514]]), B=tensor([[0.3366]])\n",
      "Loss: 1.419095754623413\n",
      "\n",
      "> Iteration 971/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0164]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6950]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4819]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1997]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0348]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9036]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3513]]), B=tensor([[0.3365]])\n",
      "Loss: 1.3792415857315063\n",
      "\n",
      "> Iteration 972/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0164]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6949]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4818]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1997]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0348]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9036]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3512]]), B=tensor([[0.3364]])\n",
      "Loss: 1.3435001373291016\n",
      "\n",
      "> Iteration 973/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0165]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6948]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4818]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1996]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0347]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9036]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3512]]), B=tensor([[0.3363]])\n",
      "Loss: 1.3448907136917114\n",
      "\n",
      "> Iteration 974/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0165]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6947]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4817]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1996]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0346]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9037]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3512]]), B=tensor([[0.3362]])\n",
      "Loss: 1.3852741718292236\n",
      "\n",
      "> Iteration 975/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0166]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6946]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4816]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1996]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0346]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9037]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3511]]), B=tensor([[0.3361]])\n",
      "Loss: 1.4059914350509644\n",
      "\n",
      "> Iteration 976/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0167]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6945]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4815]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1996]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0345]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9037]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3511]]), B=tensor([[0.3360]])\n",
      "Loss: 1.4146668910980225\n",
      "\n",
      "> Iteration 977/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0167]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6945]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4814]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1996]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0344]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9038]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.3359]])\n",
      "Loss: 1.394834280014038\n",
      "\n",
      "> Iteration 978/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0168]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6944]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4813]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1995]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0344]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9038]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.3358]])\n",
      "Loss: 1.3595445156097412\n",
      "\n",
      "> Iteration 979/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0168]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6943]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4812]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1995]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0343]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9038]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3509]]), B=tensor([[0.3357]])\n",
      "Loss: 1.402721881866455\n",
      "\n",
      "> Iteration 980/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0169]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6942]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4811]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1995]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0342]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9039]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3508]]), B=tensor([[0.3356]])\n",
      "Loss: 1.3723869323730469\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 981/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0169]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6941]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4810]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1995]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0342]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9039]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3508]]), B=tensor([[0.3355]])\n",
      "Loss: 1.3663076162338257\n",
      "\n",
      "> Iteration 982/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0170]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6940]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4809]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1995]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0341]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9039]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3507]]), B=tensor([[0.3354]])\n",
      "Loss: 1.3769474029541016\n",
      "\n",
      "> Iteration 983/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0170]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6940]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4808]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1995]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0341]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9039]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3506]]), B=tensor([[0.3353]])\n",
      "Loss: 1.3758571147918701\n",
      "\n",
      "> Iteration 984/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0170]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6939]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4807]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1994]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0340]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9040]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3506]]), B=tensor([[0.3352]])\n",
      "Loss: 1.3608089685440063\n",
      "\n",
      "> Iteration 985/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0170]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6938]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4806]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1994]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0339]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9040]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3505]]), B=tensor([[0.3351]])\n",
      "Loss: 1.398398518562317\n",
      "\n",
      "> Iteration 986/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0171]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6937]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4806]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1994]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0339]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9040]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3505]]), B=tensor([[0.3350]])\n",
      "Loss: 1.3677899837493896\n",
      "\n",
      "> Iteration 987/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0171]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6937]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4805]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1994]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0338]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9041]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3504]]), B=tensor([[0.3349]])\n",
      "Loss: 1.3480250835418701\n",
      "\n",
      "> Iteration 988/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0172]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6936]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4804]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1994]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0338]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9041]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3504]]), B=tensor([[0.3349]])\n",
      "Loss: 1.3527891635894775\n",
      "\n",
      "> Iteration 989/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0173]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6935]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4803]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1993]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0337]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9041]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3504]]), B=tensor([[0.3348]])\n",
      "Loss: 1.3726513385772705\n",
      "\n",
      "> Iteration 990/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0173]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6935]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4803]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1993]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0337]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9041]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3504]]), B=tensor([[0.3347]])\n",
      "Loss: 1.348118543624878\n",
      "\n",
      "> Iteration 991/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0174]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6934]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4802]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1993]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0336]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9042]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3504]]), B=tensor([[0.3346]])\n",
      "Loss: 1.4232494831085205\n",
      "\n",
      "> Iteration 992/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0175]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6933]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4801]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1993]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0336]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9042]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3503]]), B=tensor([[0.3346]])\n",
      "Loss: 1.3996813297271729\n",
      "\n",
      "> Iteration 993/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0175]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6933]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4800]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1993]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0335]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9042]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3503]]), B=tensor([[0.3345]])\n",
      "Loss: 1.3529194593429565\n",
      "\n",
      "> Iteration 994/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0176]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6932]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4799]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1993]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0335]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9043]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3502]]), B=tensor([[0.3344]])\n",
      "Loss: 1.3939878940582275\n",
      "\n",
      "> Iteration 995/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0176]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6931]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4799]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1992]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0334]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9043]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3502]]), B=tensor([[0.3343]])\n",
      "Loss: 1.45223069190979\n",
      "\n",
      "> Iteration 996/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0176]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6930]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4798]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1992]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0333]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9043]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3501]]), B=tensor([[0.3342]])\n",
      "Loss: 1.4205292463302612\n",
      "\n",
      "> Iteration 997/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0176]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6930]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4797]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1992]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0333]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9043]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3500]]), B=tensor([[0.3341]])\n",
      "Loss: 1.3843601942062378\n",
      "\n",
      "> Iteration 998/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6929]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4796]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1992]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0332]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9044]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3500]]), B=tensor([[0.3340]])\n",
      "Loss: 1.372391700744629\n",
      "\n",
      "> Iteration 999/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6928]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4795]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1992]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0332]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9044]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3499]]), B=tensor([[0.3339]])\n",
      "Loss: 1.3666282892227173\n",
      "\n",
      "> Iteration 1000/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6927]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4794]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0331]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9044]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3498]]), B=tensor([[0.3338]])\n",
      "Loss: 1.400689959526062\n",
      "\n",
      "--- Update learning rate: [0.0002] ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1001/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6927]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4794]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0331]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9045]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3498]]), B=tensor([[0.3337]])\n",
      "Loss: 1.3981587886810303\n",
      "\n",
      "> Iteration 1002/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6927]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4793]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0330]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9045]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3497]]), B=tensor([[0.3337]])\n",
      "Loss: 1.3913010358810425\n",
      "\n",
      "> Iteration 1003/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6926]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4793]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0330]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9045]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3497]]), B=tensor([[0.3337]])\n",
      "Loss: 1.3649165630340576\n",
      "\n",
      "> Iteration 1004/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6926]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4793]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0330]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9045]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3496]]), B=tensor([[0.3336]])\n",
      "Loss: 1.3959051370620728\n",
      "\n",
      "> Iteration 1005/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6926]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4792]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0329]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9045]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3496]]), B=tensor([[0.3336]])\n",
      "Loss: 1.4492204189300537\n",
      "\n",
      "> Iteration 1006/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6925]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4792]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0329]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9045]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3496]]), B=tensor([[0.3335]])\n",
      "Loss: 1.387826681137085\n",
      "\n",
      "> Iteration 1007/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6925]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4791]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0329]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9045]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3495]]), B=tensor([[0.3335]])\n",
      "Loss: 1.4015517234802246\n",
      "\n",
      "> Iteration 1008/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0177]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6924]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4791]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0329]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9045]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3495]]), B=tensor([[0.3334]])\n",
      "Loss: 1.3683046102523804\n",
      "\n",
      "> Iteration 1009/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6924]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4791]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0328]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9046]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3495]]), B=tensor([[0.3334]])\n",
      "Loss: 1.3821696043014526\n",
      "\n",
      "> Iteration 1010/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6924]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4790]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0328]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9046]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3494]]), B=tensor([[0.3334]])\n",
      "Loss: 1.3842506408691406\n",
      "\n",
      "> Iteration 1011/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6923]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4790]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1991]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0328]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9046]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3494]]), B=tensor([[0.3333]])\n",
      "Loss: 1.4203264713287354\n",
      "\n",
      "> Iteration 1012/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6923]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4789]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0327]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9046]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3493]]), B=tensor([[0.3333]])\n",
      "Loss: 1.3652021884918213\n",
      "\n",
      "> Iteration 1013/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6923]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4789]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0327]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9046]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3493]]), B=tensor([[0.3332]])\n",
      "Loss: 1.377312183380127\n",
      "\n",
      "> Iteration 1014/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6922]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4789]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0327]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9046]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3493]]), B=tensor([[0.3332]])\n",
      "Loss: 1.359061360359192\n",
      "\n",
      "> Iteration 1015/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6922]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4788]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0327]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9046]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3493]]), B=tensor([[0.3331]])\n",
      "Loss: 1.3676584959030151\n",
      "\n",
      "> Iteration 1016/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6922]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4788]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0326]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9047]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3492]]), B=tensor([[0.3331]])\n",
      "Loss: 1.3845837116241455\n",
      "\n",
      "> Iteration 1017/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6921]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4788]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0326]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9047]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3492]]), B=tensor([[0.3331]])\n",
      "Loss: 1.3837920427322388\n",
      "\n",
      "> Iteration 1018/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6921]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4787]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0326]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9047]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3492]]), B=tensor([[0.3330]])\n",
      "Loss: 1.3978211879730225\n",
      "\n",
      "> Iteration 1019/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6921]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4787]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0326]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9047]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3491]]), B=tensor([[0.3330]])\n",
      "Loss: 1.3962349891662598\n",
      "\n",
      "> Iteration 1020/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6920]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4786]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0325]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9047]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3491]]), B=tensor([[0.3329]])\n",
      "Loss: 1.4098961353302002\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1021/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6920]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4786]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0325]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9047]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3490]]), B=tensor([[0.3329]])\n",
      "Loss: 1.3892921209335327\n",
      "\n",
      "> Iteration 1022/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6920]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4786]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0325]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9047]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3490]]), B=tensor([[0.3328]])\n",
      "Loss: 1.3938771486282349\n",
      "\n",
      "> Iteration 1023/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6919]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4785]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1990]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0324]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9047]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3490]]), B=tensor([[0.3328]])\n",
      "Loss: 1.381515622138977\n",
      "\n",
      "> Iteration 1024/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6919]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4785]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0324]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9048]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3489]]), B=tensor([[0.3328]])\n",
      "Loss: 1.3831202983856201\n",
      "\n",
      "> Iteration 1025/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6919]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4784]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0324]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9048]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3489]]), B=tensor([[0.3327]])\n",
      "Loss: 1.4471262693405151\n",
      "\n",
      "> Iteration 1026/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6918]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4784]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0323]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9048]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3488]]), B=tensor([[0.3327]])\n",
      "Loss: 1.3717185258865356\n",
      "\n",
      "> Iteration 1027/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6918]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4783]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0323]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9048]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3488]]), B=tensor([[0.3326]])\n",
      "Loss: 1.4033504724502563\n",
      "\n",
      "> Iteration 1028/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0178]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6918]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4783]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0323]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9048]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3487]]), B=tensor([[0.3326]])\n",
      "Loss: 1.398604154586792\n",
      "\n",
      "> Iteration 1029/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6917]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4783]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0323]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9048]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3487]]), B=tensor([[0.3325]])\n",
      "Loss: 1.400278925895691\n",
      "\n",
      "> Iteration 1030/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6917]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4782]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0322]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9048]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3487]]), B=tensor([[0.3325]])\n",
      "Loss: 1.3940733671188354\n",
      "\n",
      "> Iteration 1031/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6916]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4782]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0322]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9049]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3486]]), B=tensor([[0.3324]])\n",
      "Loss: 1.4105010032653809\n",
      "\n",
      "> Iteration 1032/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6916]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4781]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0322]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9049]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3486]]), B=tensor([[0.3324]])\n",
      "Loss: 1.4480116367340088\n",
      "\n",
      "> Iteration 1033/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6916]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4781]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0321]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9049]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3486]]), B=tensor([[0.3323]])\n",
      "Loss: 1.3709156513214111\n",
      "\n",
      "> Iteration 1034/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6915]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4781]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1989]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0321]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9049]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3485]]), B=tensor([[0.3323]])\n",
      "Loss: 1.3892078399658203\n",
      "\n",
      "> Iteration 1035/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6915]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4780]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0321]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9049]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3485]]), B=tensor([[0.3323]])\n",
      "Loss: 1.3549442291259766\n",
      "\n",
      "> Iteration 1036/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6915]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4780]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0321]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9049]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3485]]), B=tensor([[0.3322]])\n",
      "Loss: 1.3966343402862549\n",
      "\n",
      "> Iteration 1037/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6914]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4779]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0320]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9049]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3484]]), B=tensor([[0.3322]])\n",
      "Loss: 1.3685157299041748\n",
      "\n",
      "> Iteration 1038/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6914]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4779]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0320]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9050]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3484]]), B=tensor([[0.3321]])\n",
      "Loss: 1.3968449831008911\n",
      "\n",
      "> Iteration 1039/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6914]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4778]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0320]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9050]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3484]]), B=tensor([[0.3321]])\n",
      "Loss: 1.36177659034729\n",
      "\n",
      "> Iteration 1040/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6913]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4778]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0319]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9050]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3483]]), B=tensor([[0.3320]])\n",
      "Loss: 1.4045839309692383\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1041/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6913]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4778]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0319]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9050]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3483]]), B=tensor([[0.3320]])\n",
      "Loss: 1.3419874906539917\n",
      "\n",
      "> Iteration 1042/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6912]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4777]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0319]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9050]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3483]]), B=tensor([[0.3319]])\n",
      "Loss: 1.4048583507537842\n",
      "\n",
      "> Iteration 1043/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6912]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4777]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0318]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9050]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3482]]), B=tensor([[0.3319]])\n",
      "Loss: 1.3920140266418457\n",
      "\n",
      "> Iteration 1044/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6912]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4776]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0318]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9050]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3482]]), B=tensor([[0.3318]])\n",
      "Loss: 1.4103316068649292\n",
      "\n",
      "> Iteration 1045/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6911]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4776]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1988]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0318]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9051]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3482]]), B=tensor([[0.3318]])\n",
      "Loss: 1.3492789268493652\n",
      "\n",
      "> Iteration 1046/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6911]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4776]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0318]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9051]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3481]]), B=tensor([[0.3318]])\n",
      "Loss: 1.3799437284469604\n",
      "\n",
      "> Iteration 1047/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6911]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4775]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0317]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9051]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3481]]), B=tensor([[0.3317]])\n",
      "Loss: 1.4067270755767822\n",
      "\n",
      "> Iteration 1048/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6910]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4775]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0317]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9051]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3481]]), B=tensor([[0.3317]])\n",
      "Loss: 1.3612812757492065\n",
      "\n",
      "> Iteration 1049/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6910]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4775]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0317]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9051]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3480]]), B=tensor([[0.3316]])\n",
      "Loss: 1.3783140182495117\n",
      "\n",
      "> Iteration 1050/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6910]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4774]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0317]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9051]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3480]]), B=tensor([[0.3316]])\n",
      "Loss: 1.3599246740341187\n",
      "\n",
      "> Iteration 1051/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0182]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6909]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4774]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0316]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9051]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3480]]), B=tensor([[0.3316]])\n",
      "Loss: 1.3904329538345337\n",
      "\n",
      "> Iteration 1052/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0182]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6909]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4773]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0316]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9052]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3480]]), B=tensor([[0.3315]])\n",
      "Loss: 1.4003695249557495\n",
      "\n",
      "> Iteration 1053/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0182]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6909]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4773]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0316]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9052]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3480]]), B=tensor([[0.3315]])\n",
      "Loss: 1.3922021389007568\n",
      "\n",
      "> Iteration 1054/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0182]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6908]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4773]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0316]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9052]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3480]]), B=tensor([[0.3315]])\n",
      "Loss: 1.3975739479064941\n",
      "\n",
      "> Iteration 1055/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0183]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6908]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4772]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0316]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9052]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3479]]), B=tensor([[0.3314]])\n",
      "Loss: 1.3923786878585815\n",
      "\n",
      "> Iteration 1056/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0183]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6908]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4772]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0315]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9052]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3479]]), B=tensor([[0.3314]])\n",
      "Loss: 1.3691914081573486\n",
      "\n",
      "> Iteration 1057/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0183]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6908]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4772]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1987]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0315]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9052]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3479]]), B=tensor([[0.3314]])\n",
      "Loss: 1.3858336210250854\n",
      "\n",
      "> Iteration 1058/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0183]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6907]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4771]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0315]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9052]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3479]]), B=tensor([[0.3313]])\n",
      "Loss: 1.3628755807876587\n",
      "\n",
      "> Iteration 1059/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0184]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6907]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4771]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0315]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9052]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3479]]), B=tensor([[0.3313]])\n",
      "Loss: 1.37456214427948\n",
      "\n",
      "> Iteration 1060/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0184]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6907]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4771]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0315]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9052]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3479]]), B=tensor([[0.3313]])\n",
      "Loss: 1.3939467668533325\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1061/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0184]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6907]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4771]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0314]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9053]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3479]]), B=tensor([[0.3312]])\n",
      "Loss: 1.3458211421966553\n",
      "\n",
      "> Iteration 1062/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0184]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6906]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4770]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0314]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9053]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3479]]), B=tensor([[0.3312]])\n",
      "Loss: 1.3867768049240112\n",
      "\n",
      "> Iteration 1063/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0184]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6906]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4770]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0314]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9053]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3478]]), B=tensor([[0.3312]])\n",
      "Loss: 1.3965121507644653\n",
      "\n",
      "> Iteration 1064/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0184]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6906]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4770]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0314]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9053]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3478]]), B=tensor([[0.3311]])\n",
      "Loss: 1.3881772756576538\n",
      "\n",
      "> Iteration 1065/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0185]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6905]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4769]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0314]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9053]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3478]]), B=tensor([[0.3311]])\n",
      "Loss: 1.388668417930603\n",
      "\n",
      "> Iteration 1066/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0185]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6905]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4769]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0313]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9053]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3478]]), B=tensor([[0.3311]])\n",
      "Loss: 1.4056084156036377\n",
      "\n",
      "> Iteration 1067/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0185]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6905]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4769]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0313]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9053]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3477]]), B=tensor([[0.3310]])\n",
      "Loss: 1.3755759000778198\n",
      "\n",
      "> Iteration 1068/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0185]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6904]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4768]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0313]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9053]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3477]]), B=tensor([[0.3310]])\n",
      "Loss: 1.3770079612731934\n",
      "\n",
      "> Iteration 1069/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0185]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6904]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4768]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0313]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9054]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3477]]), B=tensor([[0.3309]])\n",
      "Loss: 1.3651323318481445\n",
      "\n",
      "> Iteration 1070/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0185]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6904]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4768]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1986]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0312]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9054]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3477]]), B=tensor([[0.3309]])\n",
      "Loss: 1.3671672344207764\n",
      "\n",
      "> Iteration 1071/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0186]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6904]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4767]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0312]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9054]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3477]]), B=tensor([[0.3309]])\n",
      "Loss: 1.4057469367980957\n",
      "\n",
      "> Iteration 1072/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0186]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6903]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4767]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0312]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9054]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3477]]), B=tensor([[0.3308]])\n",
      "Loss: 1.376668930053711\n",
      "\n",
      "> Iteration 1073/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0186]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6903]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4766]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0312]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9054]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3476]]), B=tensor([[0.3308]])\n",
      "Loss: 1.3499670028686523\n",
      "\n",
      "> Iteration 1074/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0186]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6903]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4766]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0311]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9054]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3476]]), B=tensor([[0.3307]])\n",
      "Loss: 1.3998080492019653\n",
      "\n",
      "> Iteration 1075/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0186]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6902]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4766]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0311]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9054]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3476]]), B=tensor([[0.3307]])\n",
      "Loss: 1.3928484916687012\n",
      "\n",
      "> Iteration 1076/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0187]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6902]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4765]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0311]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9054]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3476]]), B=tensor([[0.3307]])\n",
      "Loss: 1.3754349946975708\n",
      "\n",
      "> Iteration 1077/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0187]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6902]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4765]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0311]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9055]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3475]]), B=tensor([[0.3306]])\n",
      "Loss: 1.4199379682540894\n",
      "\n",
      "> Iteration 1078/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0187]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6901]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4765]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0310]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9055]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3475]]), B=tensor([[0.3306]])\n",
      "Loss: 1.4057252407073975\n",
      "\n",
      "> Iteration 1079/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0187]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6901]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4764]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0310]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9055]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3475]]), B=tensor([[0.3305]])\n",
      "Loss: 1.3594331741333008\n",
      "\n",
      "> Iteration 1080/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0187]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6901]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4764]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0310]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9055]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3475]]), B=tensor([[0.3305]])\n",
      "Loss: 1.398948311805725\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1081/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0188]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6900]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4763]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1985]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0310]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9055]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3474]]), B=tensor([[0.3305]])\n",
      "Loss: 1.3908571004867554\n",
      "\n",
      "> Iteration 1082/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0188]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6900]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4763]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0309]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9055]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3474]]), B=tensor([[0.3304]])\n",
      "Loss: 1.4389715194702148\n",
      "\n",
      "> Iteration 1083/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0188]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6899]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4763]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0309]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9055]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3474]]), B=tensor([[0.3304]])\n",
      "Loss: 1.3680424690246582\n",
      "\n",
      "> Iteration 1084/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0188]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6899]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4762]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0309]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9056]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3474]]), B=tensor([[0.3303]])\n",
      "Loss: 1.3996822834014893\n",
      "\n",
      "> Iteration 1085/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0188]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6899]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4762]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0308]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9056]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3473]]), B=tensor([[0.3303]])\n",
      "Loss: 1.4063690900802612\n",
      "\n",
      "> Iteration 1086/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0188]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6898]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4761]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0308]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9056]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3473]]), B=tensor([[0.3302]])\n",
      "Loss: 1.3796027898788452\n",
      "\n",
      "> Iteration 1087/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0188]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6898]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4761]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0308]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9056]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3472]]), B=tensor([[0.3302]])\n",
      "Loss: 1.428260326385498\n",
      "\n",
      "> Iteration 1088/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6898]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4760]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0307]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9056]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3472]]), B=tensor([[0.3301]])\n",
      "Loss: 1.416784644126892\n",
      "\n",
      "> Iteration 1089/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6897]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4760]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0307]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9056]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3472]]), B=tensor([[0.3301]])\n",
      "Loss: 1.3616238832473755\n",
      "\n",
      "> Iteration 1090/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6897]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4759]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0307]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9056]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3471]]), B=tensor([[0.3300]])\n",
      "Loss: 1.4162554740905762\n",
      "\n",
      "> Iteration 1091/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6896]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4759]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1984]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0306]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9057]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3471]]), B=tensor([[0.3300]])\n",
      "Loss: 1.3776423931121826\n",
      "\n",
      "> Iteration 1092/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6896]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4758]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0306]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9057]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3470]]), B=tensor([[0.3299]])\n",
      "Loss: 1.3744475841522217\n",
      "\n",
      "> Iteration 1093/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6896]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4758]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0306]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9057]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3470]]), B=tensor([[0.3299]])\n",
      "Loss: 1.3788697719573975\n",
      "\n",
      "> Iteration 1094/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6895]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4758]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0306]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9057]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3469]]), B=tensor([[0.3298]])\n",
      "Loss: 1.3865793943405151\n",
      "\n",
      "> Iteration 1095/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6895]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4757]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0305]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9057]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3469]]), B=tensor([[0.3298]])\n",
      "Loss: 1.3360284566879272\n",
      "\n",
      "> Iteration 1096/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6894]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4757]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0305]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9057]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3469]]), B=tensor([[0.3297]])\n",
      "Loss: 1.368532657623291\n",
      "\n",
      "> Iteration 1097/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6894]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4756]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0305]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9058]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3468]]), B=tensor([[0.3297]])\n",
      "Loss: 1.3795832395553589\n",
      "\n",
      "> Iteration 1098/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6894]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4756]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0305]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9058]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3468]]), B=tensor([[0.3297]])\n",
      "Loss: 1.3517470359802246\n",
      "\n",
      "> Iteration 1099/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0190]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6894]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4756]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0304]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9058]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3468]]), B=tensor([[0.3296]])\n",
      "Loss: 1.388732671737671\n",
      "\n",
      "> Iteration 1100/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0190]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6893]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4755]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0304]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9058]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3468]]), B=tensor([[0.3296]])\n",
      "Loss: 1.3701188564300537\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1101/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0190]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6893]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4755]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0304]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9058]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3468]]), B=tensor([[0.3296]])\n",
      "Loss: 1.3825033903121948\n",
      "\n",
      "> Iteration 1102/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0190]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6893]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4755]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1983]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0304]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9058]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3467]]), B=tensor([[0.3295]])\n",
      "Loss: 1.378666877746582\n",
      "\n",
      "> Iteration 1103/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0190]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6892]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4754]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0303]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9058]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3467]]), B=tensor([[0.3295]])\n",
      "Loss: 1.3816769123077393\n",
      "\n",
      "> Iteration 1104/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0190]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6892]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4754]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0303]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9058]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3467]]), B=tensor([[0.3295]])\n",
      "Loss: 1.3590768575668335\n",
      "\n",
      "> Iteration 1105/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6892]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4754]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0303]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9058]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3467]]), B=tensor([[0.3294]])\n",
      "Loss: 1.3593356609344482\n",
      "\n",
      "> Iteration 1106/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6892]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4753]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0303]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9059]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3466]]), B=tensor([[0.3294]])\n",
      "Loss: 1.427229881286621\n",
      "\n",
      "> Iteration 1107/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6891]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4753]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0302]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9059]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3466]]), B=tensor([[0.3293]])\n",
      "Loss: 1.3801922798156738\n",
      "\n",
      "> Iteration 1108/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6891]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4753]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0302]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9059]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3466]]), B=tensor([[0.3293]])\n",
      "Loss: 1.393912672996521\n",
      "\n",
      "> Iteration 1109/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6891]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4752]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0302]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9059]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3465]]), B=tensor([[0.3293]])\n",
      "Loss: 1.367658257484436\n",
      "\n",
      "> Iteration 1110/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6890]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4752]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0302]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9059]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3465]]), B=tensor([[0.3292]])\n",
      "Loss: 1.3710546493530273\n",
      "\n",
      "> Iteration 1111/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6890]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4752]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0301]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9059]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3464]]), B=tensor([[0.3292]])\n",
      "Loss: 1.3722771406173706\n",
      "\n",
      "> Iteration 1112/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6890]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4751]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0301]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9059]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3464]]), B=tensor([[0.3291]])\n",
      "Loss: 1.366457462310791\n",
      "\n",
      "> Iteration 1113/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6889]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4751]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0301]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9059]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3464]]), B=tensor([[0.3291]])\n",
      "Loss: 1.3914414644241333\n",
      "\n",
      "> Iteration 1114/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6889]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4750]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1982]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0301]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9060]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3463]]), B=tensor([[0.3290]])\n",
      "Loss: 1.4287254810333252\n",
      "\n",
      "> Iteration 1115/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6889]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4750]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0300]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9060]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3463]]), B=tensor([[0.3290]])\n",
      "Loss: 1.3823410272598267\n",
      "\n",
      "> Iteration 1116/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6888]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4750]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0300]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9060]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3463]]), B=tensor([[0.3290]])\n",
      "Loss: 1.4186015129089355\n",
      "\n",
      "> Iteration 1117/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6888]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4749]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0300]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9060]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3462]]), B=tensor([[0.3289]])\n",
      "Loss: 1.4127144813537598\n",
      "\n",
      "> Iteration 1118/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6887]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4749]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0299]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9060]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3462]]), B=tensor([[0.3289]])\n",
      "Loss: 1.381304144859314\n",
      "\n",
      "> Iteration 1119/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6887]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4748]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0299]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9060]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3462]]), B=tensor([[0.3288]])\n",
      "Loss: 1.3941668272018433\n",
      "\n",
      "> Iteration 1120/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6887]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4748]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0299]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9061]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3461]]), B=tensor([[0.3288]])\n",
      "Loss: 1.3937159776687622\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1121/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6886]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4747]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0299]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9061]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3461]]), B=tensor([[0.3287]])\n",
      "Loss: 1.398047924041748\n",
      "\n",
      "> Iteration 1122/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6886]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4747]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0298]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9061]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3460]]), B=tensor([[0.3287]])\n",
      "Loss: 1.3936494588851929\n",
      "\n",
      "> Iteration 1123/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6885]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4747]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0298]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9061]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3460]]), B=tensor([[0.3286]])\n",
      "Loss: 1.3552279472351074\n",
      "\n",
      "> Iteration 1124/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6885]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4746]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1981]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0298]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9061]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3459]]), B=tensor([[0.3286]])\n",
      "Loss: 1.4016404151916504\n",
      "\n",
      "> Iteration 1125/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6885]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4746]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0297]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9061]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3459]]), B=tensor([[0.3285]])\n",
      "Loss: 1.3811898231506348\n",
      "\n",
      "> Iteration 1126/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6884]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4745]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0297]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9061]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3459]]), B=tensor([[0.3285]])\n",
      "Loss: 1.4019043445587158\n",
      "\n",
      "> Iteration 1127/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6884]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4745]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0297]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9062]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3458]]), B=tensor([[0.3284]])\n",
      "Loss: 1.3655939102172852\n",
      "\n",
      "> Iteration 1128/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6884]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4744]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0296]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9062]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3457]]), B=tensor([[0.3284]])\n",
      "Loss: 1.4297282695770264\n",
      "\n",
      "> Iteration 1129/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6883]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4744]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0296]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9062]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3457]]), B=tensor([[0.3283]])\n",
      "Loss: 1.3775449991226196\n",
      "\n",
      "> Iteration 1130/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6883]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4744]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0296]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9062]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3456]]), B=tensor([[0.3283]])\n",
      "Loss: 1.3775875568389893\n",
      "\n",
      "> Iteration 1131/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6882]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4743]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0295]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9062]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3456]]), B=tensor([[0.3282]])\n",
      "Loss: 1.4275658130645752\n",
      "\n",
      "> Iteration 1132/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6882]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4742]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0295]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9062]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3455]]), B=tensor([[0.3282]])\n",
      "Loss: 1.37461519241333\n",
      "\n",
      "> Iteration 1133/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6881]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4742]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1980]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0295]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9063]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3455]]), B=tensor([[0.3281]])\n",
      "Loss: 1.3843621015548706\n",
      "\n",
      "> Iteration 1134/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6881]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4741]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0294]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9063]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3454]]), B=tensor([[0.3281]])\n",
      "Loss: 1.4206901788711548\n",
      "\n",
      "> Iteration 1135/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6881]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4741]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0294]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9063]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3453]]), B=tensor([[0.3280]])\n",
      "Loss: 1.3484106063842773\n",
      "\n",
      "> Iteration 1136/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6880]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4741]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0293]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9063]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3453]]), B=tensor([[0.3279]])\n",
      "Loss: 1.3959707021713257\n",
      "\n",
      "> Iteration 1137/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6880]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4740]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0293]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9063]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3452]]), B=tensor([[0.3279]])\n",
      "Loss: 1.3697686195373535\n",
      "\n",
      "> Iteration 1138/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6879]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4740]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0293]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9063]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3452]]), B=tensor([[0.3278]])\n",
      "Loss: 1.3464436531066895\n",
      "\n",
      "> Iteration 1139/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6879]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4739]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0292]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9064]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3451]]), B=tensor([[0.3278]])\n",
      "Loss: 1.3791086673736572\n",
      "\n",
      "> Iteration 1140/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6879]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4739]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0292]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9064]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3451]]), B=tensor([[0.3278]])\n",
      "Loss: 1.3581990003585815\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1141/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6878]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4738]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0292]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9064]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3451]]), B=tensor([[0.3277]])\n",
      "Loss: 1.4014127254486084\n",
      "\n",
      "> Iteration 1142/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6878]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4738]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0292]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9064]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3450]]), B=tensor([[0.3277]])\n",
      "Loss: 1.4134331941604614\n",
      "\n",
      "> Iteration 1143/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6878]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4738]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1979]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0291]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9064]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3450]]), B=tensor([[0.3276]])\n",
      "Loss: 1.4097537994384766\n",
      "\n",
      "> Iteration 1144/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6877]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4737]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0291]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9064]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3449]]), B=tensor([[0.3276]])\n",
      "Loss: 1.3956005573272705\n",
      "\n",
      "> Iteration 1145/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6877]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4737]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0291]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9064]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3449]]), B=tensor([[0.3275]])\n",
      "Loss: 1.372238278388977\n",
      "\n",
      "> Iteration 1146/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6877]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4736]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0290]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9065]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3448]]), B=tensor([[0.3275]])\n",
      "Loss: 1.3828125\n",
      "\n",
      "> Iteration 1147/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6876]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4736]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0290]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9065]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3448]]), B=tensor([[0.3274]])\n",
      "Loss: 1.390116572380066\n",
      "\n",
      "> Iteration 1148/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6876]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4736]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0290]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9065]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3447]]), B=tensor([[0.3274]])\n",
      "Loss: 1.4213495254516602\n",
      "\n",
      "> Iteration 1149/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6875]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4735]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0289]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9065]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3447]]), B=tensor([[0.3273]])\n",
      "Loss: 1.4088802337646484\n",
      "\n",
      "> Iteration 1150/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6875]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4735]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0289]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9065]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.3273]])\n",
      "Loss: 1.3892731666564941\n",
      "\n",
      "> Iteration 1151/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6875]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4734]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0289]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9065]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.3273]])\n",
      "Loss: 1.3763281106948853\n",
      "\n",
      "> Iteration 1152/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6874]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4734]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0289]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9065]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.3272]])\n",
      "Loss: 1.3996894359588623\n",
      "\n",
      "> Iteration 1153/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6874]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4734]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9066]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.3272]])\n",
      "Loss: 1.4018800258636475\n",
      "\n",
      "> Iteration 1154/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6874]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4733]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1978]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9066]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3445]]), B=tensor([[0.3271]])\n",
      "Loss: 1.3754748106002808\n",
      "\n",
      "> Iteration 1155/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6873]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4733]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9066]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3445]]), B=tensor([[0.3271]])\n",
      "Loss: 1.3476810455322266\n",
      "\n",
      "> Iteration 1156/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6873]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4732]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9066]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3445]]), B=tensor([[0.3271]])\n",
      "Loss: 1.3846763372421265\n",
      "\n",
      "> Iteration 1157/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6873]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4732]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9066]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3444]]), B=tensor([[0.3270]])\n",
      "Loss: 1.4110535383224487\n",
      "\n",
      "> Iteration 1158/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6872]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4732]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9066]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3444]]), B=tensor([[0.3270]])\n",
      "Loss: 1.4030511379241943\n",
      "\n",
      "> Iteration 1159/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6872]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4731]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9066]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3444]]), B=tensor([[0.3269]])\n",
      "Loss: 1.3612624406814575\n",
      "\n",
      "> Iteration 1160/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6872]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4731]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9066]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3443]]), B=tensor([[0.3269]])\n",
      "Loss: 1.3612968921661377\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1161/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6871]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4731]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9067]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3443]]), B=tensor([[0.3268]])\n",
      "Loss: 1.385077953338623\n",
      "\n",
      "> Iteration 1162/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6871]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4730]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9067]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3442]]), B=tensor([[0.3268]])\n",
      "Loss: 1.4047499895095825\n",
      "\n",
      "> Iteration 1163/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6871]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4730]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9067]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3442]]), B=tensor([[0.3268]])\n",
      "Loss: 1.3946051597595215\n",
      "\n",
      "> Iteration 1164/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6870]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4729]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9067]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.3267]])\n",
      "Loss: 1.3639280796051025\n",
      "\n",
      "> Iteration 1165/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6870]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4729]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1977]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9067]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.3267]])\n",
      "Loss: 1.3652231693267822\n",
      "\n",
      "> Iteration 1166/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6870]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4729]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9067]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3440]]), B=tensor([[0.3266]])\n",
      "Loss: 1.3607906103134155\n",
      "\n",
      "> Iteration 1167/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6869]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4728]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9067]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3440]]), B=tensor([[0.3266]])\n",
      "Loss: 1.3932716846466064\n",
      "\n",
      "> Iteration 1168/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6869]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4728]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9068]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3439]]), B=tensor([[0.3265]])\n",
      "Loss: 1.3824102878570557\n",
      "\n",
      "> Iteration 1169/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6869]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4727]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9068]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3439]]), B=tensor([[0.3265]])\n",
      "Loss: 1.369374394416809\n",
      "\n",
      "> Iteration 1170/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6868]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4727]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9068]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3438]]), B=tensor([[0.3265]])\n",
      "Loss: 1.384490728378296\n",
      "\n",
      "> Iteration 1171/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6868]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4727]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9068]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3438]]), B=tensor([[0.3264]])\n",
      "Loss: 1.4008654356002808\n",
      "\n",
      "> Iteration 1172/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6868]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4726]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9068]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3437]]), B=tensor([[0.3264]])\n",
      "Loss: 1.4160737991333008\n",
      "\n",
      "> Iteration 1173/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6867]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4726]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9068]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3437]]), B=tensor([[0.3263]])\n",
      "Loss: 1.4617544412612915\n",
      "\n",
      "> Iteration 1174/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6867]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4725]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9068]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3437]]), B=tensor([[0.3263]])\n",
      "Loss: 1.387656569480896\n",
      "\n",
      "> Iteration 1175/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6866]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4725]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9069]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.3262]])\n",
      "Loss: 1.3738553524017334\n",
      "\n",
      "> Iteration 1176/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6866]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4724]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1976]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9069]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.3262]])\n",
      "Loss: 1.3996227979660034\n",
      "\n",
      "> Iteration 1177/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6866]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4724]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9069]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3435]]), B=tensor([[0.3261]])\n",
      "Loss: 1.403112769126892\n",
      "\n",
      "> Iteration 1178/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6865]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4724]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9069]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3435]]), B=tensor([[0.3261]])\n",
      "Loss: 1.3735816478729248\n",
      "\n",
      "> Iteration 1179/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6865]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4723]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9069]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3434]]), B=tensor([[0.3260]])\n",
      "Loss: 1.3929392099380493\n",
      "\n",
      "> Iteration 1180/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6865]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4723]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9069]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3434]]), B=tensor([[0.3260]])\n",
      "Loss: 1.3906333446502686\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1181/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6864]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4722]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9069]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.3259]])\n",
      "Loss: 1.3787062168121338\n",
      "\n",
      "> Iteration 1182/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6864]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4722]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9070]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.3259]])\n",
      "Loss: 1.3992929458618164\n",
      "\n",
      "> Iteration 1183/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6863]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4721]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9070]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3432]]), B=tensor([[0.3258]])\n",
      "Loss: 1.397029161453247\n",
      "\n",
      "> Iteration 1184/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6863]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4721]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9070]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3432]]), B=tensor([[0.3258]])\n",
      "Loss: 1.3918323516845703\n",
      "\n",
      "> Iteration 1185/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6863]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4721]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9070]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3432]]), B=tensor([[0.3257]])\n",
      "Loss: 1.429939866065979\n",
      "\n",
      "> Iteration 1186/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6862]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4720]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1975]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9070]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3431]]), B=tensor([[0.3257]])\n",
      "Loss: 1.3592649698257446\n",
      "\n",
      "> Iteration 1187/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6862]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4720]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9070]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3431]]), B=tensor([[0.3257]])\n",
      "Loss: 1.3843915462493896\n",
      "\n",
      "> Iteration 1188/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6862]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4719]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9070]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3431]]), B=tensor([[0.3256]])\n",
      "Loss: 1.4008886814117432\n",
      "\n",
      "> Iteration 1189/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0193]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6861]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4719]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9071]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3430]]), B=tensor([[0.3256]])\n",
      "Loss: 1.4047726392745972\n",
      "\n",
      "> Iteration 1190/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0193]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6861]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4719]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9071]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3430]]), B=tensor([[0.3255]])\n",
      "Loss: 1.3825783729553223\n",
      "\n",
      "> Iteration 1191/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0193]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6861]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4718]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9071]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3430]]), B=tensor([[0.3255]])\n",
      "Loss: 1.4101524353027344\n",
      "\n",
      "> Iteration 1192/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0193]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6860]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4718]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9071]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3430]]), B=tensor([[0.3254]])\n",
      "Loss: 1.4413166046142578\n",
      "\n",
      "> Iteration 1193/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0193]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6860]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4717]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9071]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3429]]), B=tensor([[0.3254]])\n",
      "Loss: 1.3950079679489136\n",
      "\n",
      "> Iteration 1194/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0193]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6860]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4717]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9071]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3429]]), B=tensor([[0.3254]])\n",
      "Loss: 1.3964952230453491\n",
      "\n",
      "> Iteration 1195/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0194]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6859]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4717]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0276]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9071]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3429]]), B=tensor([[0.3253]])\n",
      "Loss: 1.360937237739563\n",
      "\n",
      "> Iteration 1196/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0194]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6859]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4716]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1974]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0276]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9072]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3429]]), B=tensor([[0.3253]])\n",
      "Loss: 1.3800464868545532\n",
      "\n",
      "> Iteration 1197/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0194]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6859]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4716]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0276]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9072]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3428]]), B=tensor([[0.3252]])\n",
      "Loss: 1.3900293111801147\n",
      "\n",
      "> Iteration 1198/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0194]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6858]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4715]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0276]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9072]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3428]]), B=tensor([[0.3252]])\n",
      "Loss: 1.336727261543274\n",
      "\n",
      "> Iteration 1199/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0195]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6858]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4715]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9072]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3428]]), B=tensor([[0.3251]])\n",
      "Loss: 1.3604482412338257\n",
      "\n",
      "> Iteration 1200/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0195]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6857]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4715]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9072]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3428]]), B=tensor([[0.3251]])\n",
      "Loss: 1.3498384952545166\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1201/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0195]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6857]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4714]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9072]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3428]]), B=tensor([[0.3251]])\n",
      "Loss: 1.3830422163009644\n",
      "\n",
      "> Iteration 1202/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0195]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6857]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4714]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9072]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.3250]])\n",
      "Loss: 1.3781521320343018\n",
      "\n",
      "> Iteration 1203/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0195]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6856]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4713]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9073]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.3250]])\n",
      "Loss: 1.4132299423217773\n",
      "\n",
      "> Iteration 1204/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0196]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6856]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4713]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9073]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.3249]])\n",
      "Loss: 1.373159646987915\n",
      "\n",
      "> Iteration 1205/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0196]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6856]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4713]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9073]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.3249]])\n",
      "Loss: 1.3924874067306519\n",
      "\n",
      "> Iteration 1206/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0197]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6855]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4712]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1973]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9073]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.3249]])\n",
      "Loss: 1.4295892715454102\n",
      "\n",
      "> Iteration 1207/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0197]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6855]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4712]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9073]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.3248]])\n",
      "Loss: 1.3822274208068848\n",
      "\n",
      "> Iteration 1208/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0197]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6855]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4711]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9073]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.3248]])\n",
      "Loss: 1.4007478952407837\n",
      "\n",
      "> Iteration 1209/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0198]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6854]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4711]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9073]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.3247]])\n",
      "Loss: 1.366847038269043\n",
      "\n",
      "> Iteration 1210/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0198]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6854]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4711]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9074]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3247]])\n",
      "Loss: 1.3817719221115112\n",
      "\n",
      "> Iteration 1211/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0198]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6854]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4710]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9074]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3246]])\n",
      "Loss: 1.347700834274292\n",
      "\n",
      "> Iteration 1212/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0199]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6853]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4710]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0272]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9074]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3246]])\n",
      "Loss: 1.3756256103515625\n",
      "\n",
      "> Iteration 1213/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0199]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6853]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4709]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0272]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9074]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3246]])\n",
      "Loss: 1.3704947233200073\n",
      "\n",
      "> Iteration 1214/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0200]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6853]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4709]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0272]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9074]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3245]])\n",
      "Loss: 1.3919389247894287\n",
      "\n",
      "> Iteration 1215/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0200]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6852]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4709]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0272]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9074]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3245]])\n",
      "Loss: 1.3892319202423096\n",
      "\n",
      "> Iteration 1216/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0200]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6852]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4708]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1972]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0271]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9074]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3244]])\n",
      "Loss: 1.432085394859314\n",
      "\n",
      "> Iteration 1217/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0201]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6851]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4708]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0271]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9075]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3244]])\n",
      "Loss: 1.319164752960205\n",
      "\n",
      "> Iteration 1218/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0201]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6851]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4707]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0271]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9075]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3244]])\n",
      "Loss: 1.3847943544387817\n",
      "\n",
      "> Iteration 1219/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0201]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6851]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4707]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0271]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9075]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3243]])\n",
      "Loss: 1.3687701225280762\n",
      "\n",
      "> Iteration 1220/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0202]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6850]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4707]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0271]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9075]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3243]])\n",
      "Loss: 1.426281213760376\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1221/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0202]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6850]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4706]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0270]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9075]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3242]])\n",
      "Loss: 1.3710514307022095\n",
      "\n",
      "> Iteration 1222/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0202]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6850]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4706]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0270]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9075]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3242]])\n",
      "Loss: 1.3702969551086426\n",
      "\n",
      "> Iteration 1223/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0203]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6849]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4705]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0270]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9075]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.3242]])\n",
      "Loss: 1.3565630912780762\n",
      "\n",
      "> Iteration 1224/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0203]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6849]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4705]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0270]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9076]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3425]]), B=tensor([[0.3241]])\n",
      "Loss: 1.3637824058532715\n",
      "\n",
      "> Iteration 1225/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0203]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6849]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4705]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0269]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9076]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3425]]), B=tensor([[0.3241]])\n",
      "Loss: 1.3855066299438477\n",
      "\n",
      "> Iteration 1226/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0203]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6848]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4704]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1971]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0269]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9076]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3425]]), B=tensor([[0.3241]])\n",
      "Loss: 1.3871251344680786\n",
      "\n",
      "> Iteration 1227/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6848]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4704]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0269]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9076]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3425]]), B=tensor([[0.3240]])\n",
      "Loss: 1.3887333869934082\n",
      "\n",
      "> Iteration 1228/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6848]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4704]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0269]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9076]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3425]]), B=tensor([[0.3240]])\n",
      "Loss: 1.364782452583313\n",
      "\n",
      "> Iteration 1229/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6848]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4703]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9076]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3425]]), B=tensor([[0.3239]])\n",
      "Loss: 1.3698513507843018\n",
      "\n",
      "> Iteration 1230/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6847]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4703]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9076]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3424]]), B=tensor([[0.3239]])\n",
      "Loss: 1.3231664896011353\n",
      "\n",
      "> Iteration 1231/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6847]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4703]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9076]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3424]]), B=tensor([[0.3239]])\n",
      "Loss: 1.4263156652450562\n",
      "\n",
      "> Iteration 1232/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6847]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4702]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9077]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3424]]), B=tensor([[0.3238]])\n",
      "Loss: 1.3703017234802246\n",
      "\n",
      "> Iteration 1233/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6846]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4702]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9077]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3423]]), B=tensor([[0.3238]])\n",
      "Loss: 1.401726484298706\n",
      "\n",
      "> Iteration 1234/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6846]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4701]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9077]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3423]]), B=tensor([[0.3237]])\n",
      "Loss: 1.385778546333313\n",
      "\n",
      "> Iteration 1235/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6846]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4701]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9077]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3422]]), B=tensor([[0.3237]])\n",
      "Loss: 1.4078538417816162\n",
      "\n",
      "> Iteration 1236/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6845]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4701]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9077]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3422]]), B=tensor([[0.3236]])\n",
      "Loss: 1.3825125694274902\n",
      "\n",
      "> Iteration 1237/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6845]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4700]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1970]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9077]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3422]]), B=tensor([[0.3236]])\n",
      "Loss: 1.3832776546478271\n",
      "\n",
      "> Iteration 1238/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6844]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4700]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9077]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3421]]), B=tensor([[0.3236]])\n",
      "Loss: 1.3552985191345215\n",
      "\n",
      "> Iteration 1239/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6844]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4699]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9078]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3421]]), B=tensor([[0.3235]])\n",
      "Loss: 1.3982198238372803\n",
      "\n",
      "> Iteration 1240/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6844]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4699]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9078]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3421]]), B=tensor([[0.3235]])\n",
      "Loss: 1.3730626106262207\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1241/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6843]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4699]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9078]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3420]]), B=tensor([[0.3234]])\n",
      "Loss: 1.3746764659881592\n",
      "\n",
      "> Iteration 1242/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6843]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4698]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9078]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3420]]), B=tensor([[0.3234]])\n",
      "Loss: 1.373203992843628\n",
      "\n",
      "> Iteration 1243/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6843]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4698]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9078]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3420]]), B=tensor([[0.3234]])\n",
      "Loss: 1.3579087257385254\n",
      "\n",
      "> Iteration 1244/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6843]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4698]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9078]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3419]]), B=tensor([[0.3233]])\n",
      "Loss: 1.3951690196990967\n",
      "\n",
      "> Iteration 1245/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6842]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4697]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9078]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3419]]), B=tensor([[0.3233]])\n",
      "Loss: 1.3998029232025146\n",
      "\n",
      "> Iteration 1246/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6842]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4697]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9078]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3419]]), B=tensor([[0.3232]])\n",
      "Loss: 1.3999266624450684\n",
      "\n",
      "> Iteration 1247/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6842]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4697]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9079]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3419]]), B=tensor([[0.3232]])\n",
      "Loss: 1.383854866027832\n",
      "\n",
      "> Iteration 1248/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6841]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4696]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1969]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9079]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3418]]), B=tensor([[0.3232]])\n",
      "Loss: 1.4127264022827148\n",
      "\n",
      "> Iteration 1249/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0205]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6841]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4696]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9079]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3418]]), B=tensor([[0.3231]])\n",
      "Loss: 1.371382474899292\n",
      "\n",
      "> Iteration 1250/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6841]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4695]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9079]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3418]]), B=tensor([[0.3231]])\n",
      "Loss: 1.364673376083374\n",
      "\n",
      "> Iteration 1251/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6840]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4695]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9079]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3417]]), B=tensor([[0.3231]])\n",
      "Loss: 1.3877060413360596\n",
      "\n",
      "> Iteration 1252/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6840]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4695]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9079]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3417]]), B=tensor([[0.3230]])\n",
      "Loss: 1.3371959924697876\n",
      "\n",
      "> Iteration 1253/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6840]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4694]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9079]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3417]]), B=tensor([[0.3230]])\n",
      "Loss: 1.420914888381958\n",
      "\n",
      "> Iteration 1254/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6839]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4694]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9079]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3416]]), B=tensor([[0.3229]])\n",
      "Loss: 1.3388251066207886\n",
      "\n",
      "> Iteration 1255/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6839]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4694]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9080]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3416]]), B=tensor([[0.3229]])\n",
      "Loss: 1.3750780820846558\n",
      "\n",
      "> Iteration 1256/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6839]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4693]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9080]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3416]]), B=tensor([[0.3229]])\n",
      "Loss: 1.3971598148345947\n",
      "\n",
      "> Iteration 1257/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6838]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4693]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9080]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3415]]), B=tensor([[0.3228]])\n",
      "Loss: 1.3621413707733154\n",
      "\n",
      "> Iteration 1258/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6838]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4693]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0261]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9080]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3415]]), B=tensor([[0.3228]])\n",
      "Loss: 1.442359209060669\n",
      "\n",
      "> Iteration 1259/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6838]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4692]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1968]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0261]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9080]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3415]]), B=tensor([[0.3227]])\n",
      "Loss: 1.3392401933670044\n",
      "\n",
      "> Iteration 1260/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6838]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4692]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0261]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9080]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3414]]), B=tensor([[0.3227]])\n",
      "Loss: 1.3904579877853394\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1261/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6837]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4692]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0261]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9080]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3414]]), B=tensor([[0.3227]])\n",
      "Loss: 1.3702406883239746\n",
      "\n",
      "> Iteration 1262/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0206]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6837]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4691]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0260]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9080]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3414]]), B=tensor([[0.3226]])\n",
      "Loss: 1.4048595428466797\n",
      "\n",
      "> Iteration 1263/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0207]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6837]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4691]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0260]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9081]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3414]]), B=tensor([[0.3226]])\n",
      "Loss: 1.3778572082519531\n",
      "\n",
      "> Iteration 1264/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0207]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6836]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4690]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0260]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9081]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3413]]), B=tensor([[0.3226]])\n",
      "Loss: 1.4451563358306885\n",
      "\n",
      "> Iteration 1265/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0207]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6836]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4690]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0260]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9081]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3413]]), B=tensor([[0.3225]])\n",
      "Loss: 1.3666667938232422\n",
      "\n",
      "> Iteration 1266/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0207]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6836]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4690]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0259]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9081]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3413]]), B=tensor([[0.3225]])\n",
      "Loss: 1.3898745775222778\n",
      "\n",
      "> Iteration 1267/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0208]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6835]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4689]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0259]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9081]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3413]]), B=tensor([[0.3224]])\n",
      "Loss: 1.3918858766555786\n",
      "\n",
      "> Iteration 1268/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0208]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6835]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4689]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0259]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9081]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3413]]), B=tensor([[0.3224]])\n",
      "Loss: 1.3565633296966553\n",
      "\n",
      "> Iteration 1269/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0208]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6835]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4689]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0259]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9081]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3413]]), B=tensor([[0.3224]])\n",
      "Loss: 1.4006761312484741\n",
      "\n",
      "> Iteration 1270/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0208]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6834]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4688]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1967]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0259]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9081]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3412]]), B=tensor([[0.3223]])\n",
      "Loss: 1.3842787742614746\n",
      "\n",
      "> Iteration 1271/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0209]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6834]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4688]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1966]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0258]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9082]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3412]]), B=tensor([[0.3223]])\n",
      "Loss: 1.345164179801941\n",
      "\n",
      "> Iteration 1272/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0209]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6834]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4688]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1966]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0258]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9082]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3412]]), B=tensor([[0.3223]])\n",
      "Loss: 1.3884754180908203\n",
      "\n",
      "> Iteration 1273/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0209]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6833]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4687]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1966]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0258]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9082]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3412]]), B=tensor([[0.3222]])\n",
      "Loss: 1.363898754119873\n",
      "\n",
      "> Iteration 1274/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0209]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6833]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4687]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1966]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0258]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9082]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3412]]), B=tensor([[0.3222]])\n",
      "Loss: 1.3966858386993408\n",
      "\n",
      "> Iteration 1275/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0209]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6833]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4686]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1966]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0257]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9082]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3411]]), B=tensor([[0.3221]])\n",
      "Loss: 1.4247384071350098\n",
      "\n",
      "> Iteration 1276/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0210]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6832]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4686]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1966]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0257]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9082]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3411]]), B=tensor([[0.3221]])\n",
      "Loss: 1.3952168226242065\n",
      "\n",
      "> Iteration 1277/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0210]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6832]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4686]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1966]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0257]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9082]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3411]]), B=tensor([[0.3220]])\n",
      "Loss: 1.4265714883804321\n",
      "\n",
      "> Iteration 1278/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0210]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6832]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4685]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1966]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0257]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9083]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3411]]), B=tensor([[0.3220]])\n",
      "Loss: 1.4100614786148071\n",
      "\n",
      "> Iteration 1279/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0210]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6831]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4685]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1966]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0256]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9083]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3220]])\n",
      "Loss: 1.3753565549850464\n",
      "\n",
      "> Iteration 1280/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0210]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6831]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4684]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0256]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9083]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3219]])\n",
      "Loss: 1.3812183141708374\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1281/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0211]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6831]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4684]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0256]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9083]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3219]])\n",
      "Loss: 1.3615721464157104\n",
      "\n",
      "> Iteration 1282/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0211]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6830]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4684]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0256]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9083]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3218]])\n",
      "Loss: 1.415833830833435\n",
      "\n",
      "> Iteration 1283/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0211]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6830]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4683]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0255]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9083]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3218]])\n",
      "Loss: 1.3728022575378418\n",
      "\n",
      "> Iteration 1284/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0212]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6830]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4683]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0255]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9083]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3218]])\n",
      "Loss: 1.3890150785446167\n",
      "\n",
      "> Iteration 1285/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0212]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6829]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4683]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0255]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9084]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3217]])\n",
      "Loss: 1.4076677560806274\n",
      "\n",
      "> Iteration 1286/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0212]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6829]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4682]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0255]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9084]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3217]])\n",
      "Loss: 1.3800586462020874\n",
      "\n",
      "> Iteration 1287/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0213]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6829]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4682]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0255]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9084]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3217]])\n",
      "Loss: 1.378702163696289\n",
      "\n",
      "> Iteration 1288/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0213]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6828]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4682]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0255]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9084]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3216]])\n",
      "Loss: 1.370532751083374\n",
      "\n",
      "> Iteration 1289/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0213]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6828]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4681]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0254]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9084]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3216]])\n",
      "Loss: 1.3897385597229004\n",
      "\n",
      "> Iteration 1290/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0214]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6828]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4681]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1965]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0254]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9084]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3216]])\n",
      "Loss: 1.414363145828247\n",
      "\n",
      "> Iteration 1291/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0214]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6828]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4680]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1964]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0254]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9084]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3215]])\n",
      "Loss: 1.4252798557281494\n",
      "\n",
      "> Iteration 1292/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0215]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6827]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4680]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1964]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0254]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9084]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3215]])\n",
      "Loss: 1.390360713005066\n",
      "\n",
      "> Iteration 1293/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0215]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6827]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4680]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1964]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0254]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9085]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3410]]), B=tensor([[0.3214]])\n",
      "Loss: 1.4025391340255737\n",
      "\n",
      "> Iteration 1294/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0215]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6826]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4679]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1964]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0253]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9085]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3409]]), B=tensor([[0.3214]])\n",
      "Loss: 1.4008138179779053\n",
      "\n",
      "> Iteration 1295/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0215]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6826]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4679]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1964]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0253]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9085]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3409]]), B=tensor([[0.3214]])\n",
      "Loss: 1.3689161539077759\n",
      "\n",
      "> Iteration 1296/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0216]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6826]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4679]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1964]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0253]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9085]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3409]]), B=tensor([[0.3213]])\n",
      "Loss: 1.4202243089675903\n",
      "\n",
      "> Iteration 1297/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0216]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6825]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4678]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1964]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0253]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9085]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3409]]), B=tensor([[0.3213]])\n",
      "Loss: 1.3913049697875977\n",
      "\n",
      "> Iteration 1298/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0216]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6825]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4678]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1964]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0252]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9085]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3409]]), B=tensor([[0.3212]])\n",
      "Loss: 1.3734700679779053\n",
      "\n",
      "> Iteration 1299/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0216]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6825]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4677]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1964]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0252]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9085]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3408]]), B=tensor([[0.3212]])\n",
      "Loss: 1.3783771991729736\n",
      "\n",
      "> Iteration 1300/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0216]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6824]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4677]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1963]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0252]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9086]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3408]]), B=tensor([[0.3211]])\n",
      "Loss: 1.3989368677139282\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1301/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6824]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4676]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1963]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0252]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9086]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3408]]), B=tensor([[0.3211]])\n",
      "Loss: 1.3722535371780396\n",
      "\n",
      "> Iteration 1302/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6824]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4676]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1963]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0251]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9086]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3408]]), B=tensor([[0.3211]])\n",
      "Loss: 1.3725464344024658\n",
      "\n",
      "> Iteration 1303/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6823]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4676]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1963]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0251]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9086]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3407]]), B=tensor([[0.3210]])\n",
      "Loss: 1.413138508796692\n",
      "\n",
      "> Iteration 1304/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6823]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4675]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1963]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0251]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9086]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3407]]), B=tensor([[0.3210]])\n",
      "Loss: 1.3910776376724243\n",
      "\n",
      "> Iteration 1305/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6823]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4675]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1963]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0251]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9086]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3407]]), B=tensor([[0.3209]])\n",
      "Loss: 1.4264625310897827\n",
      "\n",
      "> Iteration 1306/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6822]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4674]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1963]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0250]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9086]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3406]]), B=tensor([[0.3209]])\n",
      "Loss: 1.3757907152175903\n",
      "\n",
      "> Iteration 1307/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6822]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4674]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1963]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0250]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9087]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3406]]), B=tensor([[0.3208]])\n",
      "Loss: 1.3762234449386597\n",
      "\n",
      "> Iteration 1308/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6821]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4674]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1963]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0250]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9087]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3405]]), B=tensor([[0.3208]])\n",
      "Loss: 1.3974943161010742\n",
      "\n",
      "> Iteration 1309/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6821]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4673]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1962]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0249]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9087]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3405]]), B=tensor([[0.3207]])\n",
      "Loss: 1.366256594657898\n",
      "\n",
      "> Iteration 1310/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6821]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4673]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1962]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0249]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9087]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3404]]), B=tensor([[0.3207]])\n",
      "Loss: 1.4224128723144531\n",
      "\n",
      "> Iteration 1311/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6820]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4672]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1962]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0249]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9087]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3404]]), B=tensor([[0.3206]])\n",
      "Loss: 1.390249252319336\n",
      "\n",
      "> Iteration 1312/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6820]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4672]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1962]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0248]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9087]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3403]]), B=tensor([[0.3206]])\n",
      "Loss: 1.3767999410629272\n",
      "\n",
      "> Iteration 1313/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6819]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4671]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1962]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0248]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9088]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3402]]), B=tensor([[0.3205]])\n",
      "Loss: 1.4236429929733276\n",
      "\n",
      "> Iteration 1314/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6819]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4671]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1962]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0248]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9088]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3402]]), B=tensor([[0.3205]])\n",
      "Loss: 1.362330436706543\n",
      "\n",
      "> Iteration 1315/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6819]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4670]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1962]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0247]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9088]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3401]]), B=tensor([[0.3204]])\n",
      "Loss: 1.3799785375595093\n",
      "\n",
      "> Iteration 1316/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6818]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4670]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1962]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0247]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9088]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3401]]), B=tensor([[0.3204]])\n",
      "Loss: 1.4373425245285034\n",
      "\n",
      "> Iteration 1317/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6818]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4669]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1961]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0247]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9088]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3400]]), B=tensor([[0.3203]])\n",
      "Loss: 1.377648115158081\n",
      "\n",
      "> Iteration 1318/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6817]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4669]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1961]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9089]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3400]]), B=tensor([[0.3202]])\n",
      "Loss: 1.3455413579940796\n",
      "\n",
      "> Iteration 1319/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6817]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4668]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1961]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9089]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3399]]), B=tensor([[0.3202]])\n",
      "Loss: 1.3474079370498657\n",
      "\n",
      "> Iteration 1320/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6816]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4668]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1961]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9089]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3399]]), B=tensor([[0.3202]])\n",
      "Loss: 1.3369669914245605\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1321/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6816]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4667]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1961]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0245]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9089]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3399]]), B=tensor([[0.3201]])\n",
      "Loss: 1.3805325031280518\n",
      "\n",
      "> Iteration 1322/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6816]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4667]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1961]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0245]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9089]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3398]]), B=tensor([[0.3201]])\n",
      "Loss: 1.4052523374557495\n",
      "\n",
      "> Iteration 1323/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6815]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4666]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1961]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0245]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9089]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3398]]), B=tensor([[0.3200]])\n",
      "Loss: 1.3790532350540161\n",
      "\n",
      "> Iteration 1324/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0218]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6815]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4666]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1961]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0245]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9089]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3397]]), B=tensor([[0.3200]])\n",
      "Loss: 1.3889374732971191\n",
      "\n",
      "> Iteration 1325/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0218]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6815]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4666]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1961]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0244]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9090]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3397]]), B=tensor([[0.3199]])\n",
      "Loss: 1.3691387176513672\n",
      "\n",
      "> Iteration 1326/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0218]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6814]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4665]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0244]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9090]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3397]]), B=tensor([[0.3199]])\n",
      "Loss: 1.3801379203796387\n",
      "\n",
      "> Iteration 1327/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0218]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6814]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4665]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0244]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9090]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3397]]), B=tensor([[0.3199]])\n",
      "Loss: 1.3699434995651245\n",
      "\n",
      "> Iteration 1328/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0218]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6814]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4665]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0244]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9090]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3397]]), B=tensor([[0.3198]])\n",
      "Loss: 1.4375689029693604\n",
      "\n",
      "> Iteration 1329/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0219]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6813]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4664]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0244]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9090]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3397]]), B=tensor([[0.3198]])\n",
      "Loss: 1.407050609588623\n",
      "\n",
      "> Iteration 1330/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0219]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6813]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4664]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0243]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9090]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3396]]), B=tensor([[0.3197]])\n",
      "Loss: 1.4062981605529785\n",
      "\n",
      "> Iteration 1331/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0219]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6813]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4663]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0243]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9090]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3396]]), B=tensor([[0.3197]])\n",
      "Loss: 1.4060872793197632\n",
      "\n",
      "> Iteration 1332/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0219]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6812]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4663]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0243]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9091]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3396]]), B=tensor([[0.3196]])\n",
      "Loss: 1.3897125720977783\n",
      "\n",
      "> Iteration 1333/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0220]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6812]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4662]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0243]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9091]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3396]]), B=tensor([[0.3196]])\n",
      "Loss: 1.4328250885009766\n",
      "\n",
      "> Iteration 1334/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0220]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6811]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4662]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1960]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9091]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3395]]), B=tensor([[0.3196]])\n",
      "Loss: 1.3923643827438354\n",
      "\n",
      "> Iteration 1335/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0220]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6811]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4662]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9091]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3395]]), B=tensor([[0.3195]])\n",
      "Loss: 1.3700603246688843\n",
      "\n",
      "> Iteration 1336/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0220]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6811]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4661]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9091]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3395]]), B=tensor([[0.3195]])\n",
      "Loss: 1.402578353881836\n",
      "\n",
      "> Iteration 1337/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0220]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6810]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4661]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9091]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3395]]), B=tensor([[0.3194]])\n",
      "Loss: 1.3498345613479614\n",
      "\n",
      "> Iteration 1338/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0221]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6810]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4661]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0241]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9091]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3394]]), B=tensor([[0.3194]])\n",
      "Loss: 1.3508085012435913\n",
      "\n",
      "> Iteration 1339/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0221]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6810]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4660]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0241]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9092]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3394]]), B=tensor([[0.3194]])\n",
      "Loss: 1.356751561164856\n",
      "\n",
      "> Iteration 1340/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0221]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6810]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4660]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0241]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9092]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3394]]), B=tensor([[0.3193]])\n",
      "Loss: 1.3829256296157837\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1341/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0221]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6809]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4659]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0241]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9092]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3394]]), B=tensor([[0.3193]])\n",
      "Loss: 1.3987098932266235\n",
      "\n",
      "> Iteration 1342/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0221]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6809]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4659]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0240]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9092]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3393]]), B=tensor([[0.3192]])\n",
      "Loss: 1.352432370185852\n",
      "\n",
      "> Iteration 1343/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0221]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6809]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4659]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0240]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9092]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3393]]), B=tensor([[0.3192]])\n",
      "Loss: 1.3768203258514404\n",
      "\n",
      "> Iteration 1344/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0221]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6808]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4658]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1959]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0240]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9092]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3393]]), B=tensor([[0.3192]])\n",
      "Loss: 1.3780031204223633\n",
      "\n",
      "> Iteration 1345/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0221]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6808]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4658]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1958]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0240]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9092]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3393]]), B=tensor([[0.3191]])\n",
      "Loss: 1.339635968208313\n",
      "\n",
      "> Iteration 1346/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0222]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6808]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4658]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1958]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0240]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9092]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3392]]), B=tensor([[0.3191]])\n",
      "Loss: 1.394787073135376\n",
      "\n",
      "> Iteration 1347/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0222]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6807]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4657]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1958]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0239]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9093]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3392]]), B=tensor([[0.3191]])\n",
      "Loss: 1.3775514364242554\n",
      "\n",
      "> Iteration 1348/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0222]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6807]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4657]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1958]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0239]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9093]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3392]]), B=tensor([[0.3190]])\n",
      "Loss: 1.3820396661758423\n",
      "\n",
      "> Iteration 1349/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0222]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6807]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4657]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1958]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0239]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9093]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3392]]), B=tensor([[0.3190]])\n",
      "Loss: 1.3915457725524902\n",
      "\n",
      "> Iteration 1350/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0222]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6806]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4656]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1958]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0239]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9093]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3392]]), B=tensor([[0.3189]])\n",
      "Loss: 1.3886561393737793\n",
      "\n",
      "> Iteration 1351/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0223]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6806]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4656]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1958]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0238]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9093]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3391]]), B=tensor([[0.3189]])\n",
      "Loss: 1.3426371812820435\n",
      "\n",
      "> Iteration 1352/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0223]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6806]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4655]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1958]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0238]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9093]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3391]]), B=tensor([[0.3189]])\n",
      "Loss: 1.3883517980575562\n",
      "\n",
      "> Iteration 1353/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0223]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6805]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4655]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1958]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0238]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9093]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3391]]), B=tensor([[0.3188]])\n",
      "Loss: 1.3479893207550049\n",
      "\n",
      "> Iteration 1354/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0223]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6805]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4655]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1957]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0238]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9094]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3391]]), B=tensor([[0.3188]])\n",
      "Loss: 1.3963165283203125\n",
      "\n",
      "> Iteration 1355/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0223]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6805]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4654]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1957]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0238]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9094]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3390]]), B=tensor([[0.3187]])\n",
      "Loss: 1.3767502307891846\n",
      "\n",
      "> Iteration 1356/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0224]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6804]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4654]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1957]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0237]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9094]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3390]]), B=tensor([[0.3187]])\n",
      "Loss: 1.3781232833862305\n",
      "\n",
      "> Iteration 1357/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0224]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6804]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4654]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1957]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0237]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9094]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3390]]), B=tensor([[0.3187]])\n",
      "Loss: 1.42156183719635\n",
      "\n",
      "> Iteration 1358/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0224]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6804]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4653]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1957]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0237]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9094]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3390]]), B=tensor([[0.3186]])\n",
      "Loss: 1.4100854396820068\n",
      "\n",
      "> Iteration 1359/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0224]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6803]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4653]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1957]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0237]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9094]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3389]]), B=tensor([[0.3186]])\n",
      "Loss: 1.3881272077560425\n",
      "\n",
      "> Iteration 1360/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0224]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6803]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4652]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1957]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0236]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9094]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3389]]), B=tensor([[0.3185]])\n",
      "Loss: 1.389687180519104\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1361/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0224]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6803]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4652]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1957]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0236]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9095]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3389]]), B=tensor([[0.3185]])\n",
      "Loss: 1.3748276233673096\n",
      "\n",
      "> Iteration 1362/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0225]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6802]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4651]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1957]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0236]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9095]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3389]]), B=tensor([[0.3184]])\n",
      "Loss: 1.3769557476043701\n",
      "\n",
      "> Iteration 1363/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0225]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6802]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4651]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1956]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0236]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9095]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3389]]), B=tensor([[0.3184]])\n",
      "Loss: 1.3868393898010254\n",
      "\n",
      "> Iteration 1364/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0225]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6801]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4651]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1956]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0235]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9095]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3388]]), B=tensor([[0.3183]])\n",
      "Loss: 1.3590328693389893\n",
      "\n",
      "> Iteration 1365/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0226]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6801]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4650]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1956]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0235]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9095]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3388]]), B=tensor([[0.3183]])\n",
      "Loss: 1.400051236152649\n",
      "\n",
      "> Iteration 1366/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0226]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6801]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4650]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1956]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0235]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9095]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3388]]), B=tensor([[0.3183]])\n",
      "Loss: 1.3952631950378418\n",
      "\n",
      "> Iteration 1367/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0226]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6800]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4649]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1956]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0235]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9095]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3388]]), B=tensor([[0.3182]])\n",
      "Loss: 1.3819736242294312\n",
      "\n",
      "> Iteration 1368/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0227]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6800]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4649]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1956]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0234]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9096]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3388]]), B=tensor([[0.3182]])\n",
      "Loss: 1.3570126295089722\n",
      "\n",
      "> Iteration 1369/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0227]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6799]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4648]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1956]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0234]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9096]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3388]]), B=tensor([[0.3181]])\n",
      "Loss: 1.3472687005996704\n",
      "\n",
      "> Iteration 1370/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0227]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6799]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4648]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1956]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0234]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9096]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3388]]), B=tensor([[0.3181]])\n",
      "Loss: 1.440995693206787\n",
      "\n",
      "> Iteration 1371/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0228]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6799]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4647]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1955]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0234]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9096]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.3180]])\n",
      "Loss: 1.3645635843276978\n",
      "\n",
      "> Iteration 1372/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0228]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6798]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4647]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1955]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0233]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9096]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.3180]])\n",
      "Loss: 1.399489402770996\n",
      "\n",
      "> Iteration 1373/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0228]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6798]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4647]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1955]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0233]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9096]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.3179]])\n",
      "Loss: 1.3556787967681885\n",
      "\n",
      "> Iteration 1374/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0229]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6798]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4646]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1955]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0233]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9097]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.3179]])\n",
      "Loss: 1.3775506019592285\n",
      "\n",
      "> Iteration 1375/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0229]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6797]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4646]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1955]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0233]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9097]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.3178]])\n",
      "Loss: 1.3463538885116577\n",
      "\n",
      "> Iteration 1376/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0229]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6797]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4645]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1955]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0232]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9097]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.3178]])\n",
      "Loss: 1.3633102178573608\n",
      "\n",
      "> Iteration 1377/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6796]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4645]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1955]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0232]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9097]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.3178]])\n",
      "Loss: 1.4186065196990967\n",
      "\n",
      "> Iteration 1378/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6796]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4645]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1955]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0232]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9097]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3386]]), B=tensor([[0.3177]])\n",
      "Loss: 1.3926750421524048\n",
      "\n",
      "> Iteration 1379/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6796]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4644]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0232]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9097]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3386]]), B=tensor([[0.3177]])\n",
      "Loss: 1.373475432395935\n",
      "\n",
      "> Iteration 1380/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6795]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4644]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0231]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9098]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3386]]), B=tensor([[0.3176]])\n",
      "Loss: 1.372313141822815\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1381/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6795]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4643]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0231]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9098]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.3176]])\n",
      "Loss: 1.379446268081665\n",
      "\n",
      "> Iteration 1382/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6795]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4643]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0231]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9098]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.3176]])\n",
      "Loss: 1.3683725595474243\n",
      "\n",
      "> Iteration 1383/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6794]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4643]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0231]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9098]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.3175]])\n",
      "Loss: 1.4161405563354492\n",
      "\n",
      "> Iteration 1384/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6794]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4642]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0230]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9098]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3384]]), B=tensor([[0.3175]])\n",
      "Loss: 1.386718511581421\n",
      "\n",
      "> Iteration 1385/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6794]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4642]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0230]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9098]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3384]]), B=tensor([[0.3174]])\n",
      "Loss: 1.375510573387146\n",
      "\n",
      "> Iteration 1386/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0231]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6793]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4641]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0230]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9098]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3384]]), B=tensor([[0.3174]])\n",
      "Loss: 1.3992663621902466\n",
      "\n",
      "> Iteration 1387/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0231]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6793]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4641]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0230]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9098]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3384]]), B=tensor([[0.3174]])\n",
      "Loss: 1.374782681465149\n",
      "\n",
      "> Iteration 1388/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0231]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6793]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4641]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1954]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0230]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9099]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3384]]), B=tensor([[0.3173]])\n",
      "Loss: 1.3932586908340454\n",
      "\n",
      "> Iteration 1389/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0231]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6792]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4640]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1953]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0229]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9099]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3383]]), B=tensor([[0.3173]])\n",
      "Loss: 1.384077548980713\n",
      "\n",
      "> Iteration 1390/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0232]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6792]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4640]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1953]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0229]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9099]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3383]]), B=tensor([[0.3172]])\n",
      "Loss: 1.3834823369979858\n",
      "\n",
      "> Iteration 1391/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0232]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6792]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4640]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1953]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0229]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9099]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3383]]), B=tensor([[0.3172]])\n",
      "Loss: 1.3769420385360718\n",
      "\n",
      "> Iteration 1392/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0232]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6791]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4639]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1953]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0229]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9099]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3382]]), B=tensor([[0.3171]])\n",
      "Loss: 1.3993176221847534\n",
      "\n",
      "> Iteration 1393/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0232]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6791]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4639]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1953]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0228]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9099]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3382]]), B=tensor([[0.3171]])\n",
      "Loss: 1.3690314292907715\n",
      "\n",
      "> Iteration 1394/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0232]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6791]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4638]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1953]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0228]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9099]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3382]]), B=tensor([[0.3171]])\n",
      "Loss: 1.404129147529602\n",
      "\n",
      "> Iteration 1395/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0232]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6790]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4638]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1953]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0228]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9100]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3382]]), B=tensor([[0.3170]])\n",
      "Loss: 1.3831108808517456\n",
      "\n",
      "> Iteration 1396/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0233]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6790]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4638]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1953]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0228]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9100]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3382]]), B=tensor([[0.3170]])\n",
      "Loss: 1.4170647859573364\n",
      "\n",
      "> Iteration 1397/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0233]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6790]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4637]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1952]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0228]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9100]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3382]]), B=tensor([[0.3169]])\n",
      "Loss: 1.3922775983810425\n",
      "\n",
      "> Iteration 1398/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0234]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6789]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4637]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1952]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9100]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3382]]), B=tensor([[0.3169]])\n",
      "Loss: 1.3362302780151367\n",
      "\n",
      "> Iteration 1399/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0234]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6789]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4636]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1952]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9100]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3381]]), B=tensor([[0.3169]])\n",
      "Loss: 1.3867264986038208\n",
      "\n",
      "> Iteration 1400/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0234]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6789]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4636]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1952]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9100]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3381]]), B=tensor([[0.3168]])\n",
      "Loss: 1.3880850076675415\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1401/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0234]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6788]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4635]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1952]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9101]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3381]]), B=tensor([[0.3168]])\n",
      "Loss: 1.39100182056427\n",
      "\n",
      "> Iteration 1402/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6788]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4635]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1952]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0226]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9101]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3381]]), B=tensor([[0.3167]])\n",
      "Loss: 1.3937523365020752\n",
      "\n",
      "> Iteration 1403/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6787]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4635]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1952]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0226]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9101]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3381]]), B=tensor([[0.3167]])\n",
      "Loss: 1.369510531425476\n",
      "\n",
      "> Iteration 1404/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6787]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4634]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1952]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0226]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9101]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3380]]), B=tensor([[0.3166]])\n",
      "Loss: 1.3902695178985596\n",
      "\n",
      "> Iteration 1405/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6787]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4634]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1952]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0226]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9101]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3380]]), B=tensor([[0.3166]])\n",
      "Loss: 1.4194440841674805\n",
      "\n",
      "> Iteration 1406/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6786]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4633]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1951]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0225]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9101]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3380]]), B=tensor([[0.3165]])\n",
      "Loss: 1.35065495967865\n",
      "\n",
      "> Iteration 1407/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6786]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4633]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1951]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0225]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9101]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3379]]), B=tensor([[0.3165]])\n",
      "Loss: 1.3746384382247925\n",
      "\n",
      "> Iteration 1408/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6786]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4632]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1951]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0225]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9102]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3379]]), B=tensor([[0.3165]])\n",
      "Loss: 1.3854621648788452\n",
      "\n",
      "> Iteration 1409/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6785]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4632]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1951]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0225]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9102]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3378]]), B=tensor([[0.3164]])\n",
      "Loss: 1.4012755155563354\n",
      "\n",
      "> Iteration 1410/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6785]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4632]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1951]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0224]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9102]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3378]]), B=tensor([[0.3164]])\n",
      "Loss: 1.3912270069122314\n",
      "\n",
      "> Iteration 1411/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6785]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4631]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1951]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0224]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9102]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3378]]), B=tensor([[0.3163]])\n",
      "Loss: 1.3793998956680298\n",
      "\n",
      "> Iteration 1412/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6784]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4631]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1951]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0224]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9102]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3377]]), B=tensor([[0.3163]])\n",
      "Loss: 1.36941397190094\n",
      "\n",
      "> Iteration 1413/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6784]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4631]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1951]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0224]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9102]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3377]]), B=tensor([[0.3162]])\n",
      "Loss: 1.374269723892212\n",
      "\n",
      "> Iteration 1414/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6784]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4630]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1951]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0223]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9102]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3376]]), B=tensor([[0.3162]])\n",
      "Loss: 1.3715189695358276\n",
      "\n",
      "> Iteration 1415/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6783]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4630]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0223]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9103]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3376]]), B=tensor([[0.3162]])\n",
      "Loss: 1.4172700643539429\n",
      "\n",
      "> Iteration 1416/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6783]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4629]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0223]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9103]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3376]]), B=tensor([[0.3161]])\n",
      "Loss: 1.3721588850021362\n",
      "\n",
      "> Iteration 1417/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6783]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4629]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0223]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9103]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3375]]), B=tensor([[0.3161]])\n",
      "Loss: 1.366932988166809\n",
      "\n",
      "> Iteration 1418/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6782]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4629]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0222]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9103]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3375]]), B=tensor([[0.3160]])\n",
      "Loss: 1.3841174840927124\n",
      "\n",
      "> Iteration 1419/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6782]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4628]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0222]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9103]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3374]]), B=tensor([[0.3160]])\n",
      "Loss: 1.4148333072662354\n",
      "\n",
      "> Iteration 1420/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6782]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4628]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0222]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9103]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3374]]), B=tensor([[0.3160]])\n",
      "Loss: 1.3887640237808228\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1421/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6781]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4628]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0222]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9103]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3373]]), B=tensor([[0.3159]])\n",
      "Loss: 1.3576456308364868\n",
      "\n",
      "> Iteration 1422/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6781]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4627]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0221]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9104]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3373]]), B=tensor([[0.3159]])\n",
      "Loss: 1.4201909303665161\n",
      "\n",
      "> Iteration 1423/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6780]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4627]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1950]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0221]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9104]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3373]]), B=tensor([[0.3158]])\n",
      "Loss: 1.3830479383468628\n",
      "\n",
      "> Iteration 1424/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6780]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4626]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1949]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0221]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9104]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3372]]), B=tensor([[0.3158]])\n",
      "Loss: 1.3549811840057373\n",
      "\n",
      "> Iteration 1425/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6780]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4626]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1949]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0220]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9104]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3372]]), B=tensor([[0.3157]])\n",
      "Loss: 1.4091243743896484\n",
      "\n",
      "> Iteration 1426/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6779]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4625]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1949]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0220]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9104]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3372]]), B=tensor([[0.3157]])\n",
      "Loss: 1.3783149719238281\n",
      "\n",
      "> Iteration 1427/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6779]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4625]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1949]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0220]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9104]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3372]]), B=tensor([[0.3157]])\n",
      "Loss: 1.3738901615142822\n",
      "\n",
      "> Iteration 1428/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6779]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4625]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1949]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0220]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9104]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3372]]), B=tensor([[0.3156]])\n",
      "Loss: 1.3691588640213013\n",
      "\n",
      "> Iteration 1429/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6778]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4624]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1949]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0220]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9105]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3371]]), B=tensor([[0.3156]])\n",
      "Loss: 1.3985484838485718\n",
      "\n",
      "> Iteration 1430/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6778]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4624]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1949]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0219]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9105]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3371]]), B=tensor([[0.3155]])\n",
      "Loss: 1.380920171737671\n",
      "\n",
      "> Iteration 1431/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6778]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4623]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1949]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0219]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9105]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3371]]), B=tensor([[0.3155]])\n",
      "Loss: 1.4364925622940063\n",
      "\n",
      "> Iteration 1432/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6777]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4623]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1948]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0219]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9105]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3370]]), B=tensor([[0.3154]])\n",
      "Loss: 1.3908300399780273\n",
      "\n",
      "> Iteration 1433/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6777]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4622]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1948]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0219]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9105]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3370]]), B=tensor([[0.3154]])\n",
      "Loss: 1.4053101539611816\n",
      "\n",
      "> Iteration 1434/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0238]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6777]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4622]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1948]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0218]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9105]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3370]]), B=tensor([[0.3153]])\n",
      "Loss: 1.3539975881576538\n",
      "\n",
      "> Iteration 1435/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0238]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6776]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4622]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1948]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0218]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9106]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3370]]), B=tensor([[0.3153]])\n",
      "Loss: 1.404590368270874\n",
      "\n",
      "> Iteration 1436/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0238]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6776]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4621]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1948]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0218]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9106]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3369]]), B=tensor([[0.3153]])\n",
      "Loss: 1.3860434293746948\n",
      "\n",
      "> Iteration 1437/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0238]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6776]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4621]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1948]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0218]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9106]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3369]]), B=tensor([[0.3152]])\n",
      "Loss: 1.3771626949310303\n",
      "\n",
      "> Iteration 1438/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0238]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6775]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4621]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1948]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9106]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3369]]), B=tensor([[0.3152]])\n",
      "Loss: 1.3752598762512207\n",
      "\n",
      "> Iteration 1439/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0238]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6775]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4620]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1948]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9106]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3369]]), B=tensor([[0.3151]])\n",
      "Loss: 1.3868050575256348\n",
      "\n",
      "> Iteration 1440/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0239]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6775]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4620]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1948]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9106]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3369]]), B=tensor([[0.3151]])\n",
      "Loss: 1.382737398147583\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1441/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0239]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6774]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4620]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1947]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9106]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3368]]), B=tensor([[0.3151]])\n",
      "Loss: 1.3677035570144653\n",
      "\n",
      "> Iteration 1442/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0239]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6774]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4619]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1947]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9106]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3368]]), B=tensor([[0.3150]])\n",
      "Loss: 1.4025744199752808\n",
      "\n",
      "> Iteration 1443/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0240]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6774]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4619]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1947]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0216]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9107]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3368]]), B=tensor([[0.3150]])\n",
      "Loss: 1.4607279300689697\n",
      "\n",
      "> Iteration 1444/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0240]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6773]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4618]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1947]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0216]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9107]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3368]]), B=tensor([[0.3150]])\n",
      "Loss: 1.3380248546600342\n",
      "\n",
      "> Iteration 1445/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0240]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6773]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4618]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1947]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0216]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9107]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3368]]), B=tensor([[0.3149]])\n",
      "Loss: 1.3654731512069702\n",
      "\n",
      "> Iteration 1446/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0240]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6773]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4618]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1947]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0216]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9107]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3368]]), B=tensor([[0.3149]])\n",
      "Loss: 1.397395372390747\n",
      "\n",
      "> Iteration 1447/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6772]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4617]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1947]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0216]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9107]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3368]]), B=tensor([[0.3148]])\n",
      "Loss: 1.4174704551696777\n",
      "\n",
      "> Iteration 1448/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6772]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4617]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1947]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0215]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9107]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3367]]), B=tensor([[0.3148]])\n",
      "Loss: 1.385087490081787\n",
      "\n",
      "> Iteration 1449/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6772]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4616]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1946]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0215]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9107]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3367]]), B=tensor([[0.3148]])\n",
      "Loss: 1.3678103685379028\n",
      "\n",
      "> Iteration 1450/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6771]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4616]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1946]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0215]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9108]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3367]]), B=tensor([[0.3147]])\n",
      "Loss: 1.3964933156967163\n",
      "\n",
      "> Iteration 1451/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6771]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4616]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1946]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0215]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9108]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3366]]), B=tensor([[0.3147]])\n",
      "Loss: 1.374833583831787\n",
      "\n",
      "> Iteration 1452/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6770]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4615]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1946]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0214]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9108]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3366]]), B=tensor([[0.3146]])\n",
      "Loss: 1.3836606740951538\n",
      "\n",
      "> Iteration 1453/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6770]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4615]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1946]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0214]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9108]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3366]]), B=tensor([[0.3146]])\n",
      "Loss: 1.418814778327942\n",
      "\n",
      "> Iteration 1454/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0242]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6770]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4614]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1946]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0214]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9108]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3365]]), B=tensor([[0.3145]])\n",
      "Loss: 1.354335904121399\n",
      "\n",
      "> Iteration 1455/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0242]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6769]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4614]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1946]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0214]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9108]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3365]]), B=tensor([[0.3145]])\n",
      "Loss: 1.3674821853637695\n",
      "\n",
      "> Iteration 1456/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0242]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6769]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4613]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1946]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9109]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3365]]), B=tensor([[0.3144]])\n",
      "Loss: 1.3579167127609253\n",
      "\n",
      "> Iteration 1457/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0242]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6769]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4613]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9109]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3365]]), B=tensor([[0.3144]])\n",
      "Loss: 1.3892550468444824\n",
      "\n",
      "> Iteration 1458/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0243]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6768]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4613]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9109]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3365]]), B=tensor([[0.3144]])\n",
      "Loss: 1.404081106185913\n",
      "\n",
      "> Iteration 1459/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0243]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6768]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4612]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9109]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3365]]), B=tensor([[0.3143]])\n",
      "Loss: 1.3695690631866455\n",
      "\n",
      "> Iteration 1460/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0243]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6768]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4612]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9109]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3365]]), B=tensor([[0.3143]])\n",
      "Loss: 1.3650590181350708\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1461/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0244]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6767]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4612]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9109]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3142]])\n",
      "Loss: 1.377358317375183\n",
      "\n",
      "> Iteration 1462/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0244]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6767]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4611]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9109]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3142]])\n",
      "Loss: 1.33061683177948\n",
      "\n",
      "> Iteration 1463/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0244]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6767]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4611]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9110]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3142]])\n",
      "Loss: 1.3862498998641968\n",
      "\n",
      "> Iteration 1464/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0245]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6766]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4611]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9110]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3141]])\n",
      "Loss: 1.381466269493103\n",
      "\n",
      "> Iteration 1465/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0245]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6766]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4610]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1945]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9110]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3141]])\n",
      "Loss: 1.3732259273529053\n",
      "\n",
      "> Iteration 1466/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0245]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6766]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4610]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1944]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9110]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3141]])\n",
      "Loss: 1.3613355159759521\n",
      "\n",
      "> Iteration 1467/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0246]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6765]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4609]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1944]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9110]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3140]])\n",
      "Loss: 1.4210807085037231\n",
      "\n",
      "> Iteration 1468/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0246]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6765]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4609]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1944]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9110]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3140]])\n",
      "Loss: 1.3784037828445435\n",
      "\n",
      "> Iteration 1469/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0246]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6765]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4609]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1944]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9110]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3140]])\n",
      "Loss: 1.4027931690216064\n",
      "\n",
      "> Iteration 1470/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0246]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6764]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4608]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1944]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9110]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.3139]])\n",
      "Loss: 1.3675167560577393\n",
      "\n",
      "> Iteration 1471/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0247]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6764]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4608]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1944]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9111]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3139]])\n",
      "Loss: 1.4147411584854126\n",
      "\n",
      "> Iteration 1472/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0247]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6764]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4607]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1944]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9111]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3138]])\n",
      "Loss: 1.3835617303848267\n",
      "\n",
      "> Iteration 1473/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0247]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6763]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4607]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1944]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9111]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3138]])\n",
      "Loss: 1.3763389587402344\n",
      "\n",
      "> Iteration 1474/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0248]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6763]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4607]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1943]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9111]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3137]])\n",
      "Loss: 1.3957452774047852\n",
      "\n",
      "> Iteration 1475/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0248]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6763]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4606]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1943]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9111]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3137]])\n",
      "Loss: 1.3530720472335815\n",
      "\n",
      "> Iteration 1476/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0248]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6762]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4606]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1943]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0209]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9111]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3137]])\n",
      "Loss: 1.3486424684524536\n",
      "\n",
      "> Iteration 1477/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0249]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6762]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4605]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1943]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0209]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9112]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3136]])\n",
      "Loss: 1.4032111167907715\n",
      "\n",
      "> Iteration 1478/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0249]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6762]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4605]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1943]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0209]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9112]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3136]])\n",
      "Loss: 1.400113821029663\n",
      "\n",
      "> Iteration 1479/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0249]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6761]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4605]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1943]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0209]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9112]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3136]])\n",
      "Loss: 1.3756331205368042\n",
      "\n",
      "> Iteration 1480/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0250]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6761]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4604]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1943]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0209]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9112]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3362]]), B=tensor([[0.3135]])\n",
      "Loss: 1.4167017936706543\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1481/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0250]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6761]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4604]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1943]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9112]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.3135]])\n",
      "Loss: 1.3491238355636597\n",
      "\n",
      "> Iteration 1482/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0250]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6760]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4604]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1943]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9112]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3362]]), B=tensor([[0.3134]])\n",
      "Loss: 1.4335198402404785\n",
      "\n",
      "> Iteration 1483/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0251]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6760]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4603]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1942]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9112]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3362]]), B=tensor([[0.3134]])\n",
      "Loss: 1.4011762142181396\n",
      "\n",
      "> Iteration 1484/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0251]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6760]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4603]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1942]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9113]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3362]]), B=tensor([[0.3134]])\n",
      "Loss: 1.3753231763839722\n",
      "\n",
      "> Iteration 1485/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0251]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6759]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4602]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1942]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9113]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3362]]), B=tensor([[0.3133]])\n",
      "Loss: 1.427985668182373\n",
      "\n",
      "> Iteration 1486/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0251]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6759]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4602]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1942]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0207]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9113]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3362]]), B=tensor([[0.3133]])\n",
      "Loss: 1.3803333044052124\n",
      "\n",
      "> Iteration 1487/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6759]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4602]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1942]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0207]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9113]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3362]]), B=tensor([[0.3132]])\n",
      "Loss: 1.3942420482635498\n",
      "\n",
      "> Iteration 1488/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6758]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4601]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1942]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0207]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9113]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3361]]), B=tensor([[0.3132]])\n",
      "Loss: 1.4087727069854736\n",
      "\n",
      "> Iteration 1489/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6758]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4601]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1942]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0207]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9113]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3361]]), B=tensor([[0.3131]])\n",
      "Loss: 1.3875906467437744\n",
      "\n",
      "> Iteration 1490/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6757]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4600]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1941]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9113]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3361]]), B=tensor([[0.3131]])\n",
      "Loss: 1.3754805326461792\n",
      "\n",
      "> Iteration 1491/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6757]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4600]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1941]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9114]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3360]]), B=tensor([[0.3131]])\n",
      "Loss: 1.3749221563339233\n",
      "\n",
      "> Iteration 1492/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6757]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4599]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1941]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9114]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3360]]), B=tensor([[0.3130]])\n",
      "Loss: 1.3991038799285889\n",
      "\n",
      "> Iteration 1493/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6756]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4599]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1941]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9114]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3360]]), B=tensor([[0.3130]])\n",
      "Loss: 1.3578147888183594\n",
      "\n",
      "> Iteration 1494/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6756]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4599]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1941]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0205]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9114]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3359]]), B=tensor([[0.3129]])\n",
      "Loss: 1.3522202968597412\n",
      "\n",
      "> Iteration 1495/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6756]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4598]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1941]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0205]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9114]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3359]]), B=tensor([[0.3129]])\n",
      "Loss: 1.382729411125183\n",
      "\n",
      "> Iteration 1496/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6755]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4598]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1941]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0205]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9114]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3359]]), B=tensor([[0.3128]])\n",
      "Loss: 1.3989194631576538\n",
      "\n",
      "> Iteration 1497/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6755]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4597]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1941]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0205]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9115]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.3128]])\n",
      "Loss: 1.3860723972320557\n",
      "\n",
      "> Iteration 1498/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6755]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4597]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1940]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9115]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.3127]])\n",
      "Loss: 1.3728245496749878\n",
      "\n",
      "> Iteration 1499/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6754]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4597]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1940]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9115]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3357]]), B=tensor([[0.3127]])\n",
      "Loss: 1.3722484111785889\n",
      "\n",
      "> Iteration 1500/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6754]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4596]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1940]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9115]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3357]]), B=tensor([[0.3127]])\n",
      "Loss: 1.3880531787872314\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1501/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6754]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4596]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1940]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9115]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3356]]), B=tensor([[0.3126]])\n",
      "Loss: 1.3815356492996216\n",
      "\n",
      "> Iteration 1502/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6753]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4595]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1940]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0203]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9115]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3356]]), B=tensor([[0.3126]])\n",
      "Loss: 1.389983057975769\n",
      "\n",
      "> Iteration 1503/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6753]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4595]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1940]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0203]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9115]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3355]]), B=tensor([[0.3125]])\n",
      "Loss: 1.4040368795394897\n",
      "\n",
      "> Iteration 1504/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6752]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4595]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1940]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0203]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9116]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3355]]), B=tensor([[0.3125]])\n",
      "Loss: 1.4148210287094116\n",
      "\n",
      "> Iteration 1505/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6752]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4594]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1940]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0203]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9116]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3354]]), B=tensor([[0.3124]])\n",
      "Loss: 1.4097816944122314\n",
      "\n",
      "> Iteration 1506/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6752]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4594]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1939]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0202]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9116]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3354]]), B=tensor([[0.3124]])\n",
      "Loss: 1.368759036064148\n",
      "\n",
      "> Iteration 1507/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6751]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4593]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1939]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0202]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9116]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3354]]), B=tensor([[0.3123]])\n",
      "Loss: 1.3913649320602417\n",
      "\n",
      "> Iteration 1508/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6751]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4593]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1939]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0202]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9116]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.3123]])\n",
      "Loss: 1.4462262392044067\n",
      "\n",
      "> Iteration 1509/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6750]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4592]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1939]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9116]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.3122]])\n",
      "Loss: 1.4073115587234497\n",
      "\n",
      "> Iteration 1510/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6750]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4592]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1939]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9117]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3352]]), B=tensor([[0.3122]])\n",
      "Loss: 1.3796744346618652\n",
      "\n",
      "> Iteration 1511/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6750]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4591]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1939]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9117]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3352]]), B=tensor([[0.3121]])\n",
      "Loss: 1.3470994234085083\n",
      "\n",
      "> Iteration 1512/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6749]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4591]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1939]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9117]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3351]]), B=tensor([[0.3121]])\n",
      "Loss: 1.359687328338623\n",
      "\n",
      "> Iteration 1513/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6749]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4591]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1939]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0200]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9117]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3351]]), B=tensor([[0.3121]])\n",
      "Loss: 1.3331856727600098\n",
      "\n",
      "> Iteration 1514/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6749]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4590]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1938]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0200]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9117]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3351]]), B=tensor([[0.3120]])\n",
      "Loss: 1.4026310443878174\n",
      "\n",
      "> Iteration 1515/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6748]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4590]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1938]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0200]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9117]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3350]]), B=tensor([[0.3120]])\n",
      "Loss: 1.4028220176696777\n",
      "\n",
      "> Iteration 1516/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6748]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4589]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1938]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0200]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9117]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3350]]), B=tensor([[0.3119]])\n",
      "Loss: 1.3886125087738037\n",
      "\n",
      "> Iteration 1517/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6748]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4589]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1938]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0199]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9118]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3350]]), B=tensor([[0.3119]])\n",
      "Loss: 1.3606547117233276\n",
      "\n",
      "> Iteration 1518/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6747]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4589]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1938]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0199]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9118]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3349]]), B=tensor([[0.3118]])\n",
      "Loss: 1.410108208656311\n",
      "\n",
      "> Iteration 1519/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6747]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4588]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1938]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0199]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9118]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3349]]), B=tensor([[0.3118]])\n",
      "Loss: 1.3910630941390991\n",
      "\n",
      "> Iteration 1520/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6747]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4588]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1938]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0199]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9118]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3348]]), B=tensor([[0.3118]])\n",
      "Loss: 1.3844162225723267\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1521/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6746]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4588]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1938]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0199]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9118]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3348]]), B=tensor([[0.3117]])\n",
      "Loss: 1.3741405010223389\n",
      "\n",
      "> Iteration 1522/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6746]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4587]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1938]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0198]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9118]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3348]]), B=tensor([[0.3117]])\n",
      "Loss: 1.4036505222320557\n",
      "\n",
      "> Iteration 1523/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6746]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4587]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1937]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0198]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9118]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3347]]), B=tensor([[0.3116]])\n",
      "Loss: 1.4183541536331177\n",
      "\n",
      "> Iteration 1524/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6745]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4586]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1937]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0198]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9119]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3347]]), B=tensor([[0.3116]])\n",
      "Loss: 1.4099971055984497\n",
      "\n",
      "> Iteration 1525/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6745]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4586]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1937]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0198]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9119]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3347]]), B=tensor([[0.3116]])\n",
      "Loss: 1.3872231245040894\n",
      "\n",
      "> Iteration 1526/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6745]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4586]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1937]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0197]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9119]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3346]]), B=tensor([[0.3115]])\n",
      "Loss: 1.3789186477661133\n",
      "\n",
      "> Iteration 1527/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6745]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4585]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1937]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0197]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9119]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3346]]), B=tensor([[0.3115]])\n",
      "Loss: 1.3802595138549805\n",
      "\n",
      "> Iteration 1528/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6744]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4585]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1937]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0197]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9119]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3345]]), B=tensor([[0.3115]])\n",
      "Loss: 1.4151051044464111\n",
      "\n",
      "> Iteration 1529/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6744]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4585]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1937]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0197]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9119]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3345]]), B=tensor([[0.3114]])\n",
      "Loss: 1.4016610383987427\n",
      "\n",
      "> Iteration 1530/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6744]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4584]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1937]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0197]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9119]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3345]]), B=tensor([[0.3114]])\n",
      "Loss: 1.3827439546585083\n",
      "\n",
      "> Iteration 1531/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6743]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4584]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1937]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0196]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9119]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3344]]), B=tensor([[0.3113]])\n",
      "Loss: 1.4039286375045776\n",
      "\n",
      "> Iteration 1532/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6743]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4584]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1936]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0196]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9120]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3344]]), B=tensor([[0.3113]])\n",
      "Loss: 1.3634718656539917\n",
      "\n",
      "> Iteration 1533/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6743]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4583]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1936]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0196]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9120]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3344]]), B=tensor([[0.3113]])\n",
      "Loss: 1.3683451414108276\n",
      "\n",
      "> Iteration 1534/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0254]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6742]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4583]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1936]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0196]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9120]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3112]])\n",
      "Loss: 1.3897536993026733\n",
      "\n",
      "> Iteration 1535/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0254]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6742]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4583]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1936]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0195]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9120]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3112]])\n",
      "Loss: 1.3608644008636475\n",
      "\n",
      "> Iteration 1536/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0254]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6742]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4582]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1936]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0195]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9120]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3112]])\n",
      "Loss: 1.387426495552063\n",
      "\n",
      "> Iteration 1537/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0254]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6741]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4582]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1936]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0195]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9120]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3111]])\n",
      "Loss: 1.403050422668457\n",
      "\n",
      "> Iteration 1538/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0255]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6741]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4581]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1936]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0195]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9120]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3111]])\n",
      "Loss: 1.3909039497375488\n",
      "\n",
      "> Iteration 1539/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0255]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6741]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4581]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1936]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0195]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9121]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3110]])\n",
      "Loss: 1.409098505973816\n",
      "\n",
      "> Iteration 1540/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0255]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6740]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4581]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0195]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9121]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3110]])\n",
      "Loss: 1.3718246221542358\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1541/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0256]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6740]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4580]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0194]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9121]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3110]])\n",
      "Loss: 1.3692138195037842\n",
      "\n",
      "> Iteration 1542/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0256]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6740]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4580]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0194]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9121]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3109]])\n",
      "Loss: 1.3893530368804932\n",
      "\n",
      "> Iteration 1543/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0256]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6739]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4580]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0194]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9121]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3343]]), B=tensor([[0.3109]])\n",
      "Loss: 1.3930232524871826\n",
      "\n",
      "> Iteration 1544/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0256]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6739]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4579]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0194]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9121]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3342]]), B=tensor([[0.3109]])\n",
      "Loss: 1.4224965572357178\n",
      "\n",
      "> Iteration 1545/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0257]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6739]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4579]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0194]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9121]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3342]]), B=tensor([[0.3108]])\n",
      "Loss: 1.3558725118637085\n",
      "\n",
      "> Iteration 1546/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0257]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6739]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4579]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0193]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9121]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3342]]), B=tensor([[0.3108]])\n",
      "Loss: 1.3965107202529907\n",
      "\n",
      "> Iteration 1547/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0257]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6738]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4578]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0193]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9122]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3342]]), B=tensor([[0.3107]])\n",
      "Loss: 1.3421926498413086\n",
      "\n",
      "> Iteration 1548/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0258]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6738]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4578]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1935]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0193]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9122]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3342]]), B=tensor([[0.3107]])\n",
      "Loss: 1.3718372583389282\n",
      "\n",
      "> Iteration 1549/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0258]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6738]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4577]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1934]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0193]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9122]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3342]]), B=tensor([[0.3107]])\n",
      "Loss: 1.3967329263687134\n",
      "\n",
      "> Iteration 1550/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0258]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6737]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4577]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1934]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0193]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9122]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3342]]), B=tensor([[0.3106]])\n",
      "Loss: 1.4043357372283936\n",
      "\n",
      "> Iteration 1551/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0258]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6737]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4577]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1934]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0192]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9122]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3342]]), B=tensor([[0.3106]])\n",
      "Loss: 1.3551311492919922\n",
      "\n",
      "> Iteration 1552/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0259]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6737]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4576]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1934]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0192]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9122]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3341]]), B=tensor([[0.3106]])\n",
      "Loss: 1.3736084699630737\n",
      "\n",
      "> Iteration 1553/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0259]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6736]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4576]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1934]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0192]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9122]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3341]]), B=tensor([[0.3105]])\n",
      "Loss: 1.375016450881958\n",
      "\n",
      "> Iteration 1554/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0259]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6736]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4576]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1934]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0192]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9123]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3341]]), B=tensor([[0.3105]])\n",
      "Loss: 1.3903858661651611\n",
      "\n",
      "> Iteration 1555/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0259]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6736]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4575]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1934]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0192]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9123]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3341]]), B=tensor([[0.3104]])\n",
      "Loss: 1.3960185050964355\n",
      "\n",
      "> Iteration 1556/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0259]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6735]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4575]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1934]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0191]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9123]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3340]]), B=tensor([[0.3104]])\n",
      "Loss: 1.450323462486267\n",
      "\n",
      "> Iteration 1557/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0259]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6735]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4574]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1933]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0191]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9123]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3340]]), B=tensor([[0.3104]])\n",
      "Loss: 1.3483335971832275\n",
      "\n",
      "> Iteration 1558/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0259]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6735]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4574]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1933]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0191]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9123]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3340]]), B=tensor([[0.3103]])\n",
      "Loss: 1.4482191801071167\n",
      "\n",
      "> Iteration 1559/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0259]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6734]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4574]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1933]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0191]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9123]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3339]]), B=tensor([[0.3103]])\n",
      "Loss: 1.4074174165725708\n",
      "\n",
      "> Iteration 1560/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0259]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6734]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4573]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1933]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0191]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9123]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3339]]), B=tensor([[0.3102]])\n",
      "Loss: 1.3809421062469482\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1561/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0260]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6734]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4573]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1933]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0190]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9124]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3339]]), B=tensor([[0.3102]])\n",
      "Loss: 1.4268484115600586\n",
      "\n",
      "> Iteration 1562/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0260]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6733]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4573]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1933]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0190]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9124]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3339]]), B=tensor([[0.3102]])\n",
      "Loss: 1.3790839910507202\n",
      "\n",
      "> Iteration 1563/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0260]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6733]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4572]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1933]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0190]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9124]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3339]]), B=tensor([[0.3101]])\n",
      "Loss: 1.3963252305984497\n",
      "\n",
      "> Iteration 1564/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0260]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6733]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4572]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1933]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0190]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9124]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3338]]), B=tensor([[0.3101]])\n",
      "Loss: 1.4223250150680542\n",
      "\n",
      "> Iteration 1565/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0260]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6732]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4572]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1933]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0190]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9124]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3338]]), B=tensor([[0.3101]])\n",
      "Loss: 1.3848427534103394\n",
      "\n",
      "> Iteration 1566/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0261]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6732]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4571]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1932]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0189]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9124]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3338]]), B=tensor([[0.3100]])\n",
      "Loss: 1.3839774131774902\n",
      "\n",
      "> Iteration 1567/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0261]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6732]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4571]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1932]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0189]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9124]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3338]]), B=tensor([[0.3100]])\n",
      "Loss: 1.367301106452942\n",
      "\n",
      "> Iteration 1568/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0261]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6731]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4570]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1932]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0189]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9124]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3338]]), B=tensor([[0.3099]])\n",
      "Loss: 1.3856885433197021\n",
      "\n",
      "> Iteration 1569/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0262]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6731]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4570]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1932]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0189]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9125]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3338]]), B=tensor([[0.3099]])\n",
      "Loss: 1.3548158407211304\n",
      "\n",
      "> Iteration 1570/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0262]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6731]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4570]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1932]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0189]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9125]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3338]]), B=tensor([[0.3099]])\n",
      "Loss: 1.4214739799499512\n",
      "\n",
      "> Iteration 1571/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0262]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6730]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4569]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1932]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0188]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9125]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3337]]), B=tensor([[0.3098]])\n",
      "Loss: 1.3864033222198486\n",
      "\n",
      "> Iteration 1572/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0262]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6730]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4569]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1932]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0188]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9125]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3337]]), B=tensor([[0.3098]])\n",
      "Loss: 1.380873203277588\n",
      "\n",
      "> Iteration 1573/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0262]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6730]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4568]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1932]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0188]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9125]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3337]]), B=tensor([[0.3097]])\n",
      "Loss: 1.3904531002044678\n",
      "\n",
      "> Iteration 1574/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0262]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6729]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4568]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1931]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0188]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9125]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3336]]), B=tensor([[0.3097]])\n",
      "Loss: 1.3575161695480347\n",
      "\n",
      "> Iteration 1575/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0262]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6729]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4567]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1931]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0187]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9126]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3336]]), B=tensor([[0.3096]])\n",
      "Loss: 1.4214985370635986\n",
      "\n",
      "> Iteration 1576/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0263]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6729]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4567]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1931]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0187]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9126]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3336]]), B=tensor([[0.3096]])\n",
      "Loss: 1.3511687517166138\n",
      "\n",
      "> Iteration 1577/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0263]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6728]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4567]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1931]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0187]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9126]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3095]])\n",
      "Loss: 1.3892656564712524\n",
      "\n",
      "> Iteration 1578/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0263]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6728]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4566]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1931]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0187]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9126]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3095]])\n",
      "Loss: 1.3469221591949463\n",
      "\n",
      "> Iteration 1579/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0263]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6727]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4566]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1931]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0186]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9126]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3095]])\n",
      "Loss: 1.4033381938934326\n",
      "\n",
      "> Iteration 1580/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0263]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6727]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4565]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1931]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0186]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9126]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3094]])\n",
      "Loss: 1.353230357170105\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1581/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0263]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6727]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4565]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1930]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0186]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9126]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3094]])\n",
      "Loss: 1.3606840372085571\n",
      "\n",
      "> Iteration 1582/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0263]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6727]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4565]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1930]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0186]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9127]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3093]])\n",
      "Loss: 1.3900853395462036\n",
      "\n",
      "> Iteration 1583/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0263]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6726]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4564]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1930]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0186]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9127]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3093]])\n",
      "Loss: 1.3633909225463867\n",
      "\n",
      "> Iteration 1584/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0264]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6726]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4564]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1930]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0186]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9127]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3093]])\n",
      "Loss: 1.3933037519454956\n",
      "\n",
      "> Iteration 1585/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0264]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6726]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4564]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1930]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0185]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9127]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3092]])\n",
      "Loss: 1.3871760368347168\n",
      "\n",
      "> Iteration 1586/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0265]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6725]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4563]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1930]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0185]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9127]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3092]])\n",
      "Loss: 1.3542369604110718\n",
      "\n",
      "> Iteration 1587/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0265]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6725]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4563]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1930]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0185]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9127]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3092]])\n",
      "Loss: 1.3707188367843628\n",
      "\n",
      "> Iteration 1588/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0266]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6725]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4563]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1930]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0185]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9127]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3092]])\n",
      "Loss: 1.415269374847412\n",
      "\n",
      "> Iteration 1589/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0266]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6725]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4562]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1930]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0185]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9127]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3091]])\n",
      "Loss: 1.3613321781158447\n",
      "\n",
      "> Iteration 1590/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0267]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6724]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4562]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1929]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0185]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9128]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3091]])\n",
      "Loss: 1.3943265676498413\n",
      "\n",
      "> Iteration 1591/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0267]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6724]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4562]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1929]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0185]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9128]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3091]])\n",
      "Loss: 1.3656682968139648\n",
      "\n",
      "> Iteration 1592/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0268]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6724]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4561]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1929]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9128]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3090]])\n",
      "Loss: 1.3695307970046997\n",
      "\n",
      "> Iteration 1593/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0268]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6723]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4561]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1929]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9128]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3090]])\n",
      "Loss: 1.3610424995422363\n",
      "\n",
      "> Iteration 1594/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0268]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6723]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4561]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1929]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9128]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3090]])\n",
      "Loss: 1.4199672937393188\n",
      "\n",
      "> Iteration 1595/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0269]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6723]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4560]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1929]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9128]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3089]])\n",
      "Loss: 1.400589108467102\n",
      "\n",
      "> Iteration 1596/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0269]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6722]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4560]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1929]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9128]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3089]])\n",
      "Loss: 1.4051395654678345\n",
      "\n",
      "> Iteration 1597/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0270]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6722]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4560]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1929]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9129]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3088]])\n",
      "Loss: 1.3854050636291504\n",
      "\n",
      "> Iteration 1598/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0270]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6722]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4559]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1928]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0183]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9129]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3088]])\n",
      "Loss: 1.3695530891418457\n",
      "\n",
      "> Iteration 1599/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0270]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6721]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4559]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1928]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0183]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9129]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3088]])\n",
      "Loss: 1.3715869188308716\n",
      "\n",
      "> Iteration 1600/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0271]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6721]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4558]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1928]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0183]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9129]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3087]])\n",
      "Loss: 1.3942335844039917\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1601/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0271]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6721]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4558]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1928]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0183]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9129]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3087]])\n",
      "Loss: 1.375886082649231\n",
      "\n",
      "> Iteration 1602/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0271]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6720]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4558]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1928]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0183]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9129]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3086]])\n",
      "Loss: 1.3859797716140747\n",
      "\n",
      "> Iteration 1603/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0272]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6720]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4557]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1928]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0183]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9129]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3086]])\n",
      "Loss: 1.4374977350234985\n",
      "\n",
      "> Iteration 1604/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0272]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6720]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4557]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1928]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9130]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3086]])\n",
      "Loss: 1.4405378103256226\n",
      "\n",
      "> Iteration 1605/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0272]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6719]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4556]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1927]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9130]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3085]])\n",
      "Loss: 1.3454540967941284\n",
      "\n",
      "> Iteration 1606/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0273]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6719]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4556]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1927]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9130]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3085]])\n",
      "Loss: 1.3929165601730347\n",
      "\n",
      "> Iteration 1607/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0273]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6719]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4556]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1927]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9130]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3085]])\n",
      "Loss: 1.39542555809021\n",
      "\n",
      "> Iteration 1608/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0274]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6718]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4555]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1927]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9130]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3084]])\n",
      "Loss: 1.3668315410614014\n",
      "\n",
      "> Iteration 1609/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0275]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6718]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4555]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1927]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9130]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3084]])\n",
      "Loss: 1.3905762434005737\n",
      "\n",
      "> Iteration 1610/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0275]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6718]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4555]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1927]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9130]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3084]])\n",
      "Loss: 1.3685044050216675\n",
      "\n",
      "> Iteration 1611/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0276]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6717]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4554]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1927]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9131]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3083]])\n",
      "Loss: 1.3971495628356934\n",
      "\n",
      "> Iteration 1612/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0276]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6717]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4554]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1927]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9131]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3083]])\n",
      "Loss: 1.3770616054534912\n",
      "\n",
      "> Iteration 1613/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0277]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6717]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4553]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1926]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9131]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3082]])\n",
      "Loss: 1.3891524076461792\n",
      "\n",
      "> Iteration 1614/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0277]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6716]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4553]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1926]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9131]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3082]])\n",
      "Loss: 1.4331163167953491\n",
      "\n",
      "> Iteration 1615/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0278]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6716]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4553]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1926]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9131]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3082]])\n",
      "Loss: 1.3656994104385376\n",
      "\n",
      "> Iteration 1616/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0278]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6715]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4552]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1926]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9131]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3081]])\n",
      "Loss: 1.4045624732971191\n",
      "\n",
      "> Iteration 1617/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0279]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6715]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4552]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1926]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9132]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3081]])\n",
      "Loss: 1.391884207725525\n",
      "\n",
      "> Iteration 1618/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0280]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6715]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4551]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1926]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9132]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3336]]), B=tensor([[0.3080]])\n",
      "Loss: 1.406790852546692\n",
      "\n",
      "> Iteration 1619/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0280]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6714]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4551]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1925]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9132]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3336]]), B=tensor([[0.3080]])\n",
      "Loss: 1.381076455116272\n",
      "\n",
      "> Iteration 1620/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0281]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6714]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4550]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1925]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9132]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3336]]), B=tensor([[0.3079]])\n",
      "Loss: 1.3849234580993652\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1621/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0281]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6714]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4550]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1925]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9132]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3336]]), B=tensor([[0.3079]])\n",
      "Loss: 1.3842782974243164\n",
      "\n",
      "> Iteration 1622/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0281]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6713]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4549]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1925]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9132]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3079]])\n",
      "Loss: 1.4071311950683594\n",
      "\n",
      "> Iteration 1623/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0282]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6713]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4549]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1925]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9133]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3078]])\n",
      "Loss: 1.3800251483917236\n",
      "\n",
      "> Iteration 1624/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0282]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6712]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4549]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1925]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9133]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3078]])\n",
      "Loss: 1.4250123500823975\n",
      "\n",
      "> Iteration 1625/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0282]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6712]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4548]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1924]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9133]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3077]])\n",
      "Loss: 1.4029366970062256\n",
      "\n",
      "> Iteration 1626/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0283]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6712]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4548]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1924]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0178]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9133]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3077]])\n",
      "Loss: 1.3726977109909058\n",
      "\n",
      "> Iteration 1627/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0283]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6711]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4547]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1924]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0178]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9133]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3076]])\n",
      "Loss: 1.3913202285766602\n",
      "\n",
      "> Iteration 1628/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0283]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6711]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4547]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1924]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0178]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9134]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.3076]])\n",
      "Loss: 1.3972140550613403\n",
      "\n",
      "> Iteration 1629/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0284]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6710]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4546]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1924]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0178]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9134]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3075]])\n",
      "Loss: 1.3875325918197632\n",
      "\n",
      "> Iteration 1630/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0284]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6710]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4546]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1924]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0178]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9134]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3075]])\n",
      "Loss: 1.3935801982879639\n",
      "\n",
      "> Iteration 1631/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0284]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6710]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4546]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1924]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0178]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9134]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3075]])\n",
      "Loss: 1.3770766258239746\n",
      "\n",
      "> Iteration 1632/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0284]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6710]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4545]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1924]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0177]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9134]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3074]])\n",
      "Loss: 1.3818399906158447\n",
      "\n",
      "> Iteration 1633/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0285]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6709]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4545]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1923]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0177]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9134]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3074]])\n",
      "Loss: 1.3653661012649536\n",
      "\n",
      "> Iteration 1634/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0285]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6709]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4545]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1923]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0177]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9134]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3074]])\n",
      "Loss: 1.3773541450500488\n",
      "\n",
      "> Iteration 1635/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0285]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6709]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4544]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1923]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0177]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9134]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3073]])\n",
      "Loss: 1.339386224746704\n",
      "\n",
      "> Iteration 1636/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0285]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6708]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4544]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1923]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0177]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9135]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3073]])\n",
      "Loss: 1.3759605884552002\n",
      "\n",
      "> Iteration 1637/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0286]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6708]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4544]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1923]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0177]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9135]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.3073]])\n",
      "Loss: 1.3644894361495972\n",
      "\n",
      "> Iteration 1638/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0286]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6708]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4543]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1923]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0176]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9135]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.3072]])\n",
      "Loss: 1.3914368152618408\n",
      "\n",
      "> Iteration 1639/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0286]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6707]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4543]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1923]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0176]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9135]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.3072]])\n",
      "Loss: 1.376992106437683\n",
      "\n",
      "> Iteration 1640/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0286]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6707]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4543]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1923]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0176]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9135]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.3072]])\n",
      "Loss: 1.3850741386413574\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1641/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0286]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6707]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4542]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1922]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0176]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9135]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.3071]])\n",
      "Loss: 1.3428157567977905\n",
      "\n",
      "> Iteration 1642/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0287]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6707]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4542]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1922]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0176]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9135]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.3071]])\n",
      "Loss: 1.3889029026031494\n",
      "\n",
      "> Iteration 1643/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0287]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6706]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4541]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1922]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0175]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9136]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3332]]), B=tensor([[0.3070]])\n",
      "Loss: 1.4438447952270508\n",
      "\n",
      "> Iteration 1644/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0287]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6706]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4541]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1922]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0175]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9136]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3332]]), B=tensor([[0.3070]])\n",
      "Loss: 1.3987557888031006\n",
      "\n",
      "> Iteration 1645/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0287]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6705]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4541]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1922]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0175]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9136]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3332]]), B=tensor([[0.3069]])\n",
      "Loss: 1.3703761100769043\n",
      "\n",
      "> Iteration 1646/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0287]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6705]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4540]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1922]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0175]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9136]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3069]])\n",
      "Loss: 1.3997728824615479\n",
      "\n",
      "> Iteration 1647/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0287]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6705]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4540]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1922]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0175]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9136]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3069]])\n",
      "Loss: 1.3354949951171875\n",
      "\n",
      "> Iteration 1648/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0287]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6705]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4540]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1922]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9136]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3068]])\n",
      "Loss: 1.396538257598877\n",
      "\n",
      "> Iteration 1649/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0287]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6704]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4539]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9136]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3068]])\n",
      "Loss: 1.3918776512145996\n",
      "\n",
      "> Iteration 1650/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0288]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6704]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4539]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3068]])\n",
      "Loss: 1.388620138168335\n",
      "\n",
      "> Iteration 1651/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0288]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6704]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4539]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3330]]), B=tensor([[0.3067]])\n",
      "Loss: 1.3657933473587036\n",
      "\n",
      "> Iteration 1652/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0288]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6703]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4538]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3067]])\n",
      "Loss: 1.4147169589996338\n",
      "\n",
      "> Iteration 1653/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0289]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6703]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4538]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3067]])\n",
      "Loss: 1.341013789176941\n",
      "\n",
      "> Iteration 1654/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0289]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6703]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4538]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3067]])\n",
      "Loss: 1.4019417762756348\n",
      "\n",
      "> Iteration 1655/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0289]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6703]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4538]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3066]])\n",
      "Loss: 1.3592820167541504\n",
      "\n",
      "> Iteration 1656/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0289]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6703]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4537]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0173]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3066]])\n",
      "Loss: 1.3624452352523804\n",
      "\n",
      "> Iteration 1657/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6702]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4537]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1921]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0173]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.3066]])\n",
      "Loss: 1.3395423889160156\n",
      "\n",
      "> Iteration 1658/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6702]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4537]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0173]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3330]]), B=tensor([[0.3066]])\n",
      "Loss: 1.384848952293396\n",
      "\n",
      "> Iteration 1659/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6702]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4536]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0173]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9137]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3330]]), B=tensor([[0.3065]])\n",
      "Loss: 1.3876187801361084\n",
      "\n",
      "> Iteration 1660/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6702]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4536]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0173]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9138]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3330]]), B=tensor([[0.3065]])\n",
      "Loss: 1.3643288612365723\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1661/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6701]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4536]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0173]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9138]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3330]]), B=tensor([[0.3065]])\n",
      "Loss: 1.3651371002197266\n",
      "\n",
      "> Iteration 1662/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6701]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4536]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0173]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9138]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3329]]), B=tensor([[0.3064]])\n",
      "Loss: 1.4241844415664673\n",
      "\n",
      "> Iteration 1663/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6701]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4535]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9138]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3329]]), B=tensor([[0.3064]])\n",
      "Loss: 1.4012062549591064\n",
      "\n",
      "> Iteration 1664/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6701]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4535]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9138]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3329]]), B=tensor([[0.3064]])\n",
      "Loss: 1.396949291229248\n",
      "\n",
      "> Iteration 1665/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6700]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4535]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9138]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3329]]), B=tensor([[0.3063]])\n",
      "Loss: 1.3939746618270874\n",
      "\n",
      "> Iteration 1666/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6700]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4534]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9138]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3328]]), B=tensor([[0.3063]])\n",
      "Loss: 1.3825914859771729\n",
      "\n",
      "> Iteration 1667/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6700]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4534]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1920]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9138]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3328]]), B=tensor([[0.3063]])\n",
      "Loss: 1.402557611465454\n",
      "\n",
      "> Iteration 1668/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6699]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4534]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1919]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9139]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3328]]), B=tensor([[0.3062]])\n",
      "Loss: 1.3907954692840576\n",
      "\n",
      "> Iteration 1669/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6699]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4533]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1919]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0171]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9139]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3327]]), B=tensor([[0.3062]])\n",
      "Loss: 1.3929070234298706\n",
      "\n",
      "> Iteration 1670/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6699]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4533]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1919]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0171]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9139]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3327]]), B=tensor([[0.3062]])\n",
      "Loss: 1.351353406906128\n",
      "\n",
      "> Iteration 1671/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6699]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4533]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1919]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0171]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9139]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3327]]), B=tensor([[0.3061]])\n",
      "Loss: 1.4088220596313477\n",
      "\n",
      "> Iteration 1672/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6698]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4533]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1919]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0171]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9139]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3061]])\n",
      "Loss: 1.359418511390686\n",
      "\n",
      "> Iteration 1673/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6698]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4532]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1919]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0171]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9139]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3061]])\n",
      "Loss: 1.379997730255127\n",
      "\n",
      "> Iteration 1674/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6698]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4532]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1919]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9139]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3060]])\n",
      "Loss: 1.3549331426620483\n",
      "\n",
      "> Iteration 1675/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0291]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6698]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4532]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1919]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9139]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3060]])\n",
      "Loss: 1.3613027334213257\n",
      "\n",
      "> Iteration 1676/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0291]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6697]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4531]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1919]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9139]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.3060]])\n",
      "Loss: 1.3976647853851318\n",
      "\n",
      "> Iteration 1677/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0291]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6697]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4531]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1918]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9140]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.3060]])\n",
      "Loss: 1.3861453533172607\n",
      "\n",
      "> Iteration 1678/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0291]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6697]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4531]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1918]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9140]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3059]])\n",
      "Loss: 1.4024016857147217\n",
      "\n",
      "> Iteration 1679/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0292]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6697]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4530]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1918]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9140]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3059]])\n",
      "Loss: 1.4098039865493774\n",
      "\n",
      "> Iteration 1680/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0292]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6696]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4530]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1918]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9140]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3059]])\n",
      "Loss: 1.3814220428466797\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1681/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0292]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6696]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4530]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1918]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9140]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3058]])\n",
      "Loss: 1.373673677444458\n",
      "\n",
      "> Iteration 1682/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0293]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6696]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4529]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1918]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9140]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3058]])\n",
      "Loss: 1.3892295360565186\n",
      "\n",
      "> Iteration 1683/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0293]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6695]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4529]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1918]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9140]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.3058]])\n",
      "Loss: 1.417539358139038\n",
      "\n",
      "> Iteration 1684/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0293]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6695]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4529]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1918]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9140]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.3057]])\n",
      "Loss: 1.3832499980926514\n",
      "\n",
      "> Iteration 1685/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0294]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6695]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4528]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1917]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9141]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.3057]])\n",
      "Loss: 1.3899680376052856\n",
      "\n",
      "> Iteration 1686/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0294]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6694]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4528]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1917]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9141]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.3056]])\n",
      "Loss: 1.411548137664795\n",
      "\n",
      "> Iteration 1687/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0294]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6694]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4527]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1917]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9141]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.3056]])\n",
      "Loss: 1.4261873960494995\n",
      "\n",
      "> Iteration 1688/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6694]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4527]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1917]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0168]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9141]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.3056]])\n",
      "Loss: 1.3486441373825073\n",
      "\n",
      "> Iteration 1689/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6693]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4527]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1917]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0168]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9141]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.3055]])\n",
      "Loss: 1.4287782907485962\n",
      "\n",
      "> Iteration 1690/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6693]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4526]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1917]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0168]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9141]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.3055]])\n",
      "Loss: 1.3780419826507568\n",
      "\n",
      "> Iteration 1691/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6693]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4526]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1917]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0168]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9142]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.3054]])\n",
      "Loss: 1.411728858947754\n",
      "\n",
      "> Iteration 1692/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6692]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4525]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1916]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0167]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9142]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.3054]])\n",
      "Loss: 1.3688199520111084\n",
      "\n",
      "> Iteration 1693/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6692]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4525]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1916]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0167]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9142]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.3053]])\n",
      "Loss: 1.3711682558059692\n",
      "\n",
      "> Iteration 1694/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6691]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4524]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1916]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0167]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9142]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3053]])\n",
      "Loss: 1.3657108545303345\n",
      "\n",
      "> Iteration 1695/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6691]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4524]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1916]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0167]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9142]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3052]])\n",
      "Loss: 1.3684687614440918\n",
      "\n",
      "> Iteration 1696/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0295]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6691]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4523]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1916]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0166]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9142]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3052]])\n",
      "Loss: 1.3782546520233154\n",
      "\n",
      "> Iteration 1697/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0296]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6690]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4523]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1916]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0166]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9143]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3051]])\n",
      "Loss: 1.4229319095611572\n",
      "\n",
      "> Iteration 1698/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0296]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6690]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4523]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1915]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0166]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9143]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3051]])\n",
      "Loss: 1.365272879600525\n",
      "\n",
      "> Iteration 1699/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0296]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6689]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4522]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1915]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0166]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9143]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3050]])\n",
      "Loss: 1.4131120443344116\n",
      "\n",
      "> Iteration 1700/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0296]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6689]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4522]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1915]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0166]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9143]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3050]])\n",
      "Loss: 1.3897950649261475\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1701/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0296]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6689]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4521]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1915]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0165]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9143]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3050]])\n",
      "Loss: 1.377772569656372\n",
      "\n",
      "> Iteration 1702/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0297]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6688]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4521]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1915]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0165]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9143]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3049]])\n",
      "Loss: 1.3924877643585205\n",
      "\n",
      "> Iteration 1703/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0297]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6688]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4520]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1915]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0165]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9144]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3049]])\n",
      "Loss: 1.4165678024291992\n",
      "\n",
      "> Iteration 1704/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0297]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6688]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4520]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1914]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0165]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9144]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3048]])\n",
      "Loss: 1.3614412546157837\n",
      "\n",
      "> Iteration 1705/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0298]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6687]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4520]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1914]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0165]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9144]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3048]])\n",
      "Loss: 1.4239863157272339\n",
      "\n",
      "> Iteration 1706/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0298]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6687]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4519]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1914]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0164]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9144]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3047]])\n",
      "Loss: 1.394503116607666\n",
      "\n",
      "> Iteration 1707/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0298]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6686]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4519]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1914]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0164]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9144]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3047]])\n",
      "Loss: 1.3105863332748413\n",
      "\n",
      "> Iteration 1708/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0299]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6686]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4518]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1914]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0164]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9144]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3046]])\n",
      "Loss: 1.402327537536621\n",
      "\n",
      "> Iteration 1709/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0299]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6686]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4518]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1914]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0164]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9145]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3046]])\n",
      "Loss: 1.39781653881073\n",
      "\n",
      "> Iteration 1710/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0299]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6685]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4517]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1913]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0164]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9145]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3046]])\n",
      "Loss: 1.3458820581436157\n",
      "\n",
      "> Iteration 1711/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0300]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6685]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4517]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1913]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9145]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3045]])\n",
      "Loss: 1.373843789100647\n",
      "\n",
      "> Iteration 1712/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0300]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6685]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4517]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1913]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9145]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3045]])\n",
      "Loss: 1.408251166343689\n",
      "\n",
      "> Iteration 1713/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0300]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6684]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4516]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1913]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9145]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3044]])\n",
      "Loss: 1.3716567754745483\n",
      "\n",
      "> Iteration 1714/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0301]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6684]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4516]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1913]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9145]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3044]])\n",
      "Loss: 1.3571412563323975\n",
      "\n",
      "> Iteration 1715/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0301]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6684]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4515]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1913]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9146]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3044]])\n",
      "Loss: 1.3503694534301758\n",
      "\n",
      "> Iteration 1716/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0302]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6683]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4515]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1913]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9146]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3043]])\n",
      "Loss: 1.3616870641708374\n",
      "\n",
      "> Iteration 1717/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0302]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6683]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4515]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1912]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9146]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3043]])\n",
      "Loss: 1.3671016693115234\n",
      "\n",
      "> Iteration 1718/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0303]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6683]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4515]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1912]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0162]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9146]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3043]])\n",
      "Loss: 1.339004397392273\n",
      "\n",
      "> Iteration 1719/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0303]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6682]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4514]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1912]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0162]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9146]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3042]])\n",
      "Loss: 1.3972444534301758\n",
      "\n",
      "> Iteration 1720/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0304]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6682]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4514]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1912]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0162]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9146]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3042]])\n",
      "Loss: 1.3783646821975708\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1721/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0304]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6682]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4514]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1912]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0162]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9146]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3042]])\n",
      "Loss: 1.3730844259262085\n",
      "\n",
      "> Iteration 1722/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0305]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6682]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4513]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1912]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0162]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9146]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3042]])\n",
      "Loss: 1.3954836130142212\n",
      "\n",
      "> Iteration 1723/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0305]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6681]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4513]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1912]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0162]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9147]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3041]])\n",
      "Loss: 1.400770664215088\n",
      "\n",
      "> Iteration 1724/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0306]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6681]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4513]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1912]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0162]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9147]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3041]])\n",
      "Loss: 1.3924307823181152\n",
      "\n",
      "> Iteration 1725/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0306]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6681]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4512]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1911]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0162]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9147]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3041]])\n",
      "Loss: 1.4008690118789673\n",
      "\n",
      "> Iteration 1726/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0307]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6680]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4512]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1911]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9147]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3040]])\n",
      "Loss: 1.3667761087417603\n",
      "\n",
      "> Iteration 1727/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0307]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6680]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4511]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1911]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9147]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3040]])\n",
      "Loss: 1.4003735780715942\n",
      "\n",
      "> Iteration 1728/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0307]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6680]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4511]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1911]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9147]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3039]])\n",
      "Loss: 1.374483346939087\n",
      "\n",
      "> Iteration 1729/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0308]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6679]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4511]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1911]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9148]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3039]])\n",
      "Loss: 1.4184154272079468\n",
      "\n",
      "> Iteration 1730/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0308]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6679]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4510]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1911]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9148]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3039]])\n",
      "Loss: 1.3724501132965088\n",
      "\n",
      "> Iteration 1731/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0309]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6679]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4510]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1911]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9148]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3038]])\n",
      "Loss: 1.3939268589019775\n",
      "\n",
      "> Iteration 1732/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0309]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6678]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4510]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1910]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9148]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3038]])\n",
      "Loss: 1.3581706285476685\n",
      "\n",
      "> Iteration 1733/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0309]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6678]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4509]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1910]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9148]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3038]])\n",
      "Loss: 1.3743067979812622\n",
      "\n",
      "> Iteration 1734/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0310]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6678]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4509]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1910]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9148]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3037]])\n",
      "Loss: 1.4405598640441895\n",
      "\n",
      "> Iteration 1735/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0310]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6678]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4509]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1910]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9148]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3037]])\n",
      "Loss: 1.3599796295166016\n",
      "\n",
      "> Iteration 1736/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0310]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6677]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4508]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1910]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9148]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3037]])\n",
      "Loss: 1.3977969884872437\n",
      "\n",
      "> Iteration 1737/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0310]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6677]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4508]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1910]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9149]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3036]])\n",
      "Loss: 1.3769876956939697\n",
      "\n",
      "> Iteration 1738/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0311]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6677]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4508]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1910]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9149]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3036]])\n",
      "Loss: 1.4241371154785156\n",
      "\n",
      "> Iteration 1739/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0311]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6676]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4507]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1909]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9149]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3036]])\n",
      "Loss: 1.3547824621200562\n",
      "\n",
      "> Iteration 1740/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0312]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6676]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4507]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1909]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9149]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3035]])\n",
      "Loss: 1.3668420314788818\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1741/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0312]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6676]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4507]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1909]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9149]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3035]])\n",
      "Loss: 1.3738869428634644\n",
      "\n",
      "> Iteration 1742/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0312]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6675]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4506]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1909]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9149]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3035]])\n",
      "Loss: 1.3951374292373657\n",
      "\n",
      "> Iteration 1743/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6675]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4506]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1909]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9149]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3034]])\n",
      "Loss: 1.3667922019958496\n",
      "\n",
      "> Iteration 1744/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6675]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4506]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1909]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3034]])\n",
      "Loss: 1.3617825508117676\n",
      "\n",
      "> Iteration 1745/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6675]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4505]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1909]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3034]])\n",
      "Loss: 1.3949180841445923\n",
      "\n",
      "> Iteration 1746/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0314]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6674]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4505]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1909]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3034]])\n",
      "Loss: 1.37803316116333\n",
      "\n",
      "> Iteration 1747/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0314]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6674]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4505]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1908]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3033]])\n",
      "Loss: 1.407909870147705\n",
      "\n",
      "> Iteration 1748/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0315]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6674]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4504]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1908]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3033]])\n",
      "Loss: 1.3234575986862183\n",
      "\n",
      "> Iteration 1749/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0315]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6674]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4504]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1908]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3033]])\n",
      "Loss: 1.3355190753936768\n",
      "\n",
      "> Iteration 1750/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0315]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6673]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4504]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1908]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3032]])\n",
      "Loss: 1.38472580909729\n",
      "\n",
      "> Iteration 1751/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0316]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6673]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4504]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1908]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3032]])\n",
      "Loss: 1.357234001159668\n",
      "\n",
      "> Iteration 1752/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0316]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6673]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4503]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1908]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9150]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3032]])\n",
      "Loss: 1.3652797937393188\n",
      "\n",
      "> Iteration 1753/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0316]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6673]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4503]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1908]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9151]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3032]])\n",
      "Loss: 1.3970469236373901\n",
      "\n",
      "> Iteration 1754/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0317]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6672]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4503]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1908]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9151]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3031]])\n",
      "Loss: 1.4070872068405151\n",
      "\n",
      "> Iteration 1755/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0317]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6672]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4502]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1907]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9151]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3031]])\n",
      "Loss: 1.3518855571746826\n",
      "\n",
      "> Iteration 1756/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0318]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6672]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4502]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1907]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9151]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3031]])\n",
      "Loss: 1.3730792999267578\n",
      "\n",
      "> Iteration 1757/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0318]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6672]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4502]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1907]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9151]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3030]])\n",
      "Loss: 1.3997280597686768\n",
      "\n",
      "> Iteration 1758/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0318]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6671]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4502]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1907]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9151]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3030]])\n",
      "Loss: 1.324233889579773\n",
      "\n",
      "> Iteration 1759/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0319]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6671]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4501]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1907]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9151]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3030]])\n",
      "Loss: 1.3558449745178223\n",
      "\n",
      "> Iteration 1760/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0319]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6671]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4501]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1907]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9151]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3030]])\n",
      "Loss: 1.3708823919296265\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1761/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0320]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6671]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4501]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1907]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9152]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3029]])\n",
      "Loss: 1.3959120512008667\n",
      "\n",
      "> Iteration 1762/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0320]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6670]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4500]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1907]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9152]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3029]])\n",
      "Loss: 1.403632402420044\n",
      "\n",
      "> Iteration 1763/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0320]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6670]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4500]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1906]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9152]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3029]])\n",
      "Loss: 1.380765676498413\n",
      "\n",
      "> Iteration 1764/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0321]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6670]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4499]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1906]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9152]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3028]])\n",
      "Loss: 1.4255608320236206\n",
      "\n",
      "> Iteration 1765/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0321]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6669]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4499]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1906]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9152]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3028]])\n",
      "Loss: 1.37904953956604\n",
      "\n",
      "> Iteration 1766/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0321]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6669]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4499]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1906]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9152]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3028]])\n",
      "Loss: 1.3961611986160278\n",
      "\n",
      "> Iteration 1767/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0322]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6669]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4499]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1906]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9152]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3027]])\n",
      "Loss: 1.3730216026306152\n",
      "\n",
      "> Iteration 1768/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0322]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6669]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4498]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1906]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9153]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3027]])\n",
      "Loss: 1.4049261808395386\n",
      "\n",
      "> Iteration 1769/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0322]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6668]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4498]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1906]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9153]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3027]])\n",
      "Loss: 1.3667458295822144\n",
      "\n",
      "> Iteration 1770/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6668]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4498]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1905]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9153]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3026]])\n",
      "Loss: 1.3751219511032104\n",
      "\n",
      "> Iteration 1771/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6668]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4497]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1905]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9153]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.3026]])\n",
      "Loss: 1.373119592666626\n",
      "\n",
      "> Iteration 1772/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6667]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4497]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1905]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9153]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3026]])\n",
      "Loss: 1.3791170120239258\n",
      "\n",
      "> Iteration 1773/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6667]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4497]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1905]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9153]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3025]])\n",
      "Loss: 1.3727623224258423\n",
      "\n",
      "> Iteration 1774/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6667]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4496]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1905]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9153]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3025]])\n",
      "Loss: 1.420860767364502\n",
      "\n",
      "> Iteration 1775/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6666]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4496]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1905]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9153]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3025]])\n",
      "Loss: 1.3769737482070923\n",
      "\n",
      "> Iteration 1776/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6666]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4495]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1905]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9154]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.3024]])\n",
      "Loss: 1.374723196029663\n",
      "\n",
      "> Iteration 1777/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6666]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4495]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1905]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9154]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3024]])\n",
      "Loss: 1.3867677450180054\n",
      "\n",
      "> Iteration 1778/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6666]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4495]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1904]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9154]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3023]])\n",
      "Loss: 1.4247896671295166\n",
      "\n",
      "> Iteration 1779/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6665]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4494]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1904]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9154]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3023]])\n",
      "Loss: 1.4101301431655884\n",
      "\n",
      "> Iteration 1780/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6665]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4494]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1904]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9154]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3023]])\n",
      "Loss: 1.3727970123291016\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1781/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6665]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4494]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1904]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9154]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3022]])\n",
      "Loss: 1.3441673517227173\n",
      "\n",
      "> Iteration 1782/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6664]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4493]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1904]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9154]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3022]])\n",
      "Loss: 1.3672370910644531\n",
      "\n",
      "> Iteration 1783/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6664]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4493]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1904]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9155]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3022]])\n",
      "Loss: 1.3949015140533447\n",
      "\n",
      "> Iteration 1784/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6664]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4493]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1904]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9155]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3021]])\n",
      "Loss: 1.388755202293396\n",
      "\n",
      "> Iteration 1785/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6663]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4492]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1903]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9155]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3021]])\n",
      "Loss: 1.3678449392318726\n",
      "\n",
      "> Iteration 1786/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6663]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4492]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1903]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9155]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3021]])\n",
      "Loss: 1.396660327911377\n",
      "\n",
      "> Iteration 1787/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6663]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4492]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1903]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9155]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3020]])\n",
      "Loss: 1.383531928062439\n",
      "\n",
      "> Iteration 1788/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6662]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4491]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1903]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9155]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3020]])\n",
      "Loss: 1.3602286577224731\n",
      "\n",
      "> Iteration 1789/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6662]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4491]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1903]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9155]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3019]])\n",
      "Loss: 1.3803080320358276\n",
      "\n",
      "> Iteration 1790/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6662]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4491]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1903]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9156]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3019]])\n",
      "Loss: 1.3843731880187988\n",
      "\n",
      "> Iteration 1791/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0327]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6662]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4490]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1903]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9156]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3019]])\n",
      "Loss: 1.395137071609497\n",
      "\n",
      "> Iteration 1792/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0327]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6661]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4490]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1902]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9156]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3018]])\n",
      "Loss: 1.4055079221725464\n",
      "\n",
      "> Iteration 1793/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0328]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6661]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4490]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1902]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9156]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3018]])\n",
      "Loss: 1.3723158836364746\n",
      "\n",
      "> Iteration 1794/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0328]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6661]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4489]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1902]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9156]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3018]])\n",
      "Loss: 1.3926116228103638\n",
      "\n",
      "> Iteration 1795/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0329]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6660]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4489]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1902]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9156]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3017]])\n",
      "Loss: 1.4018828868865967\n",
      "\n",
      "> Iteration 1796/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0329]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6660]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4489]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1902]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9156]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3017]])\n",
      "Loss: 1.4195207357406616\n",
      "\n",
      "> Iteration 1797/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0330]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6660]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4488]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1902]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9157]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3017]])\n",
      "Loss: 1.3838719129562378\n",
      "\n",
      "> Iteration 1798/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0330]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6659]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4488]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1902]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9157]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3016]])\n",
      "Loss: 1.3789863586425781\n",
      "\n",
      "> Iteration 1799/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0330]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6659]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4487]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1901]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9157]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3016]])\n",
      "Loss: 1.3580293655395508\n",
      "\n",
      "> Iteration 1800/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6659]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4487]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1901]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9157]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3016]])\n",
      "Loss: 1.3843061923980713\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1801/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6659]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4487]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1901]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9157]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3015]])\n",
      "Loss: 1.4194540977478027\n",
      "\n",
      "> Iteration 1802/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0332]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6658]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4486]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1901]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9157]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3015]])\n",
      "Loss: 1.3742376565933228\n",
      "\n",
      "> Iteration 1803/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0332]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6658]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4486]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1901]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9157]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3015]])\n",
      "Loss: 1.338181495666504\n",
      "\n",
      "> Iteration 1804/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0332]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6658]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4486]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1901]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9158]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3014]])\n",
      "Loss: 1.3376734256744385\n",
      "\n",
      "> Iteration 1805/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0333]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6657]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4485]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1900]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9158]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3014]])\n",
      "Loss: 1.3875749111175537\n",
      "\n",
      "> Iteration 1806/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0333]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6657]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4485]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1900]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9158]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3014]])\n",
      "Loss: 1.421598196029663\n",
      "\n",
      "> Iteration 1807/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0334]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6657]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4485]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1900]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9158]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3013]])\n",
      "Loss: 1.3692131042480469\n",
      "\n",
      "> Iteration 1808/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0334]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6656]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4484]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1900]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9158]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3013]])\n",
      "Loss: 1.4106661081314087\n",
      "\n",
      "> Iteration 1809/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0335]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6656]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4484]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1900]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9158]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3013]])\n",
      "Loss: 1.397655963897705\n",
      "\n",
      "> Iteration 1810/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0335]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6656]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4483]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1900]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9159]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3012]])\n",
      "Loss: 1.4148402214050293\n",
      "\n",
      "> Iteration 1811/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0336]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6655]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4483]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1899]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9159]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3012]])\n",
      "Loss: 1.3413069248199463\n",
      "\n",
      "> Iteration 1812/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0336]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6655]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4483]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1899]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9159]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3012]])\n",
      "Loss: 1.390737533569336\n",
      "\n",
      "> Iteration 1813/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0336]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6655]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4482]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1899]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9159]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3011]])\n",
      "Loss: 1.360538363456726\n",
      "\n",
      "> Iteration 1814/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0336]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6654]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4482]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1899]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9159]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3011]])\n",
      "Loss: 1.362064242362976\n",
      "\n",
      "> Iteration 1815/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0337]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6654]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4482]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1899]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9159]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3010]])\n",
      "Loss: 1.3852452039718628\n",
      "\n",
      "> Iteration 1816/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0337]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6654]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4481]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1899]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9159]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3010]])\n",
      "Loss: 1.3712948560714722\n",
      "\n",
      "> Iteration 1817/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0337]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6654]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4481]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1899]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9159]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3010]])\n",
      "Loss: 1.3631558418273926\n",
      "\n",
      "> Iteration 1818/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0338]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6653]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4481]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1899]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9160]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3010]])\n",
      "Loss: 1.371435284614563\n",
      "\n",
      "> Iteration 1819/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0338]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6653]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4480]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1898]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9160]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3009]])\n",
      "Loss: 1.3677608966827393\n",
      "\n",
      "> Iteration 1820/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0338]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6653]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4480]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1898]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9160]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3009]])\n",
      "Loss: 1.3918381929397583\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1821/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0338]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6652]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4480]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1898]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9160]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3009]])\n",
      "Loss: 1.3563027381896973\n",
      "\n",
      "> Iteration 1822/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0339]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6652]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4479]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1898]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9160]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3008]])\n",
      "Loss: 1.3685723543167114\n",
      "\n",
      "> Iteration 1823/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0339]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6652]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4479]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1898]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9160]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3008]])\n",
      "Loss: 1.378045916557312\n",
      "\n",
      "> Iteration 1824/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0339]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6652]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4479]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1898]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9160]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3008]])\n",
      "Loss: 1.3571014404296875\n",
      "\n",
      "> Iteration 1825/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0340]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6651]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4478]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1898]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9161]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3007]])\n",
      "Loss: 1.3355945348739624\n",
      "\n",
      "> Iteration 1826/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0340]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6651]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4478]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1897]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9161]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3007]])\n",
      "Loss: 1.4489597082138062\n",
      "\n",
      "> Iteration 1827/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0340]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6651]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4478]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1897]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9161]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3007]])\n",
      "Loss: 1.3955743312835693\n",
      "\n",
      "> Iteration 1828/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0340]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6651]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4478]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1897]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9161]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3006]])\n",
      "Loss: 1.443860650062561\n",
      "\n",
      "> Iteration 1829/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0341]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6650]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4477]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1897]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9161]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3006]])\n",
      "Loss: 1.3723653554916382\n",
      "\n",
      "> Iteration 1830/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0341]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6650]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4477]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1897]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9161]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3006]])\n",
      "Loss: 1.3637174367904663\n",
      "\n",
      "> Iteration 1831/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0342]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6650]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4477]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1897]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9161]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3318]]), B=tensor([[0.3006]])\n",
      "Loss: 1.3761272430419922\n",
      "\n",
      "> Iteration 1832/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0342]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6649]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4476]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1897]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9161]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3005]])\n",
      "Loss: 1.3999587297439575\n",
      "\n",
      "> Iteration 1833/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6649]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4476]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1896]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9162]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3005]])\n",
      "Loss: 1.3400146961212158\n",
      "\n",
      "> Iteration 1834/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6649]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4476]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1896]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9162]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3005]])\n",
      "Loss: 1.4065477848052979\n",
      "\n",
      "> Iteration 1835/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0344]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6649]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4475]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1896]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9162]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3004]])\n",
      "Loss: 1.3923325538635254\n",
      "\n",
      "> Iteration 1836/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0344]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6648]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4475]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1896]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9162]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3004]])\n",
      "Loss: 1.359676480293274\n",
      "\n",
      "> Iteration 1837/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0344]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6648]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4475]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1896]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9162]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3004]])\n",
      "Loss: 1.3613494634628296\n",
      "\n",
      "> Iteration 1838/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0345]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6648]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4474]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1896]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9162]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3003]])\n",
      "Loss: 1.4324886798858643\n",
      "\n",
      "> Iteration 1839/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0345]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6648]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4474]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1896]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9162]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3003]])\n",
      "Loss: 1.39460027217865\n",
      "\n",
      "> Iteration 1840/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0345]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6647]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4474]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1895]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9162]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3003]])\n",
      "Loss: 1.3975895643234253\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1841/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0345]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6647]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4473]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1895]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9163]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3002]])\n",
      "Loss: 1.4000016450881958\n",
      "\n",
      "> Iteration 1842/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0346]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6647]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4473]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1895]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9163]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3002]])\n",
      "Loss: 1.3893646001815796\n",
      "\n",
      "> Iteration 1843/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0346]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6646]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4473]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1895]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9163]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3002]])\n",
      "Loss: 1.32668137550354\n",
      "\n",
      "> Iteration 1844/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0346]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6646]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4472]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1895]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9163]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3001]])\n",
      "Loss: 1.3626867532730103\n",
      "\n",
      "> Iteration 1845/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0347]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6646]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4472]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1895]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9163]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3001]])\n",
      "Loss: 1.3488811254501343\n",
      "\n",
      "> Iteration 1846/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0348]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6645]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4472]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1895]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9163]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3319]]), B=tensor([[0.3001]])\n",
      "Loss: 1.4328892230987549\n",
      "\n",
      "> Iteration 1847/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0348]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6645]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4471]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1894]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9163]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3000]])\n",
      "Loss: 1.3892838954925537\n",
      "\n",
      "> Iteration 1848/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0349]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6645]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4471]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1894]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9164]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3000]])\n",
      "Loss: 1.3618296384811401\n",
      "\n",
      "> Iteration 1849/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0349]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6645]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4471]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1894]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9164]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3000]])\n",
      "Loss: 1.4117889404296875\n",
      "\n",
      "> Iteration 1850/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0350]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6644]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4470]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1894]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9164]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.3000]])\n",
      "Loss: 1.3824278116226196\n",
      "\n",
      "> Iteration 1851/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0350]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6644]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4470]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1894]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9164]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2999]])\n",
      "Loss: 1.3664257526397705\n",
      "\n",
      "> Iteration 1852/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0351]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6644]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4470]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1894]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9164]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2999]])\n",
      "Loss: 1.4137839078903198\n",
      "\n",
      "> Iteration 1853/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0351]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6644]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4470]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1894]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9164]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2999]])\n",
      "Loss: 1.3961517810821533\n",
      "\n",
      "> Iteration 1854/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0352]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6643]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4469]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1893]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9164]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2998]])\n",
      "Loss: 1.3811982870101929\n",
      "\n",
      "> Iteration 1855/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0352]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6643]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4469]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1893]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9165]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2998]])\n",
      "Loss: 1.3691962957382202\n",
      "\n",
      "> Iteration 1856/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0352]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6643]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4469]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1893]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9165]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2998]])\n",
      "Loss: 1.3789422512054443\n",
      "\n",
      "> Iteration 1857/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0353]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6643]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4468]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1893]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9165]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2998]])\n",
      "Loss: 1.4197280406951904\n",
      "\n",
      "> Iteration 1858/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0353]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6642]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4468]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1893]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9165]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2997]])\n",
      "Loss: 1.3940414190292358\n",
      "\n",
      "> Iteration 1859/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0353]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6642]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4468]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1893]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9165]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2997]])\n",
      "Loss: 1.4312077760696411\n",
      "\n",
      "> Iteration 1860/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0354]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6642]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4467]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1893]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9165]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2997]])\n",
      "Loss: 1.4160887002944946\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1861/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0354]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6641]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4467]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1892]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9165]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2996]])\n",
      "Loss: 1.3686041831970215\n",
      "\n",
      "> Iteration 1862/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0354]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6641]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4467]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1892]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9165]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2996]])\n",
      "Loss: 1.3947080373764038\n",
      "\n",
      "> Iteration 1863/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0354]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6641]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4466]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1892]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9166]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2996]])\n",
      "Loss: 1.4249980449676514\n",
      "\n",
      "> Iteration 1864/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0355]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6640]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4466]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1892]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9166]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2995]])\n",
      "Loss: 1.3760079145431519\n",
      "\n",
      "> Iteration 1865/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0355]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6640]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4466]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1892]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9166]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2995]])\n",
      "Loss: 1.3958885669708252\n",
      "\n",
      "> Iteration 1866/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0355]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6640]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4465]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1892]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9166]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2995]])\n",
      "Loss: 1.4009673595428467\n",
      "\n",
      "> Iteration 1867/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0355]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6640]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4465]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1892]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9166]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2994]])\n",
      "Loss: 1.3647385835647583\n",
      "\n",
      "> Iteration 1868/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0355]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6639]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4465]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1891]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9166]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2994]])\n",
      "Loss: 1.3554139137268066\n",
      "\n",
      "> Iteration 1869/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0356]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6639]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4465]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1891]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9166]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2994]])\n",
      "Loss: 1.3651851415634155\n",
      "\n",
      "> Iteration 1870/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0356]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6639]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4464]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1891]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9166]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2994]])\n",
      "Loss: 1.3776739835739136\n",
      "\n",
      "> Iteration 1871/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0356]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6639]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4464]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1891]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2993]])\n",
      "Loss: 1.380733847618103\n",
      "\n",
      "> Iteration 1872/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0357]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6639]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4464]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1891]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2993]])\n",
      "Loss: 1.3517343997955322\n",
      "\n",
      "> Iteration 1873/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0357]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6638]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4463]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1891]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2993]])\n",
      "Loss: 1.3455510139465332\n",
      "\n",
      "> Iteration 1874/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0357]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6638]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4463]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1891]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2992]])\n",
      "Loss: 1.402143120765686\n",
      "\n",
      "> Iteration 1875/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0358]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6638]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4463]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1891]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2992]])\n",
      "Loss: 1.351265788078308\n",
      "\n",
      "> Iteration 1876/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0358]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6638]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4463]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2992]])\n",
      "Loss: 1.4127336740493774\n",
      "\n",
      "> Iteration 1877/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0359]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6637]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4463]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2992]])\n",
      "Loss: 1.3345228433609009\n",
      "\n",
      "> Iteration 1878/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0359]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6637]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4462]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2992]])\n",
      "Loss: 1.3654844760894775\n",
      "\n",
      "> Iteration 1879/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0360]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6637]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4462]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2992]])\n",
      "Loss: 1.3430639505386353\n",
      "\n",
      "> Iteration 1880/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0360]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6637]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4462]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2991]])\n",
      "Loss: 1.3817490339279175\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1881/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0360]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6637]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4462]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9167]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2991]])\n",
      "Loss: 1.3738707304000854\n",
      "\n",
      "> Iteration 1882/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0360]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6637]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4462]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9168]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2991]])\n",
      "Loss: 1.3692500591278076\n",
      "\n",
      "> Iteration 1883/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0361]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6636]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4461]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9168]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2991]])\n",
      "Loss: 1.4165936708450317\n",
      "\n",
      "> Iteration 1884/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0361]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6636]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4461]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9168]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2990]])\n",
      "Loss: 1.3793033361434937\n",
      "\n",
      "> Iteration 1885/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0361]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6636]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4461]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1890]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9168]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2990]])\n",
      "Loss: 1.3917114734649658\n",
      "\n",
      "> Iteration 1886/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0362]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6636]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4460]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1889]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9168]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2990]])\n",
      "Loss: 1.3894360065460205\n",
      "\n",
      "> Iteration 1887/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0362]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6635]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4460]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1889]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9168]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2990]])\n",
      "Loss: 1.4080485105514526\n",
      "\n",
      "> Iteration 1888/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0362]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6635]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4460]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1889]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9168]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2989]])\n",
      "Loss: 1.4113601446151733\n",
      "\n",
      "> Iteration 1889/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0362]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6635]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4459]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1889]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9168]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2989]])\n",
      "Loss: 1.4198601245880127\n",
      "\n",
      "> Iteration 1890/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0362]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6635]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4459]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1889]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9169]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2989]])\n",
      "Loss: 1.3656939268112183\n",
      "\n",
      "> Iteration 1891/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0363]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6634]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4459]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1889]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9169]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2988]])\n",
      "Loss: 1.4037740230560303\n",
      "\n",
      "> Iteration 1892/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0363]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6634]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4459]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1889]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9169]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2988]])\n",
      "Loss: 1.3995883464813232\n",
      "\n",
      "> Iteration 1893/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0364]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6634]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4458]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1888]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9169]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2988]])\n",
      "Loss: 1.394047498703003\n",
      "\n",
      "> Iteration 1894/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0364]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6634]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4458]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1888]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9169]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2988]])\n",
      "Loss: 1.3647041320800781\n",
      "\n",
      "> Iteration 1895/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0364]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6633]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4458]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1888]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9169]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2987]])\n",
      "Loss: 1.4153567552566528\n",
      "\n",
      "> Iteration 1896/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0365]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6633]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4458]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1888]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9169]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2987]])\n",
      "Loss: 1.393118977546692\n",
      "\n",
      "> Iteration 1897/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0365]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6633]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4457]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1888]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9169]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2987]])\n",
      "Loss: 1.3612717390060425\n",
      "\n",
      "> Iteration 1898/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0366]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6633]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4457]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1888]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9170]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2987]])\n",
      "Loss: 1.3717360496520996\n",
      "\n",
      "> Iteration 1899/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0366]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6632]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4457]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1888]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9170]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2986]])\n",
      "Loss: 1.3898807764053345\n",
      "\n",
      "> Iteration 1900/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0367]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6632]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4456]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1887]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9170]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2986]])\n",
      "Loss: 1.367379069328308\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1901/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6632]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4456]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1887]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9170]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2986]])\n",
      "Loss: 1.4192324876785278\n",
      "\n",
      "> Iteration 1902/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6631]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4456]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1887]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9170]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2985]])\n",
      "Loss: 1.3587397336959839\n",
      "\n",
      "> Iteration 1903/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6631]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4455]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1887]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9170]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2985]])\n",
      "Loss: 1.39316725730896\n",
      "\n",
      "> Iteration 1904/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6631]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4455]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1887]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9170]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2985]])\n",
      "Loss: 1.3975673913955688\n",
      "\n",
      "> Iteration 1905/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6631]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4455]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1887]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9171]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2984]])\n",
      "Loss: 1.3625447750091553\n",
      "\n",
      "> Iteration 1906/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6630]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4454]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1887]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9171]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2984]])\n",
      "Loss: 1.3643605709075928\n",
      "\n",
      "> Iteration 1907/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6630]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4454]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1886]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9171]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2984]])\n",
      "Loss: 1.4084781408309937\n",
      "\n",
      "> Iteration 1908/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6630]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4454]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1886]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9171]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2983]])\n",
      "Loss: 1.3939793109893799\n",
      "\n",
      "> Iteration 1909/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6629]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4453]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1886]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9171]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2983]])\n",
      "Loss: 1.4006634950637817\n",
      "\n",
      "> Iteration 1910/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0371]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6629]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4453]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1886]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9171]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2983]])\n",
      "Loss: 1.3628002405166626\n",
      "\n",
      "> Iteration 1911/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0371]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6629]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4453]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1886]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9171]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2982]])\n",
      "Loss: 1.3773335218429565\n",
      "\n",
      "> Iteration 1912/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0371]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6628]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4452]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1886]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9172]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2982]])\n",
      "Loss: 1.3895567655563354\n",
      "\n",
      "> Iteration 1913/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0371]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6628]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4452]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1885]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9172]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2982]])\n",
      "Loss: 1.3402822017669678\n",
      "\n",
      "> Iteration 1914/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6628]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4451]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1885]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9172]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2981]])\n",
      "Loss: 1.3862507343292236\n",
      "\n",
      "> Iteration 1915/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6628]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4451]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1885]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9172]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2981]])\n",
      "Loss: 1.3622504472732544\n",
      "\n",
      "> Iteration 1916/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6627]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4451]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1885]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9172]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2980]])\n",
      "Loss: 1.4215915203094482\n",
      "\n",
      "> Iteration 1917/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6627]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4450]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1885]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9172]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2980]])\n",
      "Loss: 1.388631820678711\n",
      "\n",
      "> Iteration 1918/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0373]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6627]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4450]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1885]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9172]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2980]])\n",
      "Loss: 1.3808245658874512\n",
      "\n",
      "> Iteration 1919/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0373]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6626]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4450]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1884]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2979]])\n",
      "Loss: 1.3843090534210205\n",
      "\n",
      "> Iteration 1920/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0373]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6626]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4450]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1884]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2979]])\n",
      "Loss: 1.3788151741027832\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1921/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0373]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6626]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4449]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1884]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2979]])\n",
      "Loss: 1.3656017780303955\n",
      "\n",
      "> Iteration 1922/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6626]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4449]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1884]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2979]])\n",
      "Loss: 1.356963872909546\n",
      "\n",
      "> Iteration 1923/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6625]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4449]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1884]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2978]])\n",
      "Loss: 1.3692872524261475\n",
      "\n",
      "> Iteration 1924/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6625]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4448]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1884]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2978]])\n",
      "Loss: 1.356537103652954\n",
      "\n",
      "> Iteration 1925/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0375]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6625]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4448]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1884]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2978]])\n",
      "Loss: 1.376096487045288\n",
      "\n",
      "> Iteration 1926/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0375]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6625]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4448]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1884]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2978]])\n",
      "Loss: 1.3867987394332886\n",
      "\n",
      "> Iteration 1927/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6625]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4448]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1883]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9173]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2978]])\n",
      "Loss: 1.3644795417785645\n",
      "\n",
      "> Iteration 1928/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6624]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4448]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1883]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9174]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2977]])\n",
      "Loss: 1.371216893196106\n",
      "\n",
      "> Iteration 1929/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0377]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6624]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4447]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1883]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9174]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2977]])\n",
      "Loss: 1.3719966411590576\n",
      "\n",
      "> Iteration 1930/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0377]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6624]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4447]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1883]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9174]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2977]])\n",
      "Loss: 1.355964183807373\n",
      "\n",
      "> Iteration 1931/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6624]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4447]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1883]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9174]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2977]])\n",
      "Loss: 1.3395583629608154\n",
      "\n",
      "> Iteration 1932/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6624]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4447]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1883]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9174]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2977]])\n",
      "Loss: 1.3565614223480225\n",
      "\n",
      "> Iteration 1933/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6623]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4446]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1883]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9174]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2976]])\n",
      "Loss: 1.3668872117996216\n",
      "\n",
      "> Iteration 1934/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6623]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4446]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1883]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9174]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2976]])\n",
      "Loss: 1.380181074142456\n",
      "\n",
      "> Iteration 1935/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6623]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4446]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1882]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9174]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2976]])\n",
      "Loss: 1.4235209226608276\n",
      "\n",
      "> Iteration 1936/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0380]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6623]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4446]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1882]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9174]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2976]])\n",
      "Loss: 1.3628859519958496\n",
      "\n",
      "> Iteration 1937/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0380]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6623]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4445]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1882]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9175]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2975]])\n",
      "Loss: 1.3802083730697632\n",
      "\n",
      "> Iteration 1938/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6622]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4445]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1882]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9175]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2975]])\n",
      "Loss: 1.3831260204315186\n",
      "\n",
      "> Iteration 1939/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6622]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4445]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1882]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9175]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2975]])\n",
      "Loss: 1.3806289434432983\n",
      "\n",
      "> Iteration 1940/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6622]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4444]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1882]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9175]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2974]])\n",
      "Loss: 1.4351822137832642\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1941/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0382]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6621]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4444]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1882]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9175]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2974]])\n",
      "Loss: 1.3740087747573853\n",
      "\n",
      "> Iteration 1942/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0382]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6621]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4444]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1881]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9175]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2974]])\n",
      "Loss: 1.3998663425445557\n",
      "\n",
      "> Iteration 1943/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0383]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6621]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4443]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1881]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9175]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2973]])\n",
      "Loss: 1.3752014636993408\n",
      "\n",
      "> Iteration 1944/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0383]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6621]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4443]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1881]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9175]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2973]])\n",
      "Loss: 1.3992204666137695\n",
      "\n",
      "> Iteration 1945/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0383]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6620]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4443]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1881]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9176]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2973]])\n",
      "Loss: 1.3678855895996094\n",
      "\n",
      "> Iteration 1946/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0384]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6620]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4443]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1881]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9176]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2973]])\n",
      "Loss: 1.388008952140808\n",
      "\n",
      "> Iteration 1947/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0384]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6620]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4442]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1881]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9176]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2972]])\n",
      "Loss: 1.3849084377288818\n",
      "\n",
      "> Iteration 1948/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0385]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6620]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4442]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1880]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9176]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2972]])\n",
      "Loss: 1.4265676736831665\n",
      "\n",
      "> Iteration 1949/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0385]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6619]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4442]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1880]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9176]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2972]])\n",
      "Loss: 1.3956032991409302\n",
      "\n",
      "> Iteration 1950/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0385]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6619]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4441]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1880]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9176]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2971]])\n",
      "Loss: 1.3724485635757446\n",
      "\n",
      "> Iteration 1951/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0385]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6619]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4441]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1880]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9176]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2971]])\n",
      "Loss: 1.3816499710083008\n",
      "\n",
      "> Iteration 1952/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0386]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6618]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4441]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1880]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9177]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2971]])\n",
      "Loss: 1.3710517883300781\n",
      "\n",
      "> Iteration 1953/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0386]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6618]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4440]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1880]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9177]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2970]])\n",
      "Loss: 1.3693031072616577\n",
      "\n",
      "> Iteration 1954/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0386]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6618]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4440]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1880]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9177]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2970]])\n",
      "Loss: 1.3819226026535034\n",
      "\n",
      "> Iteration 1955/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0386]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6617]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4440]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1879]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9177]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2970]])\n",
      "Loss: 1.3893150091171265\n",
      "\n",
      "> Iteration 1956/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0387]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6617]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4439]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1879]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9177]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2969]])\n",
      "Loss: 1.3625744581222534\n",
      "\n",
      "> Iteration 1957/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0387]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6617]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4439]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1879]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9177]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2969]])\n",
      "Loss: 1.421072006225586\n",
      "\n",
      "> Iteration 1958/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0387]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6617]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4439]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1879]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9177]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2969]])\n",
      "Loss: 1.418397068977356\n",
      "\n",
      "> Iteration 1959/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0388]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6616]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4438]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1879]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9178]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2968]])\n",
      "Loss: 1.4296129941940308\n",
      "\n",
      "> Iteration 1960/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0388]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6616]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4438]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1879]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9178]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2968]])\n",
      "Loss: 1.402806282043457\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1961/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0389]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6616]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4438]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1878]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9178]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2968]])\n",
      "Loss: 1.4185106754302979\n",
      "\n",
      "> Iteration 1962/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0389]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6615]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4437]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1878]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9178]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2967]])\n",
      "Loss: 1.3599518537521362\n",
      "\n",
      "> Iteration 1963/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0389]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6615]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4437]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1878]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9178]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2967]])\n",
      "Loss: 1.3980436325073242\n",
      "\n",
      "> Iteration 1964/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0390]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6615]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4436]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1878]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9178]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2966]])\n",
      "Loss: 1.404848575592041\n",
      "\n",
      "> Iteration 1965/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0390]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6614]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4436]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1878]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9179]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2966]])\n",
      "Loss: 1.4314361810684204\n",
      "\n",
      "> Iteration 1966/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0391]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6614]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4435]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1877]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9179]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2966]])\n",
      "Loss: 1.3892521858215332\n",
      "\n",
      "> Iteration 1967/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0391]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6614]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4435]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1877]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9179]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2965]])\n",
      "Loss: 1.3562805652618408\n",
      "\n",
      "> Iteration 1968/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0391]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6613]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4435]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1877]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9179]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2965]])\n",
      "Loss: 1.3833049535751343\n",
      "\n",
      "> Iteration 1969/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0392]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6613]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4434]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1877]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9179]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2964]])\n",
      "Loss: 1.398190975189209\n",
      "\n",
      "> Iteration 1970/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0392]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6612]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4434]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1876]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9180]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2964]])\n",
      "Loss: 1.4205585718154907\n",
      "\n",
      "> Iteration 1971/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0392]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6612]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4433]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1876]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9180]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2963]])\n",
      "Loss: 1.3462170362472534\n",
      "\n",
      "> Iteration 1972/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0393]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6612]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4433]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1876]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9180]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2963]])\n",
      "Loss: 1.361388921737671\n",
      "\n",
      "> Iteration 1973/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0393]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6611]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4433]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1876]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9180]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2963]])\n",
      "Loss: 1.3755546808242798\n",
      "\n",
      "> Iteration 1974/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0393]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6611]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4432]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1876]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9180]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2962]])\n",
      "Loss: 1.3492416143417358\n",
      "\n",
      "> Iteration 1975/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0393]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6611]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4432]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1876]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9180]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2962]])\n",
      "Loss: 1.3970353603363037\n",
      "\n",
      "> Iteration 1976/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0394]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6611]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4432]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1875]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9180]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2962]])\n",
      "Loss: 1.3534824848175049\n",
      "\n",
      "> Iteration 1977/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0394]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6610]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4431]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1875]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9181]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2962]])\n",
      "Loss: 1.420506238937378\n",
      "\n",
      "> Iteration 1978/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0395]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6610]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4431]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1875]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9181]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2961]])\n",
      "Loss: 1.3809988498687744\n",
      "\n",
      "> Iteration 1979/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0395]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6610]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4431]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1875]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9181]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2961]])\n",
      "Loss: 1.3922698497772217\n",
      "\n",
      "> Iteration 1980/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0395]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6610]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4431]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1875]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9181]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2961]])\n",
      "Loss: 1.4194837808609009\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 1981/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0396]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6609]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4430]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1875]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9181]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2961]])\n",
      "Loss: 1.3551045656204224\n",
      "\n",
      "> Iteration 1982/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0396]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6609]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4430]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1875]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9181]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2960]])\n",
      "Loss: 1.3911563158035278\n",
      "\n",
      "> Iteration 1983/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0396]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6609]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4430]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1874]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9181]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2960]])\n",
      "Loss: 1.3860743045806885\n",
      "\n",
      "> Iteration 1984/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0396]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6609]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4429]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1874]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9181]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2960]])\n",
      "Loss: 1.3350505828857422\n",
      "\n",
      "> Iteration 1985/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0396]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6608]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4429]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1874]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2959]])\n",
      "Loss: 1.3870638608932495\n",
      "\n",
      "> Iteration 1986/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0397]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6608]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4429]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1874]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2959]])\n",
      "Loss: 1.3951491117477417\n",
      "\n",
      "> Iteration 1987/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0397]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6608]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4428]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1874]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2959]])\n",
      "Loss: 1.3685681819915771\n",
      "\n",
      "> Iteration 1988/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0397]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6608]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4428]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1874]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2958]])\n",
      "Loss: 1.3793548345565796\n",
      "\n",
      "> Iteration 1989/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0397]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6607]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4428]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1874]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2958]])\n",
      "Loss: 1.3650076389312744\n",
      "\n",
      "> Iteration 1990/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0397]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6607]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4428]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1873]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2958]])\n",
      "Loss: 1.3486573696136475\n",
      "\n",
      "> Iteration 1991/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0397]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6607]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4427]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1873]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2958]])\n",
      "Loss: 1.4077789783477783\n",
      "\n",
      "> Iteration 1992/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0397]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6607]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4427]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1873]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2957]])\n",
      "Loss: 1.3984929323196411\n",
      "\n",
      "> Iteration 1993/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0398]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6606]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4427]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1873]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9182]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2957]])\n",
      "Loss: 1.3882741928100586\n",
      "\n",
      "> Iteration 1994/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0398]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6606]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4427]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1873]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9183]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2957]])\n",
      "Loss: 1.3949248790740967\n",
      "\n",
      "> Iteration 1995/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0398]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6606]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4426]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1873]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9183]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2957]])\n",
      "Loss: 1.394394040107727\n",
      "\n",
      "> Iteration 1996/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0398]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6606]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4426]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1873]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9183]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2956]])\n",
      "Loss: 1.3983231782913208\n",
      "\n",
      "> Iteration 1997/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0398]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6605]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4426]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1873]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9183]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2956]])\n",
      "Loss: 1.3912419080734253\n",
      "\n",
      "> Iteration 1998/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0399]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6605]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4425]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1872]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9183]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2956]])\n",
      "Loss: 1.3843141794204712\n",
      "\n",
      "> Iteration 1999/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0399]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6605]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4425]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1872]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9183]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2955]])\n",
      "Loss: 1.3936313390731812\n",
      "\n",
      "> Iteration 2000/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0399]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6605]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4425]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1872]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9183]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2955]])\n",
      "Loss: 1.3823940753936768\n",
      "\n",
      "--- Update learning rate: [0.001] ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2001/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0400]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6603]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4423]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1871]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9184]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2953]])\n",
      "Loss: 1.406238079071045\n",
      "\n",
      "> Iteration 2002/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0401]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6602]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4421]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1870]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9185]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2952]])\n",
      "Loss: 1.4049626588821411\n",
      "\n",
      "> Iteration 2003/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0403]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6601]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4420]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1870]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9185]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3320]]), B=tensor([[0.2950]])\n",
      "Loss: 1.3558591604232788\n",
      "\n",
      "> Iteration 2004/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0405]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6599]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4419]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1869]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9186]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3321]]), B=tensor([[0.2949]])\n",
      "Loss: 1.3709498643875122\n",
      "\n",
      "> Iteration 2005/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0407]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6598]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4417]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1868]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9187]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2948]])\n",
      "Loss: 1.3545513153076172\n",
      "\n",
      "> Iteration 2006/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0409]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6597]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4416]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1867]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9187]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3322]]), B=tensor([[0.2946]])\n",
      "Loss: 1.415518879890442\n",
      "\n",
      "> Iteration 2007/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0411]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6596]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4414]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1867]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9188]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3323]]), B=tensor([[0.2945]])\n",
      "Loss: 1.4042459726333618\n",
      "\n",
      "> Iteration 2008/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0414]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6594]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4413]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1866]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9189]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3324]]), B=tensor([[0.2944]])\n",
      "Loss: 1.3660820722579956\n",
      "\n",
      "> Iteration 2009/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0416]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6593]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4412]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1865]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9189]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3325]]), B=tensor([[0.2942]])\n",
      "Loss: 1.3973501920700073\n",
      "\n",
      "> Iteration 2010/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0419]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6592]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4410]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1864]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0129]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9190]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3326]]), B=tensor([[0.2941]])\n",
      "Loss: 1.4077434539794922\n",
      "\n",
      "> Iteration 2011/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0422]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6591]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4409]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1863]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0129]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9191]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3327]]), B=tensor([[0.2940]])\n",
      "Loss: 1.4063997268676758\n",
      "\n",
      "> Iteration 2012/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0424]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6589]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4407]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1863]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0129]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9191]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3328]]), B=tensor([[0.2938]])\n",
      "Loss: 1.3574920892715454\n",
      "\n",
      "> Iteration 2013/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6588]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4406]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1862]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0129]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9192]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3330]]), B=tensor([[0.2937]])\n",
      "Loss: 1.3724596500396729\n",
      "\n",
      "> Iteration 2014/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0430]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6587]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4405]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1861]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0129]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9192]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3331]]), B=tensor([[0.2936]])\n",
      "Loss: 1.382095217704773\n",
      "\n",
      "> Iteration 2015/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0432]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6586]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4403]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1861]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0129]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9193]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3332]]), B=tensor([[0.2935]])\n",
      "Loss: 1.3516881465911865\n",
      "\n",
      "> Iteration 2016/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0434]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6585]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4402]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1860]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0128]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9194]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.2933]])\n",
      "Loss: 1.4565324783325195\n",
      "\n",
      "> Iteration 2017/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0436]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6583]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4400]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1859]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0128]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9194]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.2932]])\n",
      "Loss: 1.3963730335235596\n",
      "\n",
      "> Iteration 2018/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0438]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6582]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4398]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1858]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0127]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9195]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.2930]])\n",
      "Loss: 1.4010127782821655\n",
      "\n",
      "> Iteration 2019/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0440]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6580]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4397]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1857]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0127]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9196]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.2929]])\n",
      "Loss: 1.3679620027542114\n",
      "\n",
      "> Iteration 2020/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0441]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6579]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4395]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1856]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0127]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9197]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3333]]), B=tensor([[0.2927]])\n",
      "Loss: 1.3714110851287842\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2021/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0444]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6578]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4394]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1855]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0126]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9197]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.2926]])\n",
      "Loss: 1.361978530883789\n",
      "\n",
      "> Iteration 2022/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0446]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6576]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4392]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1854]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0126]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9198]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.2924]])\n",
      "Loss: 1.3791072368621826\n",
      "\n",
      "> Iteration 2023/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0448]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6575]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4391]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1854]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0126]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9199]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3334]]), B=tensor([[0.2923]])\n",
      "Loss: 1.3534740209579468\n",
      "\n",
      "> Iteration 2024/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0450]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6573]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4389]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1853]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9199]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.2921]])\n",
      "Loss: 1.4345557689666748\n",
      "\n",
      "> Iteration 2025/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0452]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6572]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4388]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1852]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9200]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3335]]), B=tensor([[0.2920]])\n",
      "Loss: 1.3552889823913574\n",
      "\n",
      "> Iteration 2026/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0455]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6571]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4386]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1851]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9201]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3337]]), B=tensor([[0.2919]])\n",
      "Loss: 1.3801652193069458\n",
      "\n",
      "> Iteration 2027/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0457]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6570]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4385]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1850]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9201]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3338]]), B=tensor([[0.2918]])\n",
      "Loss: 1.3739702701568604\n",
      "\n",
      "> Iteration 2028/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0460]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6569]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4384]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1850]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9202]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3340]]), B=tensor([[0.2916]])\n",
      "Loss: 1.3667480945587158\n",
      "\n",
      "> Iteration 2029/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0463]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6568]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4383]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1849]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9202]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3342]]), B=tensor([[0.2915]])\n",
      "Loss: 1.3644288778305054\n",
      "\n",
      "> Iteration 2030/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0466]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6567]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4382]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1848]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9203]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3344]]), B=tensor([[0.2915]])\n",
      "Loss: 1.3438323736190796\n",
      "\n",
      "> Iteration 2031/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0468]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6566]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4381]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1848]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9203]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3345]]), B=tensor([[0.2914]])\n",
      "Loss: 1.3877910375595093\n",
      "\n",
      "> Iteration 2032/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0470]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6566]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4380]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1847]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9204]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3346]]), B=tensor([[0.2913]])\n",
      "Loss: 1.3463311195373535\n",
      "\n",
      "> Iteration 2033/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0473]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6565]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4379]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1847]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9204]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3348]]), B=tensor([[0.2913]])\n",
      "Loss: 1.4283254146575928\n",
      "\n",
      "> Iteration 2034/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0475]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6564]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4378]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1846]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9205]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3349]]), B=tensor([[0.2912]])\n",
      "Loss: 1.3804389238357544\n",
      "\n",
      "> Iteration 2035/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0476]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6563]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4377]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1845]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9205]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3349]]), B=tensor([[0.2911]])\n",
      "Loss: 1.4318958520889282\n",
      "\n",
      "> Iteration 2036/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0478]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6562]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4376]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1845]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9206]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3349]]), B=tensor([[0.2909]])\n",
      "Loss: 1.4117146730422974\n",
      "\n",
      "> Iteration 2037/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0480]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6561]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4375]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1844]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9206]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3350]]), B=tensor([[0.2908]])\n",
      "Loss: 1.3968701362609863\n",
      "\n",
      "> Iteration 2038/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0482]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6560]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4374]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1843]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9207]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3351]]), B=tensor([[0.2907]])\n",
      "Loss: 1.3990180492401123\n",
      "\n",
      "> Iteration 2039/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0484]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6559]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4372]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1842]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9207]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3352]]), B=tensor([[0.2906]])\n",
      "Loss: 1.3725976943969727\n",
      "\n",
      "> Iteration 2040/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0487]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6558]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4371]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1842]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9208]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2905]])\n",
      "Loss: 1.357434868812561\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2041/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0489]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6557]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4370]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1841]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9209]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3354]]), B=tensor([[0.2904]])\n",
      "Loss: 1.357637643814087\n",
      "\n",
      "> Iteration 2042/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0492]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6556]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4368]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1840]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9209]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3355]]), B=tensor([[0.2903]])\n",
      "Loss: 1.3648316860198975\n",
      "\n",
      "> Iteration 2043/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0494]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6555]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4367]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1839]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9210]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3357]]), B=tensor([[0.2902]])\n",
      "Loss: 1.3665249347686768\n",
      "\n",
      "> Iteration 2044/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0497]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6554]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4366]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1839]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9210]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3359]]), B=tensor([[0.2901]])\n",
      "Loss: 1.3749960660934448\n",
      "\n",
      "> Iteration 2045/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0499]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6553]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4365]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1838]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9211]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3360]]), B=tensor([[0.2900]])\n",
      "Loss: 1.3995940685272217\n",
      "\n",
      "> Iteration 2046/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0500]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6552]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4364]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1837]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9211]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3360]]), B=tensor([[0.2899]])\n",
      "Loss: 1.3624093532562256\n",
      "\n",
      "> Iteration 2047/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0502]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6551]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4363]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1837]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0124]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9212]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3360]]), B=tensor([[0.2898]])\n",
      "Loss: 1.3932631015777588\n",
      "\n",
      "> Iteration 2048/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0502]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6550]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4362]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1836]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0124]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9212]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3359]]), B=tensor([[0.2896]])\n",
      "Loss: 1.3899692296981812\n",
      "\n",
      "> Iteration 2049/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0503]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6548]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4360]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1835]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0124]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9213]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.2895]])\n",
      "Loss: 1.4291281700134277\n",
      "\n",
      "> Iteration 2050/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0504]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6547]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4359]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1834]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0123]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9214]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.2893]])\n",
      "Loss: 1.3877578973770142\n",
      "\n",
      "> Iteration 2051/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0505]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6546]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4357]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1833]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0123]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9215]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3357]]), B=tensor([[0.2892]])\n",
      "Loss: 1.414577841758728\n",
      "\n",
      "> Iteration 2052/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0505]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6545]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4356]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1832]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0123]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9215]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3356]]), B=tensor([[0.2890]])\n",
      "Loss: 1.3529181480407715\n",
      "\n",
      "> Iteration 2053/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0505]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6543]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4354]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1831]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0122]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9216]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3354]]), B=tensor([[0.2889]])\n",
      "Loss: 1.4068684577941895\n",
      "\n",
      "> Iteration 2054/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0506]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6542]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4353]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1830]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0122]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9217]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2887]])\n",
      "Loss: 1.3742356300354004\n",
      "\n",
      "> Iteration 2055/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0507]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6540]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4351]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1829]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0121]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9218]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3352]]), B=tensor([[0.2885]])\n",
      "Loss: 1.3956913948059082\n",
      "\n",
      "> Iteration 2056/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0509]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6539]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4349]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1828]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0121]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9218]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2884]])\n",
      "Loss: 1.3664271831512451\n",
      "\n",
      "> Iteration 2057/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0511]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6538]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4348]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1827]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0121]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9219]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2883]])\n",
      "Loss: 1.3780238628387451\n",
      "\n",
      "> Iteration 2058/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0512]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6537]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4347]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1827]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0121]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9220]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2881]])\n",
      "Loss: 1.3705108165740967\n",
      "\n",
      "> Iteration 2059/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0513]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6536]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4345]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1826]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0121]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9220]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2880]])\n",
      "Loss: 1.380481481552124\n",
      "\n",
      "> Iteration 2060/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0515]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6534]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4344]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1825]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9221]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3354]]), B=tensor([[0.2879]])\n",
      "Loss: 1.4025232791900635\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2061/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0517]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6533]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4343]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1824]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9221]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3355]]), B=tensor([[0.2878]])\n",
      "Loss: 1.4008703231811523\n",
      "\n",
      "> Iteration 2062/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0519]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6532]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4341]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1823]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9222]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3355]]), B=tensor([[0.2876]])\n",
      "Loss: 1.3658113479614258\n",
      "\n",
      "> Iteration 2063/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0521]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6531]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4340]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1822]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9223]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3356]]), B=tensor([[0.2875]])\n",
      "Loss: 1.3482729196548462\n",
      "\n",
      "> Iteration 2064/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0522]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6530]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4339]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1821]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9223]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3355]]), B=tensor([[0.2874]])\n",
      "Loss: 1.385151982307434\n",
      "\n",
      "> Iteration 2065/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0523]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6528]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4337]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1820]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9224]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3355]]), B=tensor([[0.2872]])\n",
      "Loss: 1.3901487588882446\n",
      "\n",
      "> Iteration 2066/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0524]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6527]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4336]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1819]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0119]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9225]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3354]]), B=tensor([[0.2871]])\n",
      "Loss: 1.386113166809082\n",
      "\n",
      "> Iteration 2067/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0525]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6526]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4334]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1818]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0119]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9226]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3354]]), B=tensor([[0.2869]])\n",
      "Loss: 1.3799594640731812\n",
      "\n",
      "> Iteration 2068/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0527]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6524]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4333]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1817]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0119]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9226]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2868]])\n",
      "Loss: 1.3617101907730103\n",
      "\n",
      "> Iteration 2069/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0528]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6523]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4331]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1817]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0119]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9227]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2866]])\n",
      "Loss: 1.405063271522522\n",
      "\n",
      "> Iteration 2070/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0529]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6522]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4330]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1816]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9228]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2865]])\n",
      "Loss: 1.3760528564453125\n",
      "\n",
      "> Iteration 2071/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0530]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6521]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4328]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1815]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9228]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3352]]), B=tensor([[0.2863]])\n",
      "Loss: 1.3896151781082153\n",
      "\n",
      "> Iteration 2072/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0532]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6520]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4327]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1814]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9229]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3353]]), B=tensor([[0.2862]])\n",
      "Loss: 1.3919066190719604\n",
      "\n",
      "> Iteration 2073/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0535]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6518]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4326]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1813]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9230]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3355]]), B=tensor([[0.2861]])\n",
      "Loss: 1.3542981147766113\n",
      "\n",
      "> Iteration 2074/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0537]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6518]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4325]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1812]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9230]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3356]]), B=tensor([[0.2860]])\n",
      "Loss: 1.368857502937317\n",
      "\n",
      "> Iteration 2075/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0539]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6517]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4324]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1811]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9231]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3357]]), B=tensor([[0.2859]])\n",
      "Loss: 1.4034852981567383\n",
      "\n",
      "> Iteration 2076/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0541]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6515]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4322]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1811]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9231]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3357]]), B=tensor([[0.2858]])\n",
      "Loss: 1.3950026035308838\n",
      "\n",
      "> Iteration 2077/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0543]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6514]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4321]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1810]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9232]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.2857]])\n",
      "Loss: 1.3897393941879272\n",
      "\n",
      "> Iteration 2078/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0544]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6513]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4320]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1809]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9233]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.2855]])\n",
      "Loss: 1.3577563762664795\n",
      "\n",
      "> Iteration 2079/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0546]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6512]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4319]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1808]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9233]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.2854]])\n",
      "Loss: 1.3576868772506714\n",
      "\n",
      "> Iteration 2080/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0547]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6511]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4317]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1807]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9234]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.2853]])\n",
      "Loss: 1.3761155605316162\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2081/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0549]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6510]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4316]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1806]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9235]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.2852]])\n",
      "Loss: 1.387887716293335\n",
      "\n",
      "> Iteration 2082/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0550]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6509]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4315]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1805]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9235]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3358]]), B=tensor([[0.2851]])\n",
      "Loss: 1.379573106765747\n",
      "\n",
      "> Iteration 2083/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0551]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6508]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4314]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1805]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9236]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3359]]), B=tensor([[0.2850]])\n",
      "Loss: 1.3722225427627563\n",
      "\n",
      "> Iteration 2084/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0554]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6507]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4313]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1804]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9236]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3360]]), B=tensor([[0.2849]])\n",
      "Loss: 1.3959006071090698\n",
      "\n",
      "> Iteration 2085/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0556]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6507]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4312]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1803]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9236]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3361]]), B=tensor([[0.2848]])\n",
      "Loss: 1.3704930543899536\n",
      "\n",
      "> Iteration 2086/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0558]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6506]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4311]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1803]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9237]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3363]]), B=tensor([[0.2848]])\n",
      "Loss: 1.373007893562317\n",
      "\n",
      "> Iteration 2087/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0559]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6505]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4311]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1802]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9237]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3364]]), B=tensor([[0.2847]])\n",
      "Loss: 1.4030123949050903\n",
      "\n",
      "> Iteration 2088/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0562]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6505]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4310]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1802]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9238]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3365]]), B=tensor([[0.2846]])\n",
      "Loss: 1.3623594045639038\n",
      "\n",
      "> Iteration 2089/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0563]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6504]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4309]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1801]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9238]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3366]]), B=tensor([[0.2846]])\n",
      "Loss: 1.391917109489441\n",
      "\n",
      "> Iteration 2090/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0565]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6503]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4308]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1801]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9238]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3366]]), B=tensor([[0.2845]])\n",
      "Loss: 1.3992109298706055\n",
      "\n",
      "> Iteration 2091/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0566]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6502]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4307]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1800]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9239]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3367]]), B=tensor([[0.2844]])\n",
      "Loss: 1.3911975622177124\n",
      "\n",
      "> Iteration 2092/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0567]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6502]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4307]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1799]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9239]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3367]]), B=tensor([[0.2843]])\n",
      "Loss: 1.3984497785568237\n",
      "\n",
      "> Iteration 2093/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0569]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6501]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4306]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1799]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0118]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9240]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3368]]), B=tensor([[0.2842]])\n",
      "Loss: 1.349860668182373\n",
      "\n",
      "> Iteration 2094/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0571]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6500]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4305]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1798]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0119]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9240]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3369]]), B=tensor([[0.2842]])\n",
      "Loss: 1.381941318511963\n",
      "\n",
      "> Iteration 2095/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0573]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6500]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4305]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1798]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0119]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9240]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3371]]), B=tensor([[0.2842]])\n",
      "Loss: 1.372438669204712\n",
      "\n",
      "> Iteration 2096/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0576]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6500]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4304]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1797]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0119]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9241]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3374]]), B=tensor([[0.2841]])\n",
      "Loss: 1.4136883020401\n",
      "\n",
      "> Iteration 2097/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0579]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6499]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4304]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1797]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9241]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3376]]), B=tensor([[0.2841]])\n",
      "Loss: 1.384259819984436\n",
      "\n",
      "> Iteration 2098/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0581]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6499]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4303]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1796]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9241]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3377]]), B=tensor([[0.2840]])\n",
      "Loss: 1.3793518543243408\n",
      "\n",
      "> Iteration 2099/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0582]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6498]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4302]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1796]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9242]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3378]]), B=tensor([[0.2840]])\n",
      "Loss: 1.395020842552185\n",
      "\n",
      "> Iteration 2100/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0584]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6497]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4302]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1795]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9242]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3378]]), B=tensor([[0.2839]])\n",
      "Loss: 1.3274403810501099\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2101/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0584]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6497]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4301]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1795]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9243]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3378]]), B=tensor([[0.2838]])\n",
      "Loss: 1.3834295272827148\n",
      "\n",
      "> Iteration 2102/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0585]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6496]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4300]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1794]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9243]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3378]]), B=tensor([[0.2837]])\n",
      "Loss: 1.3653708696365356\n",
      "\n",
      "> Iteration 2103/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0586]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6495]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4299]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1794]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9243]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3378]]), B=tensor([[0.2837]])\n",
      "Loss: 1.364074945449829\n",
      "\n",
      "> Iteration 2104/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0587]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6495]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4298]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1793]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9244]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3379]]), B=tensor([[0.2836]])\n",
      "Loss: 1.4058815240859985\n",
      "\n",
      "> Iteration 2105/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0589]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6494]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4298]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1792]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9244]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3380]]), B=tensor([[0.2836]])\n",
      "Loss: 1.3811674118041992\n",
      "\n",
      "> Iteration 2106/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0590]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6493]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4297]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1792]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9244]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3380]]), B=tensor([[0.2835]])\n",
      "Loss: 1.3756905794143677\n",
      "\n",
      "> Iteration 2107/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0591]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6493]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4296]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1791]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9245]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3381]]), B=tensor([[0.2834]])\n",
      "Loss: 1.402252197265625\n",
      "\n",
      "> Iteration 2108/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0592]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6492]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4296]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1791]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9245]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3381]]), B=tensor([[0.2834]])\n",
      "Loss: 1.337353229522705\n",
      "\n",
      "> Iteration 2109/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0592]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6492]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4295]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1791]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9245]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3381]]), B=tensor([[0.2833]])\n",
      "Loss: 1.397496223449707\n",
      "\n",
      "> Iteration 2110/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0593]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6491]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4295]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1790]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9246]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3381]]), B=tensor([[0.2833]])\n",
      "Loss: 1.3551688194274902\n",
      "\n",
      "> Iteration 2111/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0595]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6491]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4294]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1790]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9246]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3382]]), B=tensor([[0.2832]])\n",
      "Loss: 1.383389949798584\n",
      "\n",
      "> Iteration 2112/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0596]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6490]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4294]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1789]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9246]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3383]]), B=tensor([[0.2832]])\n",
      "Loss: 1.3306915760040283\n",
      "\n",
      "> Iteration 2113/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0598]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6490]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4293]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1789]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9247]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3384]]), B=tensor([[0.2831]])\n",
      "Loss: 1.3558473587036133\n",
      "\n",
      "> Iteration 2114/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0599]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6490]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4293]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1788]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9247]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.2831]])\n",
      "Loss: 1.3335368633270264\n",
      "\n",
      "> Iteration 2115/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0600]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6489]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4292]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1788]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9247]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.2830]])\n",
      "Loss: 1.3834648132324219\n",
      "\n",
      "> Iteration 2116/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0602]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6488]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4291]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1787]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9248]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3386]]), B=tensor([[0.2829]])\n",
      "Loss: 1.3874537944793701\n",
      "\n",
      "> Iteration 2117/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0603]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6487]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4290]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1787]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9248]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.2829]])\n",
      "Loss: 1.366383671760559\n",
      "\n",
      "> Iteration 2118/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0604]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6487]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4289]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1786]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9249]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.2828]])\n",
      "Loss: 1.3835554122924805\n",
      "\n",
      "> Iteration 2119/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0605]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6486]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4288]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1785]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9249]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.2827]])\n",
      "Loss: 1.3943675756454468\n",
      "\n",
      "> Iteration 2120/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0606]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6485]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4287]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1784]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9250]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3386]]), B=tensor([[0.2826]])\n",
      "Loss: 1.402042031288147\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2121/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0606]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6484]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4286]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1784]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9250]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3386]]), B=tensor([[0.2825]])\n",
      "Loss: 1.3545228242874146\n",
      "\n",
      "> Iteration 2122/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0607]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6483]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4285]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1783]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9251]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.2824]])\n",
      "Loss: 1.3839889764785767\n",
      "\n",
      "> Iteration 2123/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0608]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6482]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4284]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1782]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9251]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.2823]])\n",
      "Loss: 1.3777203559875488\n",
      "\n",
      "> Iteration 2124/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0609]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6481]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4283]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1781]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9252]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.2821]])\n",
      "Loss: 1.3964554071426392\n",
      "\n",
      "> Iteration 2125/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0610]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6480]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4282]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1781]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9252]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.2820]])\n",
      "Loss: 1.3543641567230225\n",
      "\n",
      "> Iteration 2126/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0611]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6479]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4281]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1780]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9253]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3385]]), B=tensor([[0.2820]])\n",
      "Loss: 1.3991044759750366\n",
      "\n",
      "> Iteration 2127/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0613]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6479]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4280]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1779]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9253]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3386]]), B=tensor([[0.2819]])\n",
      "Loss: 1.3868461847305298\n",
      "\n",
      "> Iteration 2128/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0615]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6477]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4279]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1778]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9254]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3386]]), B=tensor([[0.2817]])\n",
      "Loss: 1.3888031244277954\n",
      "\n",
      "> Iteration 2129/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0616]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6477]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4278]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1777]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9254]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3387]]), B=tensor([[0.2817]])\n",
      "Loss: 1.3712615966796875\n",
      "\n",
      "> Iteration 2130/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0618]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6476]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4277]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1777]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0120]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9255]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3388]]), B=tensor([[0.2816]])\n",
      "Loss: 1.4088302850723267\n",
      "\n",
      "> Iteration 2131/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0621]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6475]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4276]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1776]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0121]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9255]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3390]]), B=tensor([[0.2815]])\n",
      "Loss: 1.3678944110870361\n",
      "\n",
      "> Iteration 2132/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0623]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6475]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4276]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1775]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0121]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9256]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3391]]), B=tensor([[0.2815]])\n",
      "Loss: 1.3408429622650146\n",
      "\n",
      "> Iteration 2133/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0625]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6474]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4275]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1775]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0121]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9256]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3393]]), B=tensor([[0.2814]])\n",
      "Loss: 1.3843523263931274\n",
      "\n",
      "> Iteration 2134/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0627]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6473]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4274]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1774]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0121]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9257]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3394]]), B=tensor([[0.2813]])\n",
      "Loss: 1.381434679031372\n",
      "\n",
      "> Iteration 2135/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0630]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6473]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4273]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1773]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0122]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9257]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3396]]), B=tensor([[0.2813]])\n",
      "Loss: 1.330409288406372\n",
      "\n",
      "> Iteration 2136/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0634]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6472]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4273]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1773]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0122]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9257]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3399]]), B=tensor([[0.2813]])\n",
      "Loss: 1.3920505046844482\n",
      "\n",
      "> Iteration 2137/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0637]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6472]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4272]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1772]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0123]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9258]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3402]]), B=tensor([[0.2812]])\n",
      "Loss: 1.3762773275375366\n",
      "\n",
      "> Iteration 2138/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0641]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6472]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4272]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1772]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0123]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9258]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3405]]), B=tensor([[0.2812]])\n",
      "Loss: 1.3618919849395752\n",
      "\n",
      "> Iteration 2139/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0644]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6471]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4272]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1771]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0124]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9258]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3409]]), B=tensor([[0.2812]])\n",
      "Loss: 1.4077023267745972\n",
      "\n",
      "> Iteration 2140/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0648]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6471]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4271]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1771]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0124]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9258]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3412]]), B=tensor([[0.2812]])\n",
      "Loss: 1.3764395713806152\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2141/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0651]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6471]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4271]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1770]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0125]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9259]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3415]]), B=tensor([[0.2812]])\n",
      "Loss: 1.3726651668548584\n",
      "\n",
      "> Iteration 2142/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0655]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6471]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4271]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1770]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0126]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9259]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3418]]), B=tensor([[0.2812]])\n",
      "Loss: 1.3732385635375977\n",
      "\n",
      "> Iteration 2143/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0658]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6470]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4270]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1769]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0126]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9259]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3421]]), B=tensor([[0.2812]])\n",
      "Loss: 1.36583411693573\n",
      "\n",
      "> Iteration 2144/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0661]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6470]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4270]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1769]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0127]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9260]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3424]]), B=tensor([[0.2811]])\n",
      "Loss: 1.3724623918533325\n",
      "\n",
      "> Iteration 2145/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0665]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6469]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4269]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1768]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0127]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9260]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.2811]])\n",
      "Loss: 1.390602469444275\n",
      "\n",
      "> Iteration 2146/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0668]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6469]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4269]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1768]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0128]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9260]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3430]]), B=tensor([[0.2811]])\n",
      "Loss: 1.3940503597259521\n",
      "\n",
      "> Iteration 2147/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0672]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6468]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4268]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1767]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0128]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9261]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.2810]])\n",
      "Loss: 1.3474838733673096\n",
      "\n",
      "> Iteration 2148/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0676]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6468]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4268]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1766]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0129]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9261]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2810]])\n",
      "Loss: 1.355098009109497\n",
      "\n",
      "> Iteration 2149/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0679]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6467]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4267]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1766]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0129]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9262]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3438]]), B=tensor([[0.2810]])\n",
      "Loss: 1.4192063808441162\n",
      "\n",
      "> Iteration 2150/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0682]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6466]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4266]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1765]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9262]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.2809]])\n",
      "Loss: 1.414541482925415\n",
      "\n",
      "> Iteration 2151/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0684]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6466]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4265]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1764]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9263]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3442]]), B=tensor([[0.2808]])\n",
      "Loss: 1.3970844745635986\n",
      "\n",
      "> Iteration 2152/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0686]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6465]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4264]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1763]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9263]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3443]]), B=tensor([[0.2807]])\n",
      "Loss: 1.3721634149551392\n",
      "\n",
      "> Iteration 2153/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0688]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6464]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4263]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1762]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9264]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3444]]), B=tensor([[0.2806]])\n",
      "Loss: 1.417144775390625\n",
      "\n",
      "> Iteration 2154/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0691]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6463]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4262]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1761]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9265]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3445]]), B=tensor([[0.2805]])\n",
      "Loss: 1.361784815788269\n",
      "\n",
      "> Iteration 2155/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0692]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6462]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4261]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1760]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9265]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.2804]])\n",
      "Loss: 1.385511875152588\n",
      "\n",
      "> Iteration 2156/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0695]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6461]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4260]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1759]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9266]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3447]]), B=tensor([[0.2803]])\n",
      "Loss: 1.405669093132019\n",
      "\n",
      "> Iteration 2157/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0697]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6460]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4259]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1758]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9266]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3449]]), B=tensor([[0.2802]])\n",
      "Loss: 1.3502366542816162\n",
      "\n",
      "> Iteration 2158/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0700]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6459]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4258]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1757]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9267]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3450]]), B=tensor([[0.2801]])\n",
      "Loss: 1.3966271877288818\n",
      "\n",
      "> Iteration 2159/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0701]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6458]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4257]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1756]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9267]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3451]]), B=tensor([[0.2801]])\n",
      "Loss: 1.4017139673233032\n",
      "\n",
      "> Iteration 2160/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0703]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6457]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4256]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1755]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9268]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3451]]), B=tensor([[0.2800]])\n",
      "Loss: 1.3790112733840942\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2161/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0704]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6456]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4255]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1754]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9269]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3451]]), B=tensor([[0.2798]])\n",
      "Loss: 1.4317923784255981\n",
      "\n",
      "> Iteration 2162/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0704]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6455]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4253]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1753]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9270]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3450]]), B=tensor([[0.2797]])\n",
      "Loss: 1.3588980436325073\n",
      "\n",
      "> Iteration 2163/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0705]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6454]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4252]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1752]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9270]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3449]]), B=tensor([[0.2795]])\n",
      "Loss: 1.376578688621521\n",
      "\n",
      "> Iteration 2164/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0705]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6453]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4250]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1751]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9271]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3448]]), B=tensor([[0.2794]])\n",
      "Loss: 1.4180892705917358\n",
      "\n",
      "> Iteration 2165/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0706]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6452]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4249]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1750]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9272]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3447]]), B=tensor([[0.2793]])\n",
      "Loss: 1.3412463665008545\n",
      "\n",
      "> Iteration 2166/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0706]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6451]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4248]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1749]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9272]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.2792]])\n",
      "Loss: 1.3713635206222534\n",
      "\n",
      "> Iteration 2167/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0706]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6450]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4247]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1748]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9273]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3445]]), B=tensor([[0.2791]])\n",
      "Loss: 1.382926106452942\n",
      "\n",
      "> Iteration 2168/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0706]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6449]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4246]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1747]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9273]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3444]]), B=tensor([[0.2789]])\n",
      "Loss: 1.353800654411316\n",
      "\n",
      "> Iteration 2169/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0707]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6448]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4245]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1746]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9274]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3444]]), B=tensor([[0.2788]])\n",
      "Loss: 1.4262826442718506\n",
      "\n",
      "> Iteration 2170/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0707]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6447]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4244]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1745]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9275]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3443]]), B=tensor([[0.2787]])\n",
      "Loss: 1.4049267768859863\n",
      "\n",
      "> Iteration 2171/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0708]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6446]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4243]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1744]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9275]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3442]]), B=tensor([[0.2786]])\n",
      "Loss: 1.3819209337234497\n",
      "\n",
      "> Iteration 2172/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0708]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6445]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4241]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1743]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9276]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.2785]])\n",
      "Loss: 1.3542323112487793\n",
      "\n",
      "> Iteration 2173/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0709]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6444]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4240]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1742]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9277]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.2783]])\n",
      "Loss: 1.3760539293289185\n",
      "\n",
      "> Iteration 2174/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0710]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6443]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4239]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1741]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9277]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.2782]])\n",
      "Loss: 1.368905782699585\n",
      "\n",
      "> Iteration 2175/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0711]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6442]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4238]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1740]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9278]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.2781]])\n",
      "Loss: 1.3578965663909912\n",
      "\n",
      "> Iteration 2176/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0712]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6441]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4237]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1740]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9278]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.2780]])\n",
      "Loss: 1.3767937421798706\n",
      "\n",
      "> Iteration 2177/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6440]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4236]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1738]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9279]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3440]]), B=tensor([[0.2779]])\n",
      "Loss: 1.399680733680725\n",
      "\n",
      "> Iteration 2178/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6439]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4235]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1738]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9280]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3440]]), B=tensor([[0.2778]])\n",
      "Loss: 1.3957194089889526\n",
      "\n",
      "> Iteration 2179/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6438]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4234]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1737]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9280]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3438]]), B=tensor([[0.2777]])\n",
      "Loss: 1.3869516849517822\n",
      "\n",
      "> Iteration 2180/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6437]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4233]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1736]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9281]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3437]]), B=tensor([[0.2776]])\n",
      "Loss: 1.3829143047332764\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2181/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6436]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4231]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1735]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9281]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2775]])\n",
      "Loss: 1.4061520099639893\n",
      "\n",
      "> Iteration 2182/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6435]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4230]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1734]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9282]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3435]]), B=tensor([[0.2774]])\n",
      "Loss: 1.3472557067871094\n",
      "\n",
      "> Iteration 2183/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6434]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4229]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1734]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9282]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3434]]), B=tensor([[0.2773]])\n",
      "Loss: 1.3873116970062256\n",
      "\n",
      "> Iteration 2184/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6433]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4228]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1733]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9283]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.2771]])\n",
      "Loss: 1.3711529970169067\n",
      "\n",
      "> Iteration 2185/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6432]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4227]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1732]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9284]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3432]]), B=tensor([[0.2770]])\n",
      "Loss: 1.379456639289856\n",
      "\n",
      "> Iteration 2186/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6431]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4226]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1731]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9284]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3430]]), B=tensor([[0.2768]])\n",
      "Loss: 1.3611751794815063\n",
      "\n",
      "> Iteration 2187/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6430]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4224]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1729]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9285]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3429]]), B=tensor([[0.2767]])\n",
      "Loss: 1.400111436843872\n",
      "\n",
      "> Iteration 2188/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6429]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4223]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1729]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9286]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3428]]), B=tensor([[0.2766]])\n",
      "Loss: 1.367814302444458\n",
      "\n",
      "> Iteration 2189/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6428]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4222]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1728]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9286]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.2765]])\n",
      "Loss: 1.391880750656128\n",
      "\n",
      "> Iteration 2190/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6427]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4221]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1727]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9286]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.2764]])\n",
      "Loss: 1.352753758430481\n",
      "\n",
      "> Iteration 2191/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6427]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4220]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1726]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9287]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.2763]])\n",
      "Loss: 1.386841058731079\n",
      "\n",
      "> Iteration 2192/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6426]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4220]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1726]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9287]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3424]]), B=tensor([[0.2762]])\n",
      "Loss: 1.3817721605300903\n",
      "\n",
      "> Iteration 2193/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6425]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4219]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1725]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9288]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3424]]), B=tensor([[0.2762]])\n",
      "Loss: 1.381988763809204\n",
      "\n",
      "> Iteration 2194/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6425]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4218]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1725]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9288]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3423]]), B=tensor([[0.2761]])\n",
      "Loss: 1.348929762840271\n",
      "\n",
      "> Iteration 2195/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6425]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4218]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1724]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9288]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3423]]), B=tensor([[0.2761]])\n",
      "Loss: 1.3436490297317505\n",
      "\n",
      "> Iteration 2196/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0713]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6424]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4217]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1724]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9289]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3422]]), B=tensor([[0.2760]])\n",
      "Loss: 1.3832656145095825\n",
      "\n",
      "> Iteration 2197/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0714]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6423]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4217]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1723]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9289]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3422]]), B=tensor([[0.2759]])\n",
      "Loss: 1.3860546350479126\n",
      "\n",
      "> Iteration 2198/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0715]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6423]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4216]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1723]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9289]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3422]]), B=tensor([[0.2759]])\n",
      "Loss: 1.4055734872817993\n",
      "\n",
      "> Iteration 2199/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0715]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6422]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4215]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1722]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9290]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3423]]), B=tensor([[0.2758]])\n",
      "Loss: 1.4167433977127075\n",
      "\n",
      "> Iteration 2200/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0716]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6422]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4215]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1722]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9290]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3423]]), B=tensor([[0.2758]])\n",
      "Loss: 1.3913806676864624\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2201/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0717]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6421]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4214]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1721]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0130]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9290]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3423]]), B=tensor([[0.2757]])\n",
      "Loss: 1.3515921831130981\n",
      "\n",
      "> Iteration 2202/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0719]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6421]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4214]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1721]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9290]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3425]]), B=tensor([[0.2757]])\n",
      "Loss: 1.3673427104949951\n",
      "\n",
      "> Iteration 2203/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0721]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6420]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4213]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1720]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9291]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.2756]])\n",
      "Loss: 1.3945115804672241\n",
      "\n",
      "> Iteration 2204/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0722]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6420]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4213]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1720]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9291]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.2756]])\n",
      "Loss: 1.3476693630218506\n",
      "\n",
      "> Iteration 2205/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0723]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6420]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4213]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1720]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9291]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.2756]])\n",
      "Loss: 1.3790427446365356\n",
      "\n",
      "> Iteration 2206/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0723]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6419]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4212]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1719]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9292]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.2755]])\n",
      "Loss: 1.3885750770568848\n",
      "\n",
      "> Iteration 2207/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0723]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6419]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4212]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1719]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9292]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.2755]])\n",
      "Loss: 1.410273790359497\n",
      "\n",
      "> Iteration 2208/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0724]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6419]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4211]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1718]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9292]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.2754]])\n",
      "Loss: 1.3770768642425537\n",
      "\n",
      "> Iteration 2209/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0724]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6418]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4210]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1718]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9292]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.2753]])\n",
      "Loss: 1.3911302089691162\n",
      "\n",
      "> Iteration 2210/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0724]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6417]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4210]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1717]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9293]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.2753]])\n",
      "Loss: 1.381671667098999\n",
      "\n",
      "> Iteration 2211/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0725]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6417]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4209]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1717]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9293]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.2752]])\n",
      "Loss: 1.3423900604248047\n",
      "\n",
      "> Iteration 2212/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0726]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6416]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4209]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1716]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9293]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.2752]])\n",
      "Loss: 1.3836562633514404\n",
      "\n",
      "> Iteration 2213/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0726]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6416]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4208]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1716]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0131]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9294]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3426]]), B=tensor([[0.2751]])\n",
      "Loss: 1.3988908529281616\n",
      "\n",
      "> Iteration 2214/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0727]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6416]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4208]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1715]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9294]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3427]]), B=tensor([[0.2751]])\n",
      "Loss: 1.4259542226791382\n",
      "\n",
      "> Iteration 2215/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0729]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6415]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4207]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1715]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9294]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3428]]), B=tensor([[0.2751]])\n",
      "Loss: 1.416143774986267\n",
      "\n",
      "> Iteration 2216/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0730]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6415]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4207]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1715]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9294]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3429]]), B=tensor([[0.2750]])\n",
      "Loss: 1.3887290954589844\n",
      "\n",
      "> Iteration 2217/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0732]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6415]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4207]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1714]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0132]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9295]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3430]]), B=tensor([[0.2750]])\n",
      "Loss: 1.3940247297286987\n",
      "\n",
      "> Iteration 2218/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0734]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6414]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4206]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1713]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9295]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3431]]), B=tensor([[0.2749]])\n",
      "Loss: 1.378535509109497\n",
      "\n",
      "> Iteration 2219/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0736]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6413]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4205]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1713]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9296]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3432]]), B=tensor([[0.2749]])\n",
      "Loss: 1.4142553806304932\n",
      "\n",
      "> Iteration 2220/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0737]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6412]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4204]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1712]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9296]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.2748]])\n",
      "Loss: 1.4253917932510376\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2221/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0739]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6411]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4203]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1711]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9297]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.2747]])\n",
      "Loss: 1.352340579032898\n",
      "\n",
      "> Iteration 2222/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0740]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6410]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4202]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1710]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0133]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9297]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3434]]), B=tensor([[0.2746]])\n",
      "Loss: 1.3923001289367676\n",
      "\n",
      "> Iteration 2223/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0741]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6410]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4201]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1709]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9298]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3434]]), B=tensor([[0.2745]])\n",
      "Loss: 1.3451226949691772\n",
      "\n",
      "> Iteration 2224/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0742]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6409]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4200]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1708]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9299]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3434]]), B=tensor([[0.2744]])\n",
      "Loss: 1.367469072341919\n",
      "\n",
      "> Iteration 2225/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0743]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6408]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4199]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1707]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9299]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.2743]])\n",
      "Loss: 1.376833200454712\n",
      "\n",
      "> Iteration 2226/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0743]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6407]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4198]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1706]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9300]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.2742]])\n",
      "Loss: 1.3578836917877197\n",
      "\n",
      "> Iteration 2227/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0744]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6406]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4197]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1705]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9300]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.2741]])\n",
      "Loss: 1.4034559726715088\n",
      "\n",
      "> Iteration 2228/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0745]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6405]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4196]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1705]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9301]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3432]]), B=tensor([[0.2740]])\n",
      "Loss: 1.3639779090881348\n",
      "\n",
      "> Iteration 2229/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0745]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6404]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4195]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1704]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9301]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3432]]), B=tensor([[0.2739]])\n",
      "Loss: 1.3903166055679321\n",
      "\n",
      "> Iteration 2230/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0746]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6404]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4194]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1703]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9302]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3432]]), B=tensor([[0.2738]])\n",
      "Loss: 1.4226963520050049\n",
      "\n",
      "> Iteration 2231/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0747]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6403]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4193]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1702]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9302]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.2737]])\n",
      "Loss: 1.3938053846359253\n",
      "\n",
      "> Iteration 2232/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0749]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6402]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4193]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1701]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0134]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9303]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3433]]), B=tensor([[0.2737]])\n",
      "Loss: 1.3823983669281006\n",
      "\n",
      "> Iteration 2233/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0750]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6402]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4192]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1701]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9303]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3434]]), B=tensor([[0.2736]])\n",
      "Loss: 1.399232029914856\n",
      "\n",
      "> Iteration 2234/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0751]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6402]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4192]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1700]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9303]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3435]]), B=tensor([[0.2736]])\n",
      "Loss: 1.3929909467697144\n",
      "\n",
      "> Iteration 2235/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0752]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6401]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4191]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1700]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9303]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3435]]), B=tensor([[0.2735]])\n",
      "Loss: 1.360314130783081\n",
      "\n",
      "> Iteration 2236/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0754]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6401]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4191]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1700]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9304]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2735]])\n",
      "Loss: 1.4098860025405884\n",
      "\n",
      "> Iteration 2237/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0754]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6401]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4191]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1699]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9304]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2735]])\n",
      "Loss: 1.4011837244033813\n",
      "\n",
      "> Iteration 2238/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0754]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6400]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4190]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1699]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9304]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2734]])\n",
      "Loss: 1.376244068145752\n",
      "\n",
      "> Iteration 2239/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0755]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6400]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4189]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1698]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0135]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9305]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2734]])\n",
      "Loss: 1.402891755104065\n",
      "\n",
      "> Iteration 2240/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0756]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6399]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4189]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1697]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9305]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2733]])\n",
      "Loss: 1.3446661233901978\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2241/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0757]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6398]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4188]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1697]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9305]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2732]])\n",
      "Loss: 1.3609063625335693\n",
      "\n",
      "> Iteration 2242/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0757]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6398]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4187]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1696]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9306]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2731]])\n",
      "Loss: 1.3971680402755737\n",
      "\n",
      "> Iteration 2243/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0758]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6397]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4186]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1695]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9306]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2731]])\n",
      "Loss: 1.3887896537780762\n",
      "\n",
      "> Iteration 2244/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0759]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6396]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4186]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1695]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9307]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2730]])\n",
      "Loss: 1.3983927965164185\n",
      "\n",
      "> Iteration 2245/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0760]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6396]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4185]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1694]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9307]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3436]]), B=tensor([[0.2729]])\n",
      "Loss: 1.3739748001098633\n",
      "\n",
      "> Iteration 2246/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0761]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6395]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4184]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1693]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0136]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9307]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3437]]), B=tensor([[0.2729]])\n",
      "Loss: 1.388195276260376\n",
      "\n",
      "> Iteration 2247/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0763]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6395]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4184]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1693]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9308]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3438]]), B=tensor([[0.2728]])\n",
      "Loss: 1.3764081001281738\n",
      "\n",
      "> Iteration 2248/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0765]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6394]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4183]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1692]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9308]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3439]]), B=tensor([[0.2727]])\n",
      "Loss: 1.3938472270965576\n",
      "\n",
      "> Iteration 2249/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0766]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6393]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4182]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1691]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9309]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3440]]), B=tensor([[0.2726]])\n",
      "Loss: 1.3531608581542969\n",
      "\n",
      "> Iteration 2250/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0768]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6392]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4181]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1690]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0137]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9310]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3440]]), B=tensor([[0.2725]])\n",
      "Loss: 1.4026213884353638\n",
      "\n",
      "> Iteration 2251/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0770]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6391]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4180]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1689]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9310]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.2725]])\n",
      "Loss: 1.3734248876571655\n",
      "\n",
      "> Iteration 2252/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0771]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6391]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4179]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1688]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9311]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3442]]), B=tensor([[0.2724]])\n",
      "Loss: 1.4051294326782227\n",
      "\n",
      "> Iteration 2253/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0773]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6390]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4178]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1687]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9311]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3443]]), B=tensor([[0.2723]])\n",
      "Loss: 1.3430649042129517\n",
      "\n",
      "> Iteration 2254/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0774]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6389]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4177]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1686]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0138]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9312]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3443]]), B=tensor([[0.2722]])\n",
      "Loss: 1.3985413312911987\n",
      "\n",
      "> Iteration 2255/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0775]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6389]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4177]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1686]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9312]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3444]]), B=tensor([[0.2722]])\n",
      "Loss: 1.3714802265167236\n",
      "\n",
      "> Iteration 2256/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0777]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6388]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4176]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1685]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9312]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3444]]), B=tensor([[0.2721]])\n",
      "Loss: 1.4131391048431396\n",
      "\n",
      "> Iteration 2257/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0778]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6388]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4176]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1685]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9313]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.2721]])\n",
      "Loss: 1.416693091392517\n",
      "\n",
      "> Iteration 2258/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0780]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6387]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4175]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1684]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9313]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.2720]])\n",
      "Loss: 1.4015275239944458\n",
      "\n",
      "> Iteration 2259/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0781]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6386]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4174]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1683]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9314]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3447]]), B=tensor([[0.2719]])\n",
      "Loss: 1.3832687139511108\n",
      "\n",
      "> Iteration 2260/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0781]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6386]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4173]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1682]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9314]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.2719]])\n",
      "Loss: 1.391235589981079\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2261/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0782]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6385]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4173]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1682]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9314]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.2718]])\n",
      "Loss: 1.3774124383926392\n",
      "\n",
      "> Iteration 2262/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0783]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6385]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4172]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1681]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9315]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3447]]), B=tensor([[0.2717]])\n",
      "Loss: 1.3938467502593994\n",
      "\n",
      "> Iteration 2263/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0783]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6384]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4171]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1681]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9315]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.2717]])\n",
      "Loss: 1.3992794752120972\n",
      "\n",
      "> Iteration 2264/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0782]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6383]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4171]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1680]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9315]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3445]]), B=tensor([[0.2716]])\n",
      "Loss: 1.3438383340835571\n",
      "\n",
      "> Iteration 2265/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0782]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6383]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4170]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1679]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9316]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3443]]), B=tensor([[0.2715]])\n",
      "Loss: 1.3706097602844238\n",
      "\n",
      "> Iteration 2266/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0781]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6382]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4169]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1679]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9316]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3442]]), B=tensor([[0.2714]])\n",
      "Loss: 1.3780839443206787\n",
      "\n",
      "> Iteration 2267/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0781]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6381]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4168]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1678]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0139]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9317]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.2713]])\n",
      "Loss: 1.3586539030075073\n",
      "\n",
      "> Iteration 2268/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0782]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6381]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4168]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1677]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9317]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3441]]), B=tensor([[0.2713]])\n",
      "Loss: 1.392268419265747\n",
      "\n",
      "> Iteration 2269/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0783]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6380]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4167]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1677]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9317]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3442]]), B=tensor([[0.2712]])\n",
      "Loss: 1.3717572689056396\n",
      "\n",
      "> Iteration 2270/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0785]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6380]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4166]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1676]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9318]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3443]]), B=tensor([[0.2712]])\n",
      "Loss: 1.3898850679397583\n",
      "\n",
      "> Iteration 2271/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0787]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4166]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1676]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0140]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9318]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3444]]), B=tensor([[0.2711]])\n",
      "Loss: 1.3804255723953247\n",
      "\n",
      "> Iteration 2272/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0789]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4166]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1675]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9318]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3446]]), B=tensor([[0.2711]])\n",
      "Loss: 1.355607271194458\n",
      "\n",
      "> Iteration 2273/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0791]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4166]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1675]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0141]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9319]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3448]]), B=tensor([[0.2711]])\n",
      "Loss: 1.3334667682647705\n",
      "\n",
      "> Iteration 2274/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0793]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4165]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1674]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9319]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3450]]), B=tensor([[0.2711]])\n",
      "Loss: 1.3798394203186035\n",
      "\n",
      "> Iteration 2275/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0795]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4165]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1674]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0142]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9319]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3452]]), B=tensor([[0.2711]])\n",
      "Loss: 1.3774994611740112\n",
      "\n",
      "> Iteration 2276/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0798]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4165]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1674]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9319]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3455]]), B=tensor([[0.2711]])\n",
      "Loss: 1.343549370765686\n",
      "\n",
      "> Iteration 2277/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0800]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4165]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1674]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0143]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9319]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3457]]), B=tensor([[0.2711]])\n",
      "Loss: 1.354163408279419\n",
      "\n",
      "> Iteration 2278/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0804]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4165]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1674]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9319]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3461]]), B=tensor([[0.2712]])\n",
      "Loss: 1.3711833953857422\n",
      "\n",
      "> Iteration 2279/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0807]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6379]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4165]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1673]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0144]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9319]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3464]]), B=tensor([[0.2712]])\n",
      "Loss: 1.3922171592712402\n",
      "\n",
      "> Iteration 2280/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0811]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6378]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4165]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1673]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9319]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3467]]), B=tensor([[0.2712]])\n",
      "Loss: 1.4042394161224365\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2281/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0813]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6378]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4165]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1672]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0145]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9320]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3470]]), B=tensor([[0.2712]])\n",
      "Loss: 1.3755005598068237\n",
      "\n",
      "> Iteration 2282/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0817]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6378]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4164]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1672]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0146]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9320]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3473]]), B=tensor([[0.2711]])\n",
      "Loss: 1.348105788230896\n",
      "\n",
      "> Iteration 2283/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0820]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6377]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4164]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1671]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9320]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3475]]), B=tensor([[0.2711]])\n",
      "Loss: 1.4074631929397583\n",
      "\n",
      "> Iteration 2284/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0823]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6377]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4164]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1671]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0147]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9321]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3478]]), B=tensor([[0.2711]])\n",
      "Loss: 1.4129823446273804\n",
      "\n",
      "> Iteration 2285/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0825]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6377]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4163]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1670]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9321]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3480]]), B=tensor([[0.2711]])\n",
      "Loss: 1.374725103378296\n",
      "\n",
      "> Iteration 2286/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0828]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6376]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4162]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1669]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0148]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9321]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3482]]), B=tensor([[0.2710]])\n",
      "Loss: 1.384811282157898\n",
      "\n",
      "> Iteration 2287/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0830]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6376]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4162]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1668]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9322]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3483]]), B=tensor([[0.2710]])\n",
      "Loss: 1.377346396446228\n",
      "\n",
      "> Iteration 2288/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0832]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6375]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4161]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1668]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0149]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9322]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3485]]), B=tensor([[0.2709]])\n",
      "Loss: 1.389013409614563\n",
      "\n",
      "> Iteration 2289/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0835]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6375]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4161]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1667]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9323]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3487]]), B=tensor([[0.2709]])\n",
      "Loss: 1.3956559896469116\n",
      "\n",
      "> Iteration 2290/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0836]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6374]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4160]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1666]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9323]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3488]]), B=tensor([[0.2708]])\n",
      "Loss: 1.393946647644043\n",
      "\n",
      "> Iteration 2291/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0838]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6374]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4160]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1666]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9323]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3489]]), B=tensor([[0.2708]])\n",
      "Loss: 1.3803977966308594\n",
      "\n",
      "> Iteration 2292/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0839]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6373]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4159]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1665]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0150]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9324]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3490]]), B=tensor([[0.2708]])\n",
      "Loss: 1.3745328187942505\n",
      "\n",
      "> Iteration 2293/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0841]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6373]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4159]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1665]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9324]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3491]]), B=tensor([[0.2707]])\n",
      "Loss: 1.3787275552749634\n",
      "\n",
      "> Iteration 2294/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0843]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6373]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4159]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1664]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0151]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9324]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3493]]), B=tensor([[0.2707]])\n",
      "Loss: 1.3546019792556763\n",
      "\n",
      "> Iteration 2295/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0845]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6373]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4158]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1664]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9324]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3495]]), B=tensor([[0.2707]])\n",
      "Loss: 1.3749674558639526\n",
      "\n",
      "> Iteration 2296/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0847]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6372]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4158]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1664]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9324]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3497]]), B=tensor([[0.2707]])\n",
      "Loss: 1.3846009969711304\n",
      "\n",
      "> Iteration 2297/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0849]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6372]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4158]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1663]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9325]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3498]]), B=tensor([[0.2707]])\n",
      "Loss: 1.4220656156539917\n",
      "\n",
      "> Iteration 2298/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0850]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6372]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4158]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1663]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0152]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9325]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3499]]), B=tensor([[0.2707]])\n",
      "Loss: 1.376947283744812\n",
      "\n",
      "> Iteration 2299/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0852]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6372]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4157]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1663]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9325]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3501]]), B=tensor([[0.2706]])\n",
      "Loss: 1.3736324310302734\n",
      "\n",
      "> Iteration 2300/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0854]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6371]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4157]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1662]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9325]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3502]]), B=tensor([[0.2706]])\n",
      "Loss: 1.3694732189178467\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2301/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0855]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6371]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4157]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1662]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0153]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9325]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3503]]), B=tensor([[0.2706]])\n",
      "Loss: 1.3639943599700928\n",
      "\n",
      "> Iteration 2302/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0856]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6371]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4156]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1661]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9326]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3504]]), B=tensor([[0.2706]])\n",
      "Loss: 1.3722057342529297\n",
      "\n",
      "> Iteration 2303/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0857]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6370]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4156]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1661]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9326]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3504]]), B=tensor([[0.2705]])\n",
      "Loss: 1.3680704832077026\n",
      "\n",
      "> Iteration 2304/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0858]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6370]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4155]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1660]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9326]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3505]]), B=tensor([[0.2705]])\n",
      "Loss: 1.429918646812439\n",
      "\n",
      "> Iteration 2305/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0858]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6370]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4155]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1660]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9327]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3505]]), B=tensor([[0.2704]])\n",
      "Loss: 1.3701611757278442\n",
      "\n",
      "> Iteration 2306/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0859]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6369]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4155]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1659]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9327]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3505]]), B=tensor([[0.2704]])\n",
      "Loss: 1.4401475191116333\n",
      "\n",
      "> Iteration 2307/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0859]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6369]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4154]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1659]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0154]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9327]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3505]]), B=tensor([[0.2703]])\n",
      "Loss: 1.3658287525177002\n",
      "\n",
      "> Iteration 2308/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0860]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6368]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4154]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1658]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9327]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3505]]), B=tensor([[0.2703]])\n",
      "Loss: 1.40664541721344\n",
      "\n",
      "> Iteration 2309/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0861]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6368]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4153]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1658]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9328]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3506]]), B=tensor([[0.2703]])\n",
      "Loss: 1.4042384624481201\n",
      "\n",
      "> Iteration 2310/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0862]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6368]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4153]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1658]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9328]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3506]]), B=tensor([[0.2702]])\n",
      "Loss: 1.3660768270492554\n",
      "\n",
      "> Iteration 2311/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0863]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6367]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4152]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1657]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9328]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3507]]), B=tensor([[0.2702]])\n",
      "Loss: 1.3745454549789429\n",
      "\n",
      "> Iteration 2312/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0864]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6367]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4152]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1657]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0155]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9328]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3508]]), B=tensor([[0.2702]])\n",
      "Loss: 1.3665786981582642\n",
      "\n",
      "> Iteration 2313/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0865]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6366]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4151]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1655]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9329]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3508]]), B=tensor([[0.2701]])\n",
      "Loss: 1.456109642982483\n",
      "\n",
      "> Iteration 2314/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0866]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6365]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4150]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1654]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9330]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3508]]), B=tensor([[0.2699]])\n",
      "Loss: 1.40327787399292\n",
      "\n",
      "> Iteration 2315/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0868]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6364]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4149]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1653]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0156]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9331]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3508]]), B=tensor([[0.2698]])\n",
      "Loss: 1.3988691568374634\n",
      "\n",
      "> Iteration 2316/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0870]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6363]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4147]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1652]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9331]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3509]]), B=tensor([[0.2697]])\n",
      "Loss: 1.362259030342102\n",
      "\n",
      "> Iteration 2317/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0871]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6362]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4147]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1651]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9332]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3509]]), B=tensor([[0.2696]])\n",
      "Loss: 1.3716648817062378\n",
      "\n",
      "> Iteration 2318/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0873]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6362]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4146]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1650]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0157]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9332]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.2696]])\n",
      "Loss: 1.3932565450668335\n",
      "\n",
      "> Iteration 2319/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0874]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6361]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4145]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1649]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9333]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.2695]])\n",
      "Loss: 1.3914463520050049\n",
      "\n",
      "> Iteration 2320/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0875]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6360]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4144]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1648]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9333]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.2694]])\n",
      "Loss: 1.3803308010101318\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2321/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0876]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6359]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4143]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1647]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9334]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.2693]])\n",
      "Loss: 1.4061611890792847\n",
      "\n",
      "> Iteration 2322/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0877]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6358]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4142]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1646]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0158]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9335]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.2692]])\n",
      "Loss: 1.3568363189697266\n",
      "\n",
      "> Iteration 2323/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0878]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6357]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4141]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1645]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9335]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.2691]])\n",
      "Loss: 1.367112398147583\n",
      "\n",
      "> Iteration 2324/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0879]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6356]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4140]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1643]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9336]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.2689]])\n",
      "Loss: 1.4232895374298096\n",
      "\n",
      "> Iteration 2325/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0880]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6355]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4138]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1642]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9337]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.2688]])\n",
      "Loss: 1.3739556074142456\n",
      "\n",
      "> Iteration 2326/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0880]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6354]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4137]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1641]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9337]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3509]]), B=tensor([[0.2687]])\n",
      "Loss: 1.4063122272491455\n",
      "\n",
      "> Iteration 2327/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0881]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6354]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4137]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1640]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0159]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9338]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3509]]), B=tensor([[0.2687]])\n",
      "Loss: 1.3603789806365967\n",
      "\n",
      "> Iteration 2328/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0881]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6353]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4136]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1640]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9338]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3509]]), B=tensor([[0.2686]])\n",
      "Loss: 1.391764760017395\n",
      "\n",
      "> Iteration 2329/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0883]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6352]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4135]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1639]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9339]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3509]]), B=tensor([[0.2685]])\n",
      "Loss: 1.3813111782073975\n",
      "\n",
      "> Iteration 2330/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0884]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6352]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4134]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1638]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9339]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3510]]), B=tensor([[0.2684]])\n",
      "Loss: 1.3862699270248413\n",
      "\n",
      "> Iteration 2331/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0885]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6351]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4134]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1637]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0160]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9340]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3511]]), B=tensor([[0.2684]])\n",
      "Loss: 1.3681715726852417\n",
      "\n",
      "> Iteration 2332/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0888]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6350]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4133]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1636]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9340]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3512]]), B=tensor([[0.2683]])\n",
      "Loss: 1.3787829875946045\n",
      "\n",
      "> Iteration 2333/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0890]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6350]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4132]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1635]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0161]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9341]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3514]]), B=tensor([[0.2682]])\n",
      "Loss: 1.4018222093582153\n",
      "\n",
      "> Iteration 2334/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0892]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6349]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4131]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1634]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0162]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9341]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3516]]), B=tensor([[0.2682]])\n",
      "Loss: 1.3800780773162842\n",
      "\n",
      "> Iteration 2335/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0895]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6349]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4131]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1634]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9342]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3518]]), B=tensor([[0.2682]])\n",
      "Loss: 1.3922213315963745\n",
      "\n",
      "> Iteration 2336/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0897]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6348]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4131]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1633]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9342]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3519]]), B=tensor([[0.2681]])\n",
      "Loss: 1.3745816946029663\n",
      "\n",
      "> Iteration 2337/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0899]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6348]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4130]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1632]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0163]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9342]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3521]]), B=tensor([[0.2681]])\n",
      "Loss: 1.3885382413864136\n",
      "\n",
      "> Iteration 2338/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0901]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6347]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4129]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1631]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0164]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9343]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3522]]), B=tensor([[0.2680]])\n",
      "Loss: 1.4216532707214355\n",
      "\n",
      "> Iteration 2339/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0904]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6346]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4128]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1630]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0164]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9343]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3524]]), B=tensor([[0.2679]])\n",
      "Loss: 1.3884638547897339\n",
      "\n",
      "> Iteration 2340/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0906]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6345]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4127]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1629]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0165]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9344]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3525]]), B=tensor([[0.2678]])\n",
      "Loss: 1.4060258865356445\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2341/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0907]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6345]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4126]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1628]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0165]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9345]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3525]]), B=tensor([[0.2677]])\n",
      "Loss: 1.3858999013900757\n",
      "\n",
      "> Iteration 2342/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0908]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6344]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4125]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1627]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0165]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9345]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3525]]), B=tensor([[0.2677]])\n",
      "Loss: 1.3848322629928589\n",
      "\n",
      "> Iteration 2343/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0909]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6343]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4125]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1626]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0166]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9346]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3526]]), B=tensor([[0.2676]])\n",
      "Loss: 1.3589009046554565\n",
      "\n",
      "> Iteration 2344/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0910]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6342]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4124]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1625]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0166]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9346]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3525]]), B=tensor([[0.2675]])\n",
      "Loss: 1.4086521863937378\n",
      "\n",
      "> Iteration 2345/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0911]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6342]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4123]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1624]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0166]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9347]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3526]]), B=tensor([[0.2674]])\n",
      "Loss: 1.399017095565796\n",
      "\n",
      "> Iteration 2346/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0913]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6341]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4122]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1624]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0167]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9347]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3527]]), B=tensor([[0.2674]])\n",
      "Loss: 1.37549889087677\n",
      "\n",
      "> Iteration 2347/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0914]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6340]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4121]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1623]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0167]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9348]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3527]]), B=tensor([[0.2673]])\n",
      "Loss: 1.366174340248108\n",
      "\n",
      "> Iteration 2348/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0916]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6339]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4120]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1622]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0167]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9348]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3528]]), B=tensor([[0.2672]])\n",
      "Loss: 1.3755950927734375\n",
      "\n",
      "> Iteration 2349/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0917]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6339]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4120]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1621]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0168]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9349]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3529]]), B=tensor([[0.2671]])\n",
      "Loss: 1.3624707460403442\n",
      "\n",
      "> Iteration 2350/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0919]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6338]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4119]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1620]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0168]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9349]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3530]]), B=tensor([[0.2671]])\n",
      "Loss: 1.4002677202224731\n",
      "\n",
      "> Iteration 2351/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0920]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6338]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4119]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1619]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0168]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9349]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3531]]), B=tensor([[0.2670]])\n",
      "Loss: 1.332123041152954\n",
      "\n",
      "> Iteration 2352/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0922]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6337]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4118]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1619]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9350]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3532]]), B=tensor([[0.2670]])\n",
      "Loss: 1.445839762687683\n",
      "\n",
      "> Iteration 2353/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0923]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6337]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4117]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1618]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9350]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3532]]), B=tensor([[0.2669]])\n",
      "Loss: 1.4161738157272339\n",
      "\n",
      "> Iteration 2354/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0924]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6336]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4116]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1617]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0169]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9351]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3533]]), B=tensor([[0.2668]])\n",
      "Loss: 1.4064055681228638\n",
      "\n",
      "> Iteration 2355/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0926]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6335]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4116]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1616]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9351]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3533]]), B=tensor([[0.2668]])\n",
      "Loss: 1.3723570108413696\n",
      "\n",
      "> Iteration 2356/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0927]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6335]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4115]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1615]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0170]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9352]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3534]]), B=tensor([[0.2667]])\n",
      "Loss: 1.3847815990447998\n",
      "\n",
      "> Iteration 2357/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0929]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6335]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4115]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1615]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0171]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9352]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3536]]), B=tensor([[0.2667]])\n",
      "Loss: 1.402716875076294\n",
      "\n",
      "> Iteration 2358/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0932]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6334]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4114]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1614]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0171]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9352]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3538]]), B=tensor([[0.2666]])\n",
      "Loss: 1.374697208404541\n",
      "\n",
      "> Iteration 2359/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0935]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6334]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4114]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1614]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9353]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3540]]), B=tensor([[0.2666]])\n",
      "Loss: 1.3361014127731323\n",
      "\n",
      "> Iteration 2360/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0937]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6334]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4114]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1613]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9353]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3542]]), B=tensor([[0.2666]])\n",
      "Loss: 1.3900995254516602\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2361/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0939]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6334]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4114]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1613]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0172]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9353]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3544]]), B=tensor([[0.2666]])\n",
      "Loss: 1.3837521076202393\n",
      "\n",
      "> Iteration 2362/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0941]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6333]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4114]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1613]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0173]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9353]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3547]]), B=tensor([[0.2666]])\n",
      "Loss: 1.4113452434539795\n",
      "\n",
      "> Iteration 2363/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0944]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6333]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4113]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1612]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9353]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3549]]), B=tensor([[0.2666]])\n",
      "Loss: 1.3719195127487183\n",
      "\n",
      "> Iteration 2364/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0947]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6333]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4113]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1611]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0174]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9354]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3551]]), B=tensor([[0.2666]])\n",
      "Loss: 1.3677369356155396\n",
      "\n",
      "> Iteration 2365/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0949]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6332]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4112]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1611]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0175]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9354]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3553]]), B=tensor([[0.2665]])\n",
      "Loss: 1.3719817399978638\n",
      "\n",
      "> Iteration 2366/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0952]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6332]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4112]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1610]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0175]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9354]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3555]]), B=tensor([[0.2665]])\n",
      "Loss: 1.3469244241714478\n",
      "\n",
      "> Iteration 2367/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0955]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6332]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4111]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1609]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0176]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9355]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3558]]), B=tensor([[0.2665]])\n",
      "Loss: 1.4174312353134155\n",
      "\n",
      "> Iteration 2368/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0958]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6331]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4111]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1608]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0177]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9355]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3560]]), B=tensor([[0.2664]])\n",
      "Loss: 1.3772374391555786\n",
      "\n",
      "> Iteration 2369/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0961]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6330]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4110]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1607]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0177]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9356]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3563]]), B=tensor([[0.2664]])\n",
      "Loss: 1.3945503234863281\n",
      "\n",
      "> Iteration 2370/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0964]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6330]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4110]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1607]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0178]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9356]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3565]]), B=tensor([[0.2664]])\n",
      "Loss: 1.3858230113983154\n",
      "\n",
      "> Iteration 2371/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0967]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6330]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4109]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1606]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9357]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3568]]), B=tensor([[0.2663]])\n",
      "Loss: 1.3869843482971191\n",
      "\n",
      "> Iteration 2372/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0969]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6330]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4109]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1605]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9357]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3570]]), B=tensor([[0.2663]])\n",
      "Loss: 1.3170760869979858\n",
      "\n",
      "> Iteration 2373/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0971]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6329]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4109]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1605]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9357]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3572]]), B=tensor([[0.2663]])\n",
      "Loss: 1.4170247316360474\n",
      "\n",
      "> Iteration 2374/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0973]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6329]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4109]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1605]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9357]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3573]]), B=tensor([[0.2663]])\n",
      "Loss: 1.3767164945602417\n",
      "\n",
      "> Iteration 2375/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0974]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6329]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4109]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1605]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9357]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3575]]), B=tensor([[0.2663]])\n",
      "Loss: 1.3702154159545898\n",
      "\n",
      "> Iteration 2376/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0976]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6329]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4109]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1605]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9357]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3576]]), B=tensor([[0.2663]])\n",
      "Loss: 1.3816956281661987\n",
      "\n",
      "> Iteration 2377/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0976]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6329]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4109]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1605]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9357]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3577]]), B=tensor([[0.2663]])\n",
      "Loss: 1.4009513854980469\n",
      "\n",
      "> Iteration 2378/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0976]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6329]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4109]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1604]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9357]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3577]]), B=tensor([[0.2663]])\n",
      "Loss: 1.416168451309204\n",
      "\n",
      "> Iteration 2379/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0976]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6329]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4108]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1604]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9358]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3576]]), B=tensor([[0.2663]])\n",
      "Loss: 1.3586801290512085\n",
      "\n",
      "> Iteration 2380/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0976]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6328]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4108]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1604]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9358]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3576]]), B=tensor([[0.2662]])\n",
      "Loss: 1.3582897186279297\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2381/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0976]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6328]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4108]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1603]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9358]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3575]]), B=tensor([[0.2662]])\n",
      "Loss: 1.3945547342300415\n",
      "\n",
      "> Iteration 2382/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0976]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6328]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4107]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1603]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9358]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3574]]), B=tensor([[0.2661]])\n",
      "Loss: 1.3790119886398315\n",
      "\n",
      "> Iteration 2383/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0975]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6327]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4107]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1602]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9358]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3573]]), B=tensor([[0.2661]])\n",
      "Loss: 1.395727515220642\n",
      "\n",
      "> Iteration 2384/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0975]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6327]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4106]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1602]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9359]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3572]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3456441164016724\n",
      "\n",
      "> Iteration 2385/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0974]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6327]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4106]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1602]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9359]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3572]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3958231210708618\n",
      "\n",
      "> Iteration 2386/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0974]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6326]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4105]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1601]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9359]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3571]]), B=tensor([[0.2660]])\n",
      "Loss: 1.391562819480896\n",
      "\n",
      "> Iteration 2387/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0972]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6326]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4105]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1601]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9359]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3569]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3975521326065063\n",
      "\n",
      "> Iteration 2388/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0971]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6325]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4104]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1601]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9360]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3567]]), B=tensor([[0.2658]])\n",
      "Loss: 1.398003339767456\n",
      "\n",
      "> Iteration 2389/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0970]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6325]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4104]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1600]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9360]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3566]]), B=tensor([[0.2658]])\n",
      "Loss: 1.3704787492752075\n",
      "\n",
      "> Iteration 2390/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0968]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6325]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4104]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1600]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9360]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3564]]), B=tensor([[0.2658]])\n",
      "Loss: 1.407112956047058\n",
      "\n",
      "> Iteration 2391/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0967]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6325]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4103]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1600]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9360]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3563]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3482718467712402\n",
      "\n",
      "> Iteration 2392/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0967]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6324]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4103]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1600]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9360]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3562]]), B=tensor([[0.2657]])\n",
      "Loss: 1.4076805114746094\n",
      "\n",
      "> Iteration 2393/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0966]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6324]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4103]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1600]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9360]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3561]]), B=tensor([[0.2657]])\n",
      "Loss: 1.4054911136627197\n",
      "\n",
      "> Iteration 2394/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0966]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6324]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4102]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1599]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9361]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3560]]), B=tensor([[0.2656]])\n",
      "Loss: 1.389737606048584\n",
      "\n",
      "> Iteration 2395/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0966]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6323]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4102]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1598]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9361]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3560]]), B=tensor([[0.2655]])\n",
      "Loss: 1.4284839630126953\n",
      "\n",
      "> Iteration 2396/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0967]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6323]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4101]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1598]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9361]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3560]]), B=tensor([[0.2655]])\n",
      "Loss: 1.3732295036315918\n",
      "\n",
      "> Iteration 2397/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0967]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6322]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4100]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1597]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0179]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9362]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3559]]), B=tensor([[0.2654]])\n",
      "Loss: 1.3754215240478516\n",
      "\n",
      "> Iteration 2398/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0967]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6321]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4099]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1596]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9362]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3558]]), B=tensor([[0.2653]])\n",
      "Loss: 1.3667607307434082\n",
      "\n",
      "> Iteration 2399/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0967]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6320]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4098]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1595]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9363]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3558]]), B=tensor([[0.2652]])\n",
      "Loss: 1.37891685962677\n",
      "\n",
      "> Iteration 2400/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0968]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6319]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4097]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1594]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9364]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3557]]), B=tensor([[0.2651]])\n",
      "Loss: 1.3973236083984375\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2401/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0969]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6318]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4096]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1592]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0180]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9364]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3557]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3562922477722168\n",
      "\n",
      "> Iteration 2402/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0970]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6318]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4095]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1591]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9365]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3558]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3923460245132446\n",
      "\n",
      "> Iteration 2403/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0971]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6317]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4094]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1590]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9365]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3557]]), B=tensor([[0.2648]])\n",
      "Loss: 1.374177098274231\n",
      "\n",
      "> Iteration 2404/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0972]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6316]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4093]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1589]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0181]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9366]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3557]]), B=tensor([[0.2647]])\n",
      "Loss: 1.370054006576538\n",
      "\n",
      "> Iteration 2405/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0973]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6315]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4092]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1588]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9367]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3557]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3839040994644165\n",
      "\n",
      "> Iteration 2406/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0974]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6314]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4091]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1586]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9368]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3557]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3724033832550049\n",
      "\n",
      "> Iteration 2407/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0975]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6313]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4090]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1585]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0182]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9368]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3556]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3958262205123901\n",
      "\n",
      "> Iteration 2408/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0975]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6312]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4088]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1584]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0183]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9369]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3556]]), B=tensor([[0.2642]])\n",
      "Loss: 1.3647793531417847\n",
      "\n",
      "> Iteration 2409/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0976]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6311]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4088]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1583]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0183]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9370]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3556]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3563746213912964\n",
      "\n",
      "> Iteration 2410/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0977]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6310]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4087]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1581]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0183]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9370]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3556]]), B=tensor([[0.2640]])\n",
      "Loss: 1.395614743232727\n",
      "\n",
      "> Iteration 2411/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0978]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6309]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4086]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1581]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9371]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3555]]), B=tensor([[0.2639]])\n",
      "Loss: 1.36234450340271\n",
      "\n",
      "> Iteration 2412/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0978]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6309]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4085]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1580]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9371]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3555]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3847585916519165\n",
      "\n",
      "> Iteration 2413/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0978]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6308]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1579]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9372]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3555]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3875019550323486\n",
      "\n",
      "> Iteration 2414/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0979]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1578]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0184]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9372]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3555]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3890330791473389\n",
      "\n",
      "> Iteration 2415/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0981]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1577]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0185]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9373]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3556]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3337585926055908\n",
      "\n",
      "> Iteration 2416/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0983]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1576]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0185]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9373]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3557]]), B=tensor([[0.2636]])\n",
      "Loss: 1.405663013458252\n",
      "\n",
      "> Iteration 2417/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0986]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1576]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0186]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9373]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3560]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3905421495437622\n",
      "\n",
      "> Iteration 2418/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0989]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1575]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0187]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9374]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3562]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3780735731124878\n",
      "\n",
      "> Iteration 2419/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0992]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1574]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0187]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9374]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3565]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3737435340881348\n",
      "\n",
      "> Iteration 2420/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0994]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1574]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0188]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9374]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3567]]), B=tensor([[0.2635]])\n",
      "Loss: 1.407318115234375\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2421/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0996]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1573]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0188]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9375]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3569]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3578723669052124\n",
      "\n",
      "> Iteration 2422/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.0998]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1572]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0189]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9375]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3570]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3722429275512695\n",
      "\n",
      "> Iteration 2423/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1000]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1572]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0189]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9375]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3572]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3846418857574463\n",
      "\n",
      "> Iteration 2424/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1001]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1571]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0189]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3572]]), B=tensor([[0.2634]])\n",
      "Loss: 1.359507441520691\n",
      "\n",
      "> Iteration 2425/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1002]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1571]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0190]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3573]]), B=tensor([[0.2634]])\n",
      "Loss: 1.357114553451538\n",
      "\n",
      "> Iteration 2426/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1003]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1571]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0190]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3575]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3871957063674927\n",
      "\n",
      "> Iteration 2427/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1005]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1570]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0190]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3576]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3303921222686768\n",
      "\n",
      "> Iteration 2428/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1007]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1570]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0191]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3578]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3718314170837402\n",
      "\n",
      "> Iteration 2429/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1008]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1570]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0191]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3579]]), B=tensor([[0.2634]])\n",
      "Loss: 1.4141950607299805\n",
      "\n",
      "> Iteration 2430/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1010]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1570]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0191]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3580]]), B=tensor([[0.2634]])\n",
      "Loss: 1.382979154586792\n",
      "\n",
      "> Iteration 2431/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1011]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1570]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0192]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3582]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3501007556915283\n",
      "\n",
      "> Iteration 2432/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1013]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1569]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0192]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3584]]), B=tensor([[0.2634]])\n",
      "Loss: 1.419205665588379\n",
      "\n",
      "> Iteration 2433/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1015]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1569]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0193]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3586]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3365700244903564\n",
      "\n",
      "> Iteration 2434/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1018]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1569]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0193]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3589]]), B=tensor([[0.2635]])\n",
      "Loss: 1.4001461267471313\n",
      "\n",
      "> Iteration 2435/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1021]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1569]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0194]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3593]]), B=tensor([[0.2635]])\n",
      "Loss: 1.360265851020813\n",
      "\n",
      "> Iteration 2436/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1024]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1569]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0194]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3596]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3665480613708496\n",
      "\n",
      "> Iteration 2437/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1027]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1569]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0195]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3599]]), B=tensor([[0.2636]])\n",
      "Loss: 1.381117820739746\n",
      "\n",
      "> Iteration 2438/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1030]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1569]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0195]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9376]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3602]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3774417638778687\n",
      "\n",
      "> Iteration 2439/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1033]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1568]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0196]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9377]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3605]]), B=tensor([[0.2636]])\n",
      "Loss: 1.4008071422576904\n",
      "\n",
      "> Iteration 2440/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1035]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1568]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0197]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9377]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3607]]), B=tensor([[0.2636]])\n",
      "Loss: 1.388404369354248\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2441/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1037]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1567]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0197]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9377]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3608]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3910390138626099\n",
      "\n",
      "> Iteration 2442/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1039]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1566]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0198]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9378]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3610]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3918993473052979\n",
      "\n",
      "> Iteration 2443/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1041]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1566]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0198]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9378]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3611]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3798673152923584\n",
      "\n",
      "> Iteration 2444/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1042]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1566]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0198]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9378]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3612]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3967820405960083\n",
      "\n",
      "> Iteration 2445/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1044]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1565]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0199]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9378]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3614]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3685463666915894\n",
      "\n",
      "> Iteration 2446/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1046]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1565]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0199]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9378]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3616]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3903682231903076\n",
      "\n",
      "> Iteration 2447/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1048]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1565]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0199]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9378]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3618]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3569575548171997\n",
      "\n",
      "> Iteration 2448/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1049]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1565]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0200]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3619]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3464055061340332\n",
      "\n",
      "> Iteration 2449/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1051]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1564]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0200]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3621]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3591240644454956\n",
      "\n",
      "> Iteration 2450/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1053]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1564]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3622]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3784005641937256\n",
      "\n",
      "> Iteration 2451/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1054]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1564]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3623]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3941975831985474\n",
      "\n",
      "> Iteration 2452/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1055]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1563]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3624]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3758301734924316\n",
      "\n",
      "> Iteration 2453/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1056]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1563]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0201]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3624]]), B=tensor([[0.2634]])\n",
      "Loss: 1.392576813697815\n",
      "\n",
      "> Iteration 2454/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1057]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1563]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0202]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3626]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3970727920532227\n",
      "\n",
      "> Iteration 2455/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1058]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0202]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3627]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3594129085540771\n",
      "\n",
      "> Iteration 2456/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1060]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0202]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3628]]), B=tensor([[0.2634]])\n",
      "Loss: 1.4425773620605469\n",
      "\n",
      "> Iteration 2457/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1061]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0203]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3630]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3663904666900635\n",
      "\n",
      "> Iteration 2458/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1063]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0203]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3631]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3734849691390991\n",
      "\n",
      "> Iteration 2459/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1064]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0203]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3632]]), B=tensor([[0.2634]])\n",
      "Loss: 1.357194185256958\n",
      "\n",
      "> Iteration 2460/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1065]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0203]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3633]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3388912677764893\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2461/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1065]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3633]]), B=tensor([[0.2634]])\n",
      "Loss: 1.4114786386489868\n",
      "\n",
      "> Iteration 2462/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1066]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3634]]), B=tensor([[0.2634]])\n",
      "Loss: 1.4022349119186401\n",
      "\n",
      "> Iteration 2463/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1067]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3635]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3841381072998047\n",
      "\n",
      "> Iteration 2464/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1068]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3636]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3917146921157837\n",
      "\n",
      "> Iteration 2465/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1069]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3636]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3796446323394775\n",
      "\n",
      "> Iteration 2466/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1069]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0204]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3638]]), B=tensor([[0.2634]])\n",
      "Loss: 1.360266089439392\n",
      "\n",
      "> Iteration 2467/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1072]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0205]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3640]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3403915166854858\n",
      "\n",
      "> Iteration 2468/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1074]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0205]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3642]]), B=tensor([[0.2635]])\n",
      "Loss: 1.4018616676330566\n",
      "\n",
      "> Iteration 2469/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1075]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0205]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3644]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3917832374572754\n",
      "\n",
      "> Iteration 2470/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1077]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3646]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3782325983047485\n",
      "\n",
      "> Iteration 2471/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1078]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3648]]), B=tensor([[0.2637]])\n",
      "Loss: 1.404539942741394\n",
      "\n",
      "> Iteration 2472/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1079]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3649]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3902912139892578\n",
      "\n",
      "> Iteration 2473/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1081]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3651]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3755204677581787\n",
      "\n",
      "> Iteration 2474/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1082]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0206]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3653]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3955990076065063\n",
      "\n",
      "> Iteration 2475/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1084]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0207]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3655]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3550317287445068\n",
      "\n",
      "> Iteration 2476/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1086]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0207]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3656]]), B=tensor([[0.2638]])\n",
      "Loss: 1.372609257698059\n",
      "\n",
      "> Iteration 2477/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1087]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0207]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3658]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3864167928695679\n",
      "\n",
      "> Iteration 2478/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1089]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3660]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3820244073867798\n",
      "\n",
      "> Iteration 2479/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1089]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3661]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3852908611297607\n",
      "\n",
      "> Iteration 2480/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1090]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3661]]), B=tensor([[0.2639]])\n",
      "Loss: 1.402711272239685\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2481/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1090]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3661]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3704100847244263\n",
      "\n",
      "> Iteration 2482/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1091]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1563]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3662]]), B=tensor([[0.2639]])\n",
      "Loss: 1.389317274093628\n",
      "\n",
      "> Iteration 2483/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1091]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1563]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3663]]), B=tensor([[0.2639]])\n",
      "Loss: 1.4169684648513794\n",
      "\n",
      "> Iteration 2484/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1092]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3664]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3680230379104614\n",
      "\n",
      "> Iteration 2485/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1093]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3665]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3877668380737305\n",
      "\n",
      "> Iteration 2486/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1094]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0208]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3665]]), B=tensor([[0.2639]])\n",
      "Loss: 1.4029419422149658\n",
      "\n",
      "> Iteration 2487/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1095]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0209]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3667]]), B=tensor([[0.2639]])\n",
      "Loss: 1.394898772239685\n",
      "\n",
      "> Iteration 2488/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1096]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1562]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0209]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3667]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3984401226043701\n",
      "\n",
      "> Iteration 2489/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1097]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0209]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3668]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3977129459381104\n",
      "\n",
      "> Iteration 2490/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1098]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3668]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3884743452072144\n",
      "\n",
      "> Iteration 2491/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1099]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3669]]), B=tensor([[0.2638]])\n",
      "Loss: 1.4060920476913452\n",
      "\n",
      "> Iteration 2492/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1100]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3670]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3740794658660889\n",
      "\n",
      "> Iteration 2493/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1101]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0210]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3671]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3311302661895752\n",
      "\n",
      "> Iteration 2494/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1101]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1559]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3670]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3966233730316162\n",
      "\n",
      "> Iteration 2495/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1101]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1559]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3670]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3874582052230835\n",
      "\n",
      "> Iteration 2496/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1101]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1558]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3669]]), B=tensor([[0.2637]])\n",
      "Loss: 1.4005963802337646\n",
      "\n",
      "> Iteration 2497/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1101]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3669]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3709405660629272\n",
      "\n",
      "> Iteration 2498/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1102]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0211]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3670]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3695226907730103\n",
      "\n",
      "> Iteration 2499/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1103]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3671]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3667558431625366\n",
      "\n",
      "> Iteration 2500/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1104]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3672]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3940342664718628\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2501/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1106]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3673]]), B=tensor([[0.2636]])\n",
      "Loss: 1.377974271774292\n",
      "\n",
      "> Iteration 2502/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1107]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0212]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3674]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3836690187454224\n",
      "\n",
      "> Iteration 2503/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1107]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3675]]), B=tensor([[0.2636]])\n",
      "Loss: 1.4050129652023315\n",
      "\n",
      "> Iteration 2504/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1108]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3676]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3819305896759033\n",
      "\n",
      "> Iteration 2505/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1108]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3676]]), B=tensor([[0.2636]])\n",
      "Loss: 1.4049161672592163\n",
      "\n",
      "> Iteration 2506/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1109]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3677]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3697192668914795\n",
      "\n",
      "> Iteration 2507/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1110]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3678]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3945891857147217\n",
      "\n",
      "> Iteration 2508/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1110]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3679]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3739532232284546\n",
      "\n",
      "> Iteration 2509/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1111]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0213]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3680]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3797448873519897\n",
      "\n",
      "> Iteration 2510/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1113]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0214]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3681]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3329707384109497\n",
      "\n",
      "> Iteration 2511/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1115]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0214]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3684]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3480076789855957\n",
      "\n",
      "> Iteration 2512/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1119]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0215]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3687]]), B=tensor([[0.2638]])\n",
      "Loss: 1.359646201133728\n",
      "\n",
      "> Iteration 2513/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1122]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0215]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3691]]), B=tensor([[0.2638]])\n",
      "Loss: 1.4161121845245361\n",
      "\n",
      "> Iteration 2514/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1125]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0216]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3694]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3539555072784424\n",
      "\n",
      "> Iteration 2515/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1127]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0216]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3697]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3996182680130005\n",
      "\n",
      "> Iteration 2516/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1129]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3700]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3860981464385986\n",
      "\n",
      "> Iteration 2517/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1131]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3702]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3667988777160645\n",
      "\n",
      "> Iteration 2518/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1132]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1558]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3704]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3630337715148926\n",
      "\n",
      "> Iteration 2519/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1134]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1558]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3706]]), B=tensor([[0.2642]])\n",
      "Loss: 1.347388744354248\n",
      "\n",
      "> Iteration 2520/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1136]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1559]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0217]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3708]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3717702627182007\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2521/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1138]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0218]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3711]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3984065055847168\n",
      "\n",
      "> Iteration 2522/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1139]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0218]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3713]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3650034666061401\n",
      "\n",
      "> Iteration 2523/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1141]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0218]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3715]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3753479719161987\n",
      "\n",
      "> Iteration 2524/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1143]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0218]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3718]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3667411804199219\n",
      "\n",
      "> Iteration 2525/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1145]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0219]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3720]]), B=tensor([[0.2646]])\n",
      "Loss: 1.358373761177063\n",
      "\n",
      "> Iteration 2526/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1147]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0219]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3723]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3951611518859863\n",
      "\n",
      "> Iteration 2527/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1150]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0220]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3725]]), B=tensor([[0.2647]])\n",
      "Loss: 1.4001550674438477\n",
      "\n",
      "> Iteration 2528/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1152]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1561]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0220]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3728]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3751118183135986\n",
      "\n",
      "> Iteration 2529/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1155]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0221]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9379]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3731]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3820663690567017\n",
      "\n",
      "> Iteration 2530/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1158]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1560]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0222]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3733]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3737651109695435\n",
      "\n",
      "> Iteration 2531/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1160]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1559]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0222]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3735]]), B=tensor([[0.2647]])\n",
      "Loss: 1.382112979888916\n",
      "\n",
      "> Iteration 2532/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1163]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1559]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0223]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3737]]), B=tensor([[0.2647]])\n",
      "Loss: 1.4137179851531982\n",
      "\n",
      "> Iteration 2533/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1165]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1558]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0223]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3740]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3979333639144897\n",
      "\n",
      "> Iteration 2534/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1168]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1558]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0224]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3743]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3744709491729736\n",
      "\n",
      "> Iteration 2535/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1171]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1558]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0225]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3746]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3615484237670898\n",
      "\n",
      "> Iteration 2536/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1174]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1558]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0225]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9380]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3749]]), B=tensor([[0.2648]])\n",
      "Loss: 1.374254584312439\n",
      "\n",
      "> Iteration 2537/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1176]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0226]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3751]]), B=tensor([[0.2648]])\n",
      "Loss: 1.4173606634140015\n",
      "\n",
      "> Iteration 2538/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1179]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0226]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3754]]), B=tensor([[0.2648]])\n",
      "Loss: 1.366565465927124\n",
      "\n",
      "> Iteration 2539/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1180]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3756]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3614213466644287\n",
      "\n",
      "> Iteration 2540/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1181]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3756]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3853743076324463\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2541/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1182]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3757]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3727593421936035\n",
      "\n",
      "> Iteration 2542/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1183]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3758]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3535443544387817\n",
      "\n",
      "> Iteration 2543/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1184]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6307]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0227]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3759]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3941372632980347\n",
      "\n",
      "> Iteration 2544/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1185]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1557]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0228]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3760]]), B=tensor([[0.2648]])\n",
      "Loss: 1.396921992301941\n",
      "\n",
      "> Iteration 2545/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1186]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1556]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0228]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9381]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3761]]), B=tensor([[0.2648]])\n",
      "Loss: 1.376281976699829\n",
      "\n",
      "> Iteration 2546/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1188]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1555]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0229]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3762]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3721107244491577\n",
      "\n",
      "> Iteration 2547/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1189]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1555]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0229]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3763]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3564035892486572\n",
      "\n",
      "> Iteration 2548/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1190]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1554]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0229]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9382]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3764]]), B=tensor([[0.2647]])\n",
      "Loss: 1.4170222282409668\n",
      "\n",
      "> Iteration 2549/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1553]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0230]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9383]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3763]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3735519647598267\n",
      "\n",
      "> Iteration 2550/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1191]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1552]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0230]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9384]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3763]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3781108856201172\n",
      "\n",
      "> Iteration 2551/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1551]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0231]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9384]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3763]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3695850372314453\n",
      "\n",
      "> Iteration 2552/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1192]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1550]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0231]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9384]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3763]]), B=tensor([[0.2644]])\n",
      "Loss: 1.375458836555481\n",
      "\n",
      "> Iteration 2553/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1193]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0231]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3763]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3556983470916748\n",
      "\n",
      "> Iteration 2554/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1194]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0231]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3764]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3724201917648315\n",
      "\n",
      "> Iteration 2555/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1195]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0232]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3766]]), B=tensor([[0.2644]])\n",
      "Loss: 1.356770634651184\n",
      "\n",
      "> Iteration 2556/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1197]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0232]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3767]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3853623867034912\n",
      "\n",
      "> Iteration 2557/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1198]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0232]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3769]]), B=tensor([[0.2644]])\n",
      "Loss: 1.369370460510254\n",
      "\n",
      "> Iteration 2558/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1200]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0233]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3770]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3687143325805664\n",
      "\n",
      "> Iteration 2559/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1202]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0233]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3772]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3902615308761597\n",
      "\n",
      "> Iteration 2560/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1204]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0233]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3775]]), B=tensor([[0.2645]])\n",
      "Loss: 1.342464566230774\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2561/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1207]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0234]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3778]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3547389507293701\n",
      "\n",
      "> Iteration 2562/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1210]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0235]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3782]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3660480976104736\n",
      "\n",
      "> Iteration 2563/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1214]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1549]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0235]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3785]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3733940124511719\n",
      "\n",
      "> Iteration 2564/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1217]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1548]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0236]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3788]]), B=tensor([[0.2646]])\n",
      "Loss: 1.367690920829773\n",
      "\n",
      "> Iteration 2565/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1219]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1548]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0237]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3791]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3827991485595703\n",
      "\n",
      "> Iteration 2566/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1222]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1548]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0237]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3794]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3776004314422607\n",
      "\n",
      "> Iteration 2567/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1224]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1548]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0238]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3797]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3937132358551025\n",
      "\n",
      "> Iteration 2568/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1226]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1548]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0238]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3798]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3861781358718872\n",
      "\n",
      "> Iteration 2569/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1227]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1548]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0238]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3799]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3918428421020508\n",
      "\n",
      "> Iteration 2570/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1229]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1548]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0239]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3801]]), B=tensor([[0.2648]])\n",
      "Loss: 1.380167841911316\n",
      "\n",
      "> Iteration 2571/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1230]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1547]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0239]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3802]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3596370220184326\n",
      "\n",
      "> Iteration 2572/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1232]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1547]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0239]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9385]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3804]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3778717517852783\n",
      "\n",
      "> Iteration 2573/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1233]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1546]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0240]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9386]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3804]]), B=tensor([[0.2647]])\n",
      "Loss: 1.4253442287445068\n",
      "\n",
      "> Iteration 2574/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1233]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1546]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0240]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9386]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3805]]), B=tensor([[0.2647]])\n",
      "Loss: 1.403635025024414\n",
      "\n",
      "> Iteration 2575/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1234]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1545]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0240]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9386]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3805]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3699636459350586\n",
      "\n",
      "> Iteration 2576/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1235]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1544]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0241]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3805]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3952462673187256\n",
      "\n",
      "> Iteration 2577/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1236]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1543]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0241]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3806]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3526273965835571\n",
      "\n",
      "> Iteration 2578/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1543]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0241]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3807]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3939940929412842\n",
      "\n",
      "> Iteration 2579/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1543]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3807]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3558356761932373\n",
      "\n",
      "> Iteration 2580/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1238]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3807]]), B=tensor([[0.2645]])\n",
      "Loss: 1.378422737121582\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2581/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3807]]), B=tensor([[0.2645]])\n",
      "Loss: 1.404007911682129\n",
      "\n",
      "> Iteration 2582/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3806]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3834625482559204\n",
      "\n",
      "> Iteration 2583/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3805]]), B=tensor([[0.2644]])\n",
      "Loss: 1.4100861549377441\n",
      "\n",
      "> Iteration 2584/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1541]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3805]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3909592628479004\n",
      "\n",
      "> Iteration 2585/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1541]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3805]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3599342107772827\n",
      "\n",
      "> Iteration 2586/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1541]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3805]]), B=tensor([[0.2643]])\n",
      "Loss: 1.354090929031372\n",
      "\n",
      "> Iteration 2587/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3804]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3989636898040771\n",
      "\n",
      "> Iteration 2588/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3804]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3570148944854736\n",
      "\n",
      "> Iteration 2589/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1237]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0242]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3804]]), B=tensor([[0.2643]])\n",
      "Loss: 1.35554039478302\n",
      "\n",
      "> Iteration 2590/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1238]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0243]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3805]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3959424495697021\n",
      "\n",
      "> Iteration 2591/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1239]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1539]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0243]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3806]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3594772815704346\n",
      "\n",
      "> Iteration 2592/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1241]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1539]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0243]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3808]]), B=tensor([[0.2643]])\n",
      "Loss: 1.37860107421875\n",
      "\n",
      "> Iteration 2593/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1242]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1539]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0244]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9390]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3809]]), B=tensor([[0.2643]])\n",
      "Loss: 1.4241828918457031\n",
      "\n",
      "> Iteration 2594/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1244]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1538]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0244]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9390]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3810]]), B=tensor([[0.2643]])\n",
      "Loss: 1.386093258857727\n",
      "\n",
      "> Iteration 2595/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1245]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1538]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0244]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9390]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3812]]), B=tensor([[0.2643]])\n",
      "Loss: 1.360799789428711\n",
      "\n",
      "> Iteration 2596/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1246]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1539]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0245]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3814]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3690199851989746\n",
      "\n",
      "> Iteration 2597/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1248]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1539]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0245]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3816]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3988231420516968\n",
      "\n",
      "> Iteration 2598/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1251]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1539]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0245]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3819]]), B=tensor([[0.2645]])\n",
      "Loss: 1.411536693572998\n",
      "\n",
      "> Iteration 2599/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1252]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0245]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3820]]), B=tensor([[0.2645]])\n",
      "Loss: 1.4050134420394897\n",
      "\n",
      "> Iteration 2600/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1253]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3822]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3945668935775757\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2601/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1254]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3823]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3537766933441162\n",
      "\n",
      "> Iteration 2602/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1254]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1541]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3823]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3764066696166992\n",
      "\n",
      "> Iteration 2603/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1254]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1541]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3824]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3887940645217896\n",
      "\n",
      "> Iteration 2604/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1255]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1541]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3825]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3688340187072754\n",
      "\n",
      "> Iteration 2605/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1256]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3827]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3979989290237427\n",
      "\n",
      "> Iteration 2606/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1258]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3829]]), B=tensor([[0.2648]])\n",
      "Loss: 1.4073902368545532\n",
      "\n",
      "> Iteration 2607/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1260]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0246]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3831]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3444671630859375\n",
      "\n",
      "> Iteration 2608/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1262]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0247]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3833]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3822027444839478\n",
      "\n",
      "> Iteration 2609/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1264]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0247]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3835]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3789937496185303\n",
      "\n",
      "> Iteration 2610/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1267]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0248]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3838]]), B=tensor([[0.2649]])\n",
      "Loss: 1.341673731803894\n",
      "\n",
      "> Iteration 2611/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1270]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0248]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3841]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3768436908721924\n",
      "\n",
      "> Iteration 2612/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1272]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0249]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9387]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3844]]), B=tensor([[0.2650]])\n",
      "Loss: 1.39033842086792\n",
      "\n",
      "> Iteration 2613/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1274]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1542]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0249]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3846]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3552716970443726\n",
      "\n",
      "> Iteration 2614/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1276]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1541]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0250]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3848]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3573617935180664\n",
      "\n",
      "> Iteration 2615/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1279]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1541]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0250]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3851]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3616632223129272\n",
      "\n",
      "> Iteration 2616/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1282]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1541]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0251]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3853]]), B=tensor([[0.2650]])\n",
      "Loss: 1.367716908454895\n",
      "\n",
      "> Iteration 2617/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1285]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0252]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3856]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3760359287261963\n",
      "\n",
      "> Iteration 2618/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1288]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1540]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0252]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9388]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3859]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3909636735916138\n",
      "\n",
      "> Iteration 2619/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1290]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1539]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0253]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3861]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3967046737670898\n",
      "\n",
      "> Iteration 2620/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1292]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1537]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0254]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9389]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3863]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3850973844528198\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2621/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1294]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1536]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0255]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9390]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3864]]), B=tensor([[0.2649]])\n",
      "Loss: 1.405839204788208\n",
      "\n",
      "> Iteration 2622/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1296]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1535]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0255]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9391]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3864]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3766752481460571\n",
      "\n",
      "> Iteration 2623/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1297]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1533]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0256]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9391]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3865]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3569365739822388\n",
      "\n",
      "> Iteration 2624/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1299]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1532]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0257]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3866]]), B=tensor([[0.2646]])\n",
      "Loss: 1.371738314628601\n",
      "\n",
      "> Iteration 2625/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1301]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6298]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0258]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3867]]), B=tensor([[0.2645]])\n",
      "Loss: 1.364253282546997\n",
      "\n",
      "> Iteration 2626/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1303]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6298]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0258]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3868]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3656283617019653\n",
      "\n",
      "> Iteration 2627/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1304]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0259]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3869]]), B=tensor([[0.2644]])\n",
      "Loss: 1.404892086982727\n",
      "\n",
      "> Iteration 2628/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1305]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1528]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0259]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3870]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3941245079040527\n",
      "\n",
      "> Iteration 2629/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1306]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1527]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0260]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3870]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3702372312545776\n",
      "\n",
      "> Iteration 2630/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1307]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0260]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3871]]), B=tensor([[0.2642]])\n",
      "Loss: 1.4058928489685059\n",
      "\n",
      "> Iteration 2631/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1309]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0261]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3871]]), B=tensor([[0.2642]])\n",
      "Loss: 1.3704237937927246\n",
      "\n",
      "> Iteration 2632/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1309]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0261]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3872]]), B=tensor([[0.2641]])\n",
      "Loss: 1.365480661392212\n",
      "\n",
      "> Iteration 2633/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1310]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3872]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3756173849105835\n",
      "\n",
      "> Iteration 2634/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1311]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3873]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3913763761520386\n",
      "\n",
      "> Iteration 2635/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1312]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3873]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3481523990631104\n",
      "\n",
      "> Iteration 2636/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1312]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3874]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3788175582885742\n",
      "\n",
      "> Iteration 2637/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1312]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3873]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3926962614059448\n",
      "\n",
      "> Iteration 2638/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1312]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3873]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3968141078948975\n",
      "\n",
      "> Iteration 2639/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0262]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3875]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3837482929229736\n",
      "\n",
      "> Iteration 2640/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3875]]), B=tensor([[0.2641]])\n",
      "Loss: 1.4044822454452515\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2641/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1314]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3876]]), B=tensor([[0.2642]])\n",
      "Loss: 1.4204124212265015\n",
      "\n",
      "> Iteration 2642/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1315]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3878]]), B=tensor([[0.2642]])\n",
      "Loss: 1.4135714769363403\n",
      "\n",
      "> Iteration 2643/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1317]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3880]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3928725719451904\n",
      "\n",
      "> Iteration 2644/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1319]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3882]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3559000492095947\n",
      "\n",
      "> Iteration 2645/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1319]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3883]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3840422630310059\n",
      "\n",
      "> Iteration 2646/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1320]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3884]]), B=tensor([[0.2643]])\n",
      "Loss: 1.4073396921157837\n",
      "\n",
      "> Iteration 2647/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1321]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3884]]), B=tensor([[0.2644]])\n",
      "Loss: 1.41478431224823\n",
      "\n",
      "> Iteration 2648/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1322]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3886]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3763628005981445\n",
      "\n",
      "> Iteration 2649/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3887]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3694220781326294\n",
      "\n",
      "> Iteration 2650/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3887]]), B=tensor([[0.2643]])\n",
      "Loss: 1.396219253540039\n",
      "\n",
      "> Iteration 2651/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3887]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3788502216339111\n",
      "\n",
      "> Iteration 2652/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3887]]), B=tensor([[0.2643]])\n",
      "Loss: 1.422302484512329\n",
      "\n",
      "> Iteration 2653/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3887]]), B=tensor([[0.2642]])\n",
      "Loss: 1.3891960382461548\n",
      "\n",
      "> Iteration 2654/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1327]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3889]]), B=tensor([[0.2642]])\n",
      "Loss: 1.3668161630630493\n",
      "\n",
      "> Iteration 2655/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1328]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1520]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3889]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3860164880752563\n",
      "\n",
      "> Iteration 2656/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1329]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1520]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3890]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3607220649719238\n",
      "\n",
      "> Iteration 2657/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1329]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1519]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3890]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3946149349212646\n",
      "\n",
      "> Iteration 2658/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1329]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1519]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3890]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3860936164855957\n",
      "\n",
      "> Iteration 2659/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1330]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3890]]), B=tensor([[0.2640]])\n",
      "Loss: 1.4119759798049927\n",
      "\n",
      "> Iteration 2660/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1330]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3889]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3824515342712402\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2661/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1329]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3888]]), B=tensor([[0.2638]])\n",
      "Loss: 1.349086880683899\n",
      "\n",
      "> Iteration 2662/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1328]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3886]]), B=tensor([[0.2637]])\n",
      "Loss: 1.366369366645813\n",
      "\n",
      "> Iteration 2663/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1327]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3885]]), B=tensor([[0.2637]])\n",
      "Loss: 1.4163932800292969\n",
      "\n",
      "> Iteration 2664/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3882]]), B=tensor([[0.2636]])\n",
      "Loss: 1.387671709060669\n",
      "\n",
      "> Iteration 2665/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3881]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3809517621994019\n",
      "\n",
      "> Iteration 2666/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3880]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3573535680770874\n",
      "\n",
      "> Iteration 2667/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3880]]), B=tensor([[0.2636]])\n",
      "Loss: 1.41194748878479\n",
      "\n",
      "> Iteration 2668/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3880]]), B=tensor([[0.2636]])\n",
      "Loss: 1.324867606163025\n",
      "\n",
      "> Iteration 2669/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3880]]), B=tensor([[0.2636]])\n",
      "Loss: 1.410478949546814\n",
      "\n",
      "> Iteration 2670/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3881]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3763759136199951\n",
      "\n",
      "> Iteration 2671/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3880]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3786638975143433\n",
      "\n",
      "> Iteration 2672/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1322]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3879]]), B=tensor([[0.2637]])\n",
      "Loss: 1.392511010169983\n",
      "\n",
      "> Iteration 2673/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1322]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3879]]), B=tensor([[0.2636]])\n",
      "Loss: 1.4098286628723145\n",
      "\n",
      "> Iteration 2674/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3881]]), B=tensor([[0.2637]])\n",
      "Loss: 1.388708233833313\n",
      "\n",
      "> Iteration 2675/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3882]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3755143880844116\n",
      "\n",
      "> Iteration 2676/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3883]]), B=tensor([[0.2638]])\n",
      "Loss: 1.387046456336975\n",
      "\n",
      "> Iteration 2677/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3884]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3475319147109985\n",
      "\n",
      "> Iteration 2678/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3885]]), B=tensor([[0.2639]])\n",
      "Loss: 1.396081566810608\n",
      "\n",
      "> Iteration 2679/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3885]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3642019033432007\n",
      "\n",
      "> Iteration 2680/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3886]]), B=tensor([[0.2639]])\n",
      "Loss: 1.384459376335144\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2681/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3886]]), B=tensor([[0.2640]])\n",
      "Loss: 1.386938214302063\n",
      "\n",
      "> Iteration 2682/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1519]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3886]]), B=tensor([[0.2640]])\n",
      "Loss: 1.416568636894226\n",
      "\n",
      "> Iteration 2683/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1519]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3885]]), B=tensor([[0.2640]])\n",
      "Loss: 1.394228458404541\n",
      "\n",
      "> Iteration 2684/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1519]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3885]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3942638635635376\n",
      "\n",
      "> Iteration 2685/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1519]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3885]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3948689699172974\n",
      "\n",
      "> Iteration 2686/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1519]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3884]]), B=tensor([[0.2640]])\n",
      "Loss: 1.4069063663482666\n",
      "\n",
      "> Iteration 2687/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3883]]), B=tensor([[0.2639]])\n",
      "Loss: 1.389260172843933\n",
      "\n",
      "> Iteration 2688/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1322]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3882]]), B=tensor([[0.2639]])\n",
      "Loss: 1.400875449180603\n",
      "\n",
      "> Iteration 2689/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1322]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3881]]), B=tensor([[0.2639]])\n",
      "Loss: 1.4020670652389526\n",
      "\n",
      "> Iteration 2690/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1321]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3879]]), B=tensor([[0.2638]])\n",
      "Loss: 1.39092218875885\n",
      "\n",
      "> Iteration 2691/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1319]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3877]]), B=tensor([[0.2637]])\n",
      "Loss: 1.4141112565994263\n",
      "\n",
      "> Iteration 2692/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1318]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3876]]), B=tensor([[0.2637]])\n",
      "Loss: 1.4122328758239746\n",
      "\n",
      "> Iteration 2693/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1317]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3874]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3834785223007202\n",
      "\n",
      "> Iteration 2694/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1316]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3873]]), B=tensor([[0.2636]])\n",
      "Loss: 1.370532751083374\n",
      "\n",
      "> Iteration 2695/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1315]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3871]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3831809759140015\n",
      "\n",
      "> Iteration 2696/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4064]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3870]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3874512910842896\n",
      "\n",
      "> Iteration 2697/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4064]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3869]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3646571636199951\n",
      "\n",
      "> Iteration 2698/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3869]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3855990171432495\n",
      "\n",
      "> Iteration 2699/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1313]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3870]]), B=tensor([[0.2635]])\n",
      "Loss: 1.335687518119812\n",
      "\n",
      "> Iteration 2700/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1314]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3871]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3267029523849487\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2701/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1315]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3873]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3747395277023315\n",
      "\n",
      "> Iteration 2702/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1315]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3874]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3405871391296387\n",
      "\n",
      "> Iteration 2703/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1317]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1519]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3876]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3734862804412842\n",
      "\n",
      "> Iteration 2704/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1318]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1520]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3879]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3786256313323975\n",
      "\n",
      "> Iteration 2705/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1319]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3881]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3773694038391113\n",
      "\n",
      "> Iteration 2706/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1321]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3884]]), B=tensor([[0.2642]])\n",
      "Loss: 1.3622899055480957\n",
      "\n",
      "> Iteration 2707/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1323]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3886]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3622961044311523\n",
      "\n",
      "> Iteration 2708/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1324]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3888]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3897225856781006\n",
      "\n",
      "> Iteration 2709/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1325]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3890]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3856589794158936\n",
      "\n",
      "> Iteration 2710/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1326]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6298]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1528]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3893]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3396705389022827\n",
      "\n",
      "> Iteration 2711/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1327]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3894]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3310514688491821\n",
      "\n",
      "> Iteration 2712/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1328]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3897]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3977761268615723\n",
      "\n",
      "> Iteration 2713/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1330]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3898]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3698503971099854\n",
      "\n",
      "> Iteration 2714/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2650]])\n",
      "Loss: 1.37237548828125\n",
      "\n",
      "> Iteration 2715/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3901]]), B=tensor([[0.2650]])\n",
      "Loss: 1.4142030477523804\n",
      "\n",
      "> Iteration 2716/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2650]])\n",
      "Loss: 1.369296908378601\n",
      "\n",
      "> Iteration 2717/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1532]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2651]])\n",
      "Loss: 1.3750888109207153\n",
      "\n",
      "> Iteration 2718/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0263]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3516888618469238\n",
      "\n",
      "> Iteration 2719/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3627045154571533\n",
      "\n",
      "> Iteration 2720/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1330]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3899]]), B=tensor([[0.2650]])\n",
      "Loss: 1.379990816116333\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2721/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1330]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3898]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3970754146575928\n",
      "\n",
      "> Iteration 2722/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1330]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3898]]), B=tensor([[0.2649]])\n",
      "Loss: 1.4049245119094849\n",
      "\n",
      "> Iteration 2723/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1331]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0264]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3899]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3747793436050415\n",
      "\n",
      "> Iteration 2724/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1332]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3892302513122559\n",
      "\n",
      "> Iteration 2725/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1333]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3901]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3887215852737427\n",
      "\n",
      "> Iteration 2726/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1333]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3901]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3625526428222656\n",
      "\n",
      "> Iteration 2727/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1334]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3902]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3499655723571777\n",
      "\n",
      "> Iteration 2728/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1334]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3903]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3753265142440796\n",
      "\n",
      "> Iteration 2729/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1335]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3903]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3687795400619507\n",
      "\n",
      "> Iteration 2730/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1335]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3904]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3674932718276978\n",
      "\n",
      "> Iteration 2731/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1336]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3905]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3976123332977295\n",
      "\n",
      "> Iteration 2732/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1337]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0265]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3906]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3810176849365234\n",
      "\n",
      "> Iteration 2733/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1339]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3908]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3723920583724976\n",
      "\n",
      "> Iteration 2734/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1340]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3909]]), B=tensor([[0.2650]])\n",
      "Loss: 1.3696879148483276\n",
      "\n",
      "> Iteration 2735/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1341]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0266]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3910]]), B=tensor([[0.2651]])\n",
      "Loss: 1.40714430809021\n",
      "\n",
      "> Iteration 2736/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3912]]), B=tensor([[0.2651]])\n",
      "Loss: 1.3975895643234253\n",
      "\n",
      "> Iteration 2737/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1345]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0267]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3914]]), B=tensor([[0.2651]])\n",
      "Loss: 1.3572022914886475\n",
      "\n",
      "> Iteration 2738/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1346]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3916]]), B=tensor([[0.2651]])\n",
      "Loss: 1.383201241493225\n",
      "\n",
      "> Iteration 2739/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1348]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3918]]), B=tensor([[0.2651]])\n",
      "Loss: 1.416182518005371\n",
      "\n",
      "> Iteration 2740/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1350]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0268]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3920]]), B=tensor([[0.2652]])\n",
      "Loss: 1.3808910846710205\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2741/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1351]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0269]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3921]]), B=tensor([[0.2652]])\n",
      "Loss: 1.3944827318191528\n",
      "\n",
      "> Iteration 2742/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1353]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0269]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3923]]), B=tensor([[0.2653]])\n",
      "Loss: 1.4129970073699951\n",
      "\n",
      "> Iteration 2743/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1354]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0269]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3925]]), B=tensor([[0.2653]])\n",
      "Loss: 1.3513360023498535\n",
      "\n",
      "> Iteration 2744/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1356]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0269]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3927]]), B=tensor([[0.2654]])\n",
      "Loss: 1.373517632484436\n",
      "\n",
      "> Iteration 2745/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1357]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0269]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3929]]), B=tensor([[0.2654]])\n",
      "Loss: 1.3812110424041748\n",
      "\n",
      "> Iteration 2746/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1359]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0270]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3932]]), B=tensor([[0.2655]])\n",
      "Loss: 1.3609483242034912\n",
      "\n",
      "> Iteration 2747/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1361]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1532]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0270]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9391]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3934]]), B=tensor([[0.2656]])\n",
      "Loss: 1.3775322437286377\n",
      "\n",
      "> Iteration 2748/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1363]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1532]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0270]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9391]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3936]]), B=tensor([[0.2656]])\n",
      "Loss: 1.3904708623886108\n",
      "\n",
      "> Iteration 2749/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1365]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1532]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0270]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9391]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3938]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3420732021331787\n",
      "\n",
      "> Iteration 2750/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1366]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0271]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9391]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3940]]), B=tensor([[0.2657]])\n",
      "Loss: 1.407876968383789\n",
      "\n",
      "> Iteration 2751/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0271]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9391]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3942]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3695433139801025\n",
      "\n",
      "> Iteration 2752/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0272]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9391]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3944]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3677775859832764\n",
      "\n",
      "> Iteration 2753/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1531]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0272]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3945]]), B=tensor([[0.2657]])\n",
      "Loss: 1.4109885692596436\n",
      "\n",
      "> Iteration 2754/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3947]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3496440649032593\n",
      "\n",
      "> Iteration 2755/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1375]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1530]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9392]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2656]])\n",
      "Loss: 1.3784171342849731\n",
      "\n",
      "> Iteration 2756/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1529]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2656]])\n",
      "Loss: 1.4094375371932983\n",
      "\n",
      "> Iteration 2757/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1528]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2656]])\n",
      "Loss: 1.417352318763733\n",
      "\n",
      "> Iteration 2758/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1377]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1527]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2655]])\n",
      "Loss: 1.4028394222259521\n",
      "\n",
      "> Iteration 2759/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1377]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2655]])\n",
      "Loss: 1.3370238542556763\n",
      "\n",
      "> Iteration 2760/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1377]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2654]])\n",
      "Loss: 1.3668962717056274\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2761/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2654]])\n",
      "Loss: 1.3475356101989746\n",
      "\n",
      "> Iteration 2762/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0276]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2653]])\n",
      "Loss: 1.3843958377838135\n",
      "\n",
      "> Iteration 2763/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0276]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2653]])\n",
      "Loss: 1.3983924388885498\n",
      "\n",
      "> Iteration 2764/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3950]]), B=tensor([[0.2653]])\n",
      "Loss: 1.376906394958496\n",
      "\n",
      "> Iteration 2765/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1383]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3953]]), B=tensor([[0.2653]])\n",
      "Loss: 1.3648258447647095\n",
      "\n",
      "> Iteration 2766/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1387]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3956]]), B=tensor([[0.2653]])\n",
      "Loss: 1.3996316194534302\n",
      "\n",
      "> Iteration 2767/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1390]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3960]]), B=tensor([[0.2653]])\n",
      "Loss: 1.3609356880187988\n",
      "\n",
      "> Iteration 2768/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1393]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3962]]), B=tensor([[0.2654]])\n",
      "Loss: 1.4154016971588135\n",
      "\n",
      "> Iteration 2769/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1396]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3965]]), B=tensor([[0.2654]])\n",
      "Loss: 1.3545479774475098\n",
      "\n",
      "> Iteration 2770/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1399]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3968]]), B=tensor([[0.2654]])\n",
      "Loss: 1.4075368642807007\n",
      "\n",
      "> Iteration 2771/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1402]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3971]]), B=tensor([[0.2654]])\n",
      "Loss: 1.342153787612915\n",
      "\n",
      "> Iteration 2772/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1405]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1520]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3974]]), B=tensor([[0.2654]])\n",
      "Loss: 1.3706533908843994\n",
      "\n",
      "> Iteration 2773/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1407]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3977]]), B=tensor([[0.2655]])\n",
      "Loss: 1.4112884998321533\n",
      "\n",
      "> Iteration 2774/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1409]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3980]]), B=tensor([[0.2656]])\n",
      "Loss: 1.4002691507339478\n",
      "\n",
      "> Iteration 2775/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1411]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3982]]), B=tensor([[0.2656]])\n",
      "Loss: 1.4060304164886475\n",
      "\n",
      "> Iteration 2776/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1413]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3984]]), B=tensor([[0.2656]])\n",
      "Loss: 1.3879703283309937\n",
      "\n",
      "> Iteration 2777/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1415]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3986]]), B=tensor([[0.2656]])\n",
      "Loss: 1.39308762550354\n",
      "\n",
      "> Iteration 2778/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1416]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3987]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3834608793258667\n",
      "\n",
      "> Iteration 2779/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1419]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3990]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3773159980773926\n",
      "\n",
      "> Iteration 2780/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1420]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3991]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3986716270446777\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2781/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1420]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3992]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3825478553771973\n",
      "\n",
      "> Iteration 2782/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1421]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3993]]), B=tensor([[0.2658]])\n",
      "Loss: 1.373904824256897\n",
      "\n",
      "> Iteration 2783/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1423]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3995]]), B=tensor([[0.2658]])\n",
      "Loss: 1.381013035774231\n",
      "\n",
      "> Iteration 2784/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1424]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3996]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3988038301467896\n",
      "\n",
      "> Iteration 2785/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1425]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3998]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3970872163772583\n",
      "\n",
      "> Iteration 2786/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1426]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2659]])\n",
      "Loss: 1.390519142150879\n",
      "\n",
      "> Iteration 2787/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3972623348236084\n",
      "\n",
      "> Iteration 2788/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1428]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3766005039215088\n",
      "\n",
      "> Iteration 2789/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1429]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4001]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3590470552444458\n",
      "\n",
      "> Iteration 2790/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1430]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4002]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3816221952438354\n",
      "\n",
      "> Iteration 2791/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1430]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4002]]), B=tensor([[0.2659]])\n",
      "Loss: 1.376595139503479\n",
      "\n",
      "> Iteration 2792/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1430]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4003]]), B=tensor([[0.2660]])\n",
      "Loss: 1.4033668041229248\n",
      "\n",
      "> Iteration 2793/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1429]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4003]]), B=tensor([[0.2660]])\n",
      "Loss: 1.4195311069488525\n",
      "\n",
      "> Iteration 2794/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1429]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4002]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3689700365066528\n",
      "\n",
      "> Iteration 2795/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1429]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4003]]), B=tensor([[0.2661]])\n",
      "Loss: 1.4084326028823853\n",
      "\n",
      "> Iteration 2796/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1428]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4002]]), B=tensor([[0.2661]])\n",
      "Loss: 1.3562451601028442\n",
      "\n",
      "> Iteration 2797/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1429]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4003]]), B=tensor([[0.2661]])\n",
      "Loss: 1.4030938148498535\n",
      "\n",
      "> Iteration 2798/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1429]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4002]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3975560665130615\n",
      "\n",
      "> Iteration 2799/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1428]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4001]]), B=tensor([[0.2660]])\n",
      "Loss: 1.364650011062622\n",
      "\n",
      "> Iteration 2800/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3702800273895264\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2801/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3399832248687744\n",
      "\n",
      "> Iteration 2802/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2660]])\n",
      "Loss: 1.4178211688995361\n",
      "\n",
      "> Iteration 2803/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2660]])\n",
      "Loss: 1.375084638595581\n",
      "\n",
      "> Iteration 2804/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2660]])\n",
      "Loss: 1.345321774482727\n",
      "\n",
      "> Iteration 2805/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3962109088897705\n",
      "\n",
      "> Iteration 2806/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1426]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3773409128189087\n",
      "\n",
      "> Iteration 2807/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3487459421157837\n",
      "\n",
      "> Iteration 2808/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4001]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3693281412124634\n",
      "\n",
      "> Iteration 2809/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3810902833938599\n",
      "\n",
      "> Iteration 2810/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1426]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3718026876449585\n",
      "\n",
      "> Iteration 2811/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1426]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3577322959899902\n",
      "\n",
      "> Iteration 2812/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2659]])\n",
      "Loss: 1.383065938949585\n",
      "\n",
      "> Iteration 2813/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3717728853225708\n",
      "\n",
      "> Iteration 2814/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1426]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3732622861862183\n",
      "\n",
      "> Iteration 2815/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1426]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2659]])\n",
      "Loss: 1.424910545349121\n",
      "\n",
      "> Iteration 2816/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3793878555297852\n",
      "\n",
      "> Iteration 2817/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1428]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4001]]), B=tensor([[0.2659]])\n",
      "Loss: 1.3811277151107788\n",
      "\n",
      "> Iteration 2818/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1429]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4002]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3884612321853638\n",
      "\n",
      "> Iteration 2819/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1430]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4004]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3787577152252197\n",
      "\n",
      "> Iteration 2820/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1431]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4005]]), B=tensor([[0.2661]])\n",
      "Loss: 1.3807040452957153\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2821/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1432]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4007]]), B=tensor([[0.2661]])\n",
      "Loss: 1.3795289993286133\n",
      "\n",
      "> Iteration 2822/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1434]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4008]]), B=tensor([[0.2662]])\n",
      "Loss: 1.3886038064956665\n",
      "\n",
      "> Iteration 2823/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1435]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4011]]), B=tensor([[0.2662]])\n",
      "Loss: 1.3940035104751587\n",
      "\n",
      "> Iteration 2824/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1437]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4013]]), B=tensor([[0.2663]])\n",
      "Loss: 1.359455943107605\n",
      "\n",
      "> Iteration 2825/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1438]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4014]]), B=tensor([[0.2663]])\n",
      "Loss: 1.3858708143234253\n",
      "\n",
      "> Iteration 2826/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1438]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4013]]), B=tensor([[0.2663]])\n",
      "Loss: 1.4025957584381104\n",
      "\n",
      "> Iteration 2827/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1437]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4012]]), B=tensor([[0.2663]])\n",
      "Loss: 1.3615578413009644\n",
      "\n",
      "> Iteration 2828/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1435]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0288]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4011]]), B=tensor([[0.2662]])\n",
      "Loss: 1.3827780485153198\n",
      "\n",
      "> Iteration 2829/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1434]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6306]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4009]]), B=tensor([[0.2662]])\n",
      "Loss: 1.3800803422927856\n",
      "\n",
      "> Iteration 2830/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1432]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4007]]), B=tensor([[0.2662]])\n",
      "Loss: 1.4013577699661255\n",
      "\n",
      "> Iteration 2831/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1431]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0287]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4006]]), B=tensor([[0.2662]])\n",
      "Loss: 1.3813443183898926\n",
      "\n",
      "> Iteration 2832/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1429]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4084]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4004]]), B=tensor([[0.2661]])\n",
      "Loss: 1.4015870094299316\n",
      "\n",
      "> Iteration 2833/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1427]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4002]]), B=tensor([[0.2661]])\n",
      "Loss: 1.3994110822677612\n",
      "\n",
      "> Iteration 2834/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1425]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0286]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.4000]]), B=tensor([[0.2661]])\n",
      "Loss: 1.3609250783920288\n",
      "\n",
      "> Iteration 2835/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1424]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3999]]), B=tensor([[0.2661]])\n",
      "Loss: 1.395315408706665\n",
      "\n",
      "> Iteration 2836/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1423]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3998]]), B=tensor([[0.2661]])\n",
      "Loss: 1.3707268238067627\n",
      "\n",
      "> Iteration 2837/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1422]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3997]]), B=tensor([[0.2661]])\n",
      "Loss: 1.4060670137405396\n",
      "\n",
      "> Iteration 2838/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1421]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3995]]), B=tensor([[0.2661]])\n",
      "Loss: 1.3547523021697998\n",
      "\n",
      "> Iteration 2839/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1419]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3994]]), B=tensor([[0.2660]])\n",
      "Loss: 1.379167079925537\n",
      "\n",
      "> Iteration 2840/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1418]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3993]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3928054571151733\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2841/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1417]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3991]]), B=tensor([[0.2661]])\n",
      "Loss: 1.375240445137024\n",
      "\n",
      "> Iteration 2842/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1415]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3989]]), B=tensor([[0.2661]])\n",
      "Loss: 1.4002573490142822\n",
      "\n",
      "> Iteration 2843/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1413]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1527]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3988]]), B=tensor([[0.2661]])\n",
      "Loss: 1.396852731704712\n",
      "\n",
      "> Iteration 2844/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1412]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3987]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3786884546279907\n",
      "\n",
      "> Iteration 2845/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1412]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1527]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3986]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3944205045700073\n",
      "\n",
      "> Iteration 2846/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1412]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3986]]), B=tensor([[0.2660]])\n",
      "Loss: 1.412686824798584\n",
      "\n",
      "> Iteration 2847/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1411]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3985]]), B=tensor([[0.2660]])\n",
      "Loss: 1.395789384841919\n",
      "\n",
      "> Iteration 2848/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1410]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3985]]), B=tensor([[0.2660]])\n",
      "Loss: 1.4097477197647095\n",
      "\n",
      "> Iteration 2849/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1410]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3984]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3973015546798706\n",
      "\n",
      "> Iteration 2850/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1409]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6305]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4083]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1526]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9393]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3983]]), B=tensor([[0.2660]])\n",
      "Loss: 1.3455331325531006\n",
      "\n",
      "> Iteration 2851/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1407]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6304]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4082]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1525]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3980]]), B=tensor([[0.2659]])\n",
      "Loss: 1.4212414026260376\n",
      "\n",
      "> Iteration 2852/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1405]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9394]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3978]]), B=tensor([[0.2658]])\n",
      "Loss: 1.3930187225341797\n",
      "\n",
      "> Iteration 2853/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1404]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4081]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1524]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3977]]), B=tensor([[0.2657]])\n",
      "Loss: 1.3532794713974\n",
      "\n",
      "> Iteration 2854/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1404]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6303]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1523]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3976]]), B=tensor([[0.2657]])\n",
      "Loss: 1.401709794998169\n",
      "\n",
      "> Iteration 2855/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1403]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6302]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4080]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1522]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9395]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3974]]), B=tensor([[0.2656]])\n",
      "Loss: 1.3984521627426147\n",
      "\n",
      "> Iteration 2856/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1401]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4079]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1521]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9396]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3971]]), B=tensor([[0.2655]])\n",
      "Loss: 1.388436198234558\n",
      "\n",
      "> Iteration 2857/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1399]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6301]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4078]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1520]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3968]]), B=tensor([[0.2654]])\n",
      "Loss: 1.3916082382202148\n",
      "\n",
      "> Iteration 2858/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1397]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6300]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4077]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1519]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9397]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3965]]), B=tensor([[0.2653]])\n",
      "Loss: 1.375289797782898\n",
      "\n",
      "> Iteration 2859/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1396]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6299]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4076]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3963]]), B=tensor([[0.2652]])\n",
      "Loss: 1.420889973640442\n",
      "\n",
      "> Iteration 2860/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1394]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6298]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3961]]), B=tensor([[0.2651]])\n",
      "Loss: 1.3551878929138184\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2861/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1393]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6298]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4075]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3959]]), B=tensor([[0.2650]])\n",
      "Loss: 1.364848017692566\n",
      "\n",
      "> Iteration 2862/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1391]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3956]]), B=tensor([[0.2649]])\n",
      "Loss: 1.397112250328064\n",
      "\n",
      "> Iteration 2863/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1389]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3953]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3895212411880493\n",
      "\n",
      "> Iteration 2864/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1388]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3952]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3758549690246582\n",
      "\n",
      "> Iteration 2865/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1387]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3951]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3925762176513672\n",
      "\n",
      "> Iteration 2866/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1385]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3949]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3741551637649536\n",
      "\n",
      "> Iteration 2867/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1384]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3948]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3532493114471436\n",
      "\n",
      "> Iteration 2868/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1382]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3946]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3643028736114502\n",
      "\n",
      "> Iteration 2869/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1380]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3944]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3665004968643188\n",
      "\n",
      "> Iteration 2870/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3942]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3970222473144531\n",
      "\n",
      "> Iteration 2871/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3939]]), B=tensor([[0.2646]])\n",
      "Loss: 1.420152187347412\n",
      "\n",
      "> Iteration 2872/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3937]]), B=tensor([[0.2645]])\n",
      "Loss: 1.4056851863861084\n",
      "\n",
      "> Iteration 2873/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1371]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3934]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3442254066467285\n",
      "\n",
      "> Iteration 2874/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3932]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3659677505493164\n",
      "\n",
      "> Iteration 2875/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3931]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3575516939163208\n",
      "\n",
      "> Iteration 2876/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1367]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3929]]), B=tensor([[0.2644]])\n",
      "Loss: 1.4088959693908691\n",
      "\n",
      "> Iteration 2877/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1367]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1512]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3928]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3645099401474\n",
      "\n",
      "> Iteration 2878/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1367]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1512]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3928]]), B=tensor([[0.2643]])\n",
      "Loss: 1.353603482246399\n",
      "\n",
      "> Iteration 2879/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1511]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3929]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3358945846557617\n",
      "\n",
      "> Iteration 2880/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1511]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3929]]), B=tensor([[0.2642]])\n",
      "Loss: 1.4074031114578247\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2881/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1510]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3929]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3881739377975464\n",
      "\n",
      "> Iteration 2882/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1509]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9403]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3929]]), B=tensor([[0.2641]])\n",
      "Loss: 1.378767490386963\n",
      "\n",
      "> Iteration 2883/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1371]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1508]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9403]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3930]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3396368026733398\n",
      "\n",
      "> Iteration 2884/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1508]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9403]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3930]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3805805444717407\n",
      "\n",
      "> Iteration 2885/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1507]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3931]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3896840810775757\n",
      "\n",
      "> Iteration 2886/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1373]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1507]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3931]]), B=tensor([[0.2640]])\n",
      "Loss: 1.440036416053772\n",
      "\n",
      "> Iteration 2887/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1506]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3931]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3617550134658813\n",
      "\n",
      "> Iteration 2888/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1506]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3932]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3707880973815918\n",
      "\n",
      "> Iteration 2889/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1375]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1506]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3932]]), B=tensor([[0.2639]])\n",
      "Loss: 1.360071063041687\n",
      "\n",
      "> Iteration 2890/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1375]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1505]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3933]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3968197107315063\n",
      "\n",
      "> Iteration 2891/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1505]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9405]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3933]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3855175971984863\n",
      "\n",
      "> Iteration 2892/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1504]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9405]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3933]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3410099744796753\n",
      "\n",
      "> Iteration 2893/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4064]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1503]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9406]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3932]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3671905994415283\n",
      "\n",
      "> Iteration 2894/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6288]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4064]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1502]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9406]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3931]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3630180358886719\n",
      "\n",
      "> Iteration 2895/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6288]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4063]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1501]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9406]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3930]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3665753602981567\n",
      "\n",
      "> Iteration 2896/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1375]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6287]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4062]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1500]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9407]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3929]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3797776699066162\n",
      "\n",
      "> Iteration 2897/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6287]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4062]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1500]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9407]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3928]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3954020738601685\n",
      "\n",
      "> Iteration 2898/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6286]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4061]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1499]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9408]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3927]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3830410242080688\n",
      "\n",
      "> Iteration 2899/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1373]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6286]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4061]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1499]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9408]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3926]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3792328834533691\n",
      "\n",
      "> Iteration 2900/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6286]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4061]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1499]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9408]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3924]]), B=tensor([[0.2634]])\n",
      "Loss: 1.3721401691436768\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2901/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1371]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6285]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4060]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1498]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9408]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3923]]), B=tensor([[0.2633]])\n",
      "Loss: 1.414537787437439\n",
      "\n",
      "> Iteration 2902/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6285]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4060]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1498]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9408]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3922]]), B=tensor([[0.2633]])\n",
      "Loss: 1.396322250366211\n",
      "\n",
      "> Iteration 2903/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6285]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4060]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1497]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9409]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3921]]), B=tensor([[0.2632]])\n",
      "Loss: 1.353360652923584\n",
      "\n",
      "> Iteration 2904/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6284]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4059]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1496]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9409]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3920]]), B=tensor([[0.2632]])\n",
      "Loss: 1.374274492263794\n",
      "\n",
      "> Iteration 2905/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6284]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4059]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1496]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9409]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3919]]), B=tensor([[0.2631]])\n",
      "Loss: 1.3472903966903687\n",
      "\n",
      "> Iteration 2906/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6284]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4059]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1496]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9409]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3919]]), B=tensor([[0.2631]])\n",
      "Loss: 1.383673071861267\n",
      "\n",
      "> Iteration 2907/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6283]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4058]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1495]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9410]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3918]]), B=tensor([[0.2631]])\n",
      "Loss: 1.392671823501587\n",
      "\n",
      "> Iteration 2908/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6283]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4058]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1495]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9410]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3917]]), B=tensor([[0.2630]])\n",
      "Loss: 1.4137669801712036\n",
      "\n",
      "> Iteration 2909/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6283]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4057]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1494]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9410]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3917]]), B=tensor([[0.2630]])\n",
      "Loss: 1.3871557712554932\n",
      "\n",
      "> Iteration 2910/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6282]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4057]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1493]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9411]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3916]]), B=tensor([[0.2629]])\n",
      "Loss: 1.3884475231170654\n",
      "\n",
      "> Iteration 2911/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6282]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4057]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1493]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9411]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3916]]), B=tensor([[0.2629]])\n",
      "Loss: 1.379109263420105\n",
      "\n",
      "> Iteration 2912/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1368]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6282]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4056]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1492]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9411]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3916]]), B=tensor([[0.2629]])\n",
      "Loss: 1.3677964210510254\n",
      "\n",
      "> Iteration 2913/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1367]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6281]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4056]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1492]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9411]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3915]]), B=tensor([[0.2628]])\n",
      "Loss: 1.3872205018997192\n",
      "\n",
      "> Iteration 2914/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1366]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6281]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4055]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1491]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9412]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3913]]), B=tensor([[0.2628]])\n",
      "Loss: 1.3842841386795044\n",
      "\n",
      "> Iteration 2915/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1365]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6281]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4055]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1491]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9412]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3911]]), B=tensor([[0.2627]])\n",
      "Loss: 1.3833277225494385\n",
      "\n",
      "> Iteration 2916/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1364]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6280]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4055]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1490]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9412]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3911]]), B=tensor([[0.2627]])\n",
      "Loss: 1.3730789422988892\n",
      "\n",
      "> Iteration 2917/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1364]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6280]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4055]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1491]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9412]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3911]]), B=tensor([[0.2627]])\n",
      "Loss: 1.3338552713394165\n",
      "\n",
      "> Iteration 2918/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1365]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6281]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4055]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1491]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0285]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9412]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3912]]), B=tensor([[0.2627]])\n",
      "Loss: 1.3463683128356934\n",
      "\n",
      "> Iteration 2919/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1365]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6281]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4056]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1492]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9411]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3912]]), B=tensor([[0.2628]])\n",
      "Loss: 1.351633071899414\n",
      "\n",
      "> Iteration 2920/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1365]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6282]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4057]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1493]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9411]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3914]]), B=tensor([[0.2629]])\n",
      "Loss: 1.379111647605896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2921/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1366]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6283]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4057]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1494]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9410]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3915]]), B=tensor([[0.2630]])\n",
      "Loss: 1.366471290588379\n",
      "\n",
      "> Iteration 2922/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1367]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6283]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4058]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1495]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9410]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3917]]), B=tensor([[0.2631]])\n",
      "Loss: 1.3910785913467407\n",
      "\n",
      "> Iteration 2923/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1367]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6284]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4059]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1496]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9409]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3918]]), B=tensor([[0.2631]])\n",
      "Loss: 1.362895131111145\n",
      "\n",
      "> Iteration 2924/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6285]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4060]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1498]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9408]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3920]]), B=tensor([[0.2633]])\n",
      "Loss: 1.368709921836853\n",
      "\n",
      "> Iteration 2925/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1370]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6286]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4061]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1499]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9408]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3923]]), B=tensor([[0.2634]])\n",
      "Loss: 1.371721625328064\n",
      "\n",
      "> Iteration 2926/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6286]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4062]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1500]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9407]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3925]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3685123920440674\n",
      "\n",
      "> Iteration 2927/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6287]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4063]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1501]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9407]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3928]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3596253395080566\n",
      "\n",
      "> Iteration 2928/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6288]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4063]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1502]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0284]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9406]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3931]]), B=tensor([[0.2637]])\n",
      "Loss: 1.360001564025879\n",
      "\n",
      "> Iteration 2929/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1377]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4064]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1503]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9406]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3933]]), B=tensor([[0.2638]])\n",
      "Loss: 1.363144040107727\n",
      "\n",
      "> Iteration 2930/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1504]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9405]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3934]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3890451192855835\n",
      "\n",
      "> Iteration 2931/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1504]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9405]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3936]]), B=tensor([[0.2639]])\n",
      "Loss: 1.39193856716156\n",
      "\n",
      "> Iteration 2932/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1505]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3937]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3296043872833252\n",
      "\n",
      "> Iteration 2933/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1380]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1506]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3938]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3778494596481323\n",
      "\n",
      "> Iteration 2934/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1506]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3937]]), B=tensor([[0.2640]])\n",
      "Loss: 1.4223934412002563\n",
      "\n",
      "> Iteration 2935/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1506]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0283]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3937]]), B=tensor([[0.2640]])\n",
      "Loss: 1.4259331226348877\n",
      "\n",
      "> Iteration 2936/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1506]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3937]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3761372566223145\n",
      "\n",
      "> Iteration 2937/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1506]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3937]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3730496168136597\n",
      "\n",
      "> Iteration 2938/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1507]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3937]]), B=tensor([[0.2641]])\n",
      "Loss: 1.352406620979309\n",
      "\n",
      "> Iteration 2939/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1508]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9403]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3938]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3848167657852173\n",
      "\n",
      "> Iteration 2940/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1380]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1508]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9403]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3939]]), B=tensor([[0.2642]])\n",
      "Loss: 1.4209650754928589\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2941/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1380]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1509]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9403]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3940]]), B=tensor([[0.2642]])\n",
      "Loss: 1.3839361667633057\n",
      "\n",
      "> Iteration 2942/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1509]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3941]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3652597665786743\n",
      "\n",
      "> Iteration 2943/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1510]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0282]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3942]]), B=tensor([[0.2644]])\n",
      "Loss: 1.395982027053833\n",
      "\n",
      "> Iteration 2944/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1511]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3943]]), B=tensor([[0.2644]])\n",
      "Loss: 1.400335431098938\n",
      "\n",
      "> Iteration 2945/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1512]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3943]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3649332523345947\n",
      "\n",
      "> Iteration 2946/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1382]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0281]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3945]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3503795862197876\n",
      "\n",
      "> Iteration 2947/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3945]]), B=tensor([[0.2646]])\n",
      "Loss: 1.388744592666626\n",
      "\n",
      "> Iteration 2948/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1382]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3946]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3716654777526855\n",
      "\n",
      "> Iteration 2949/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1382]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3946]]), B=tensor([[0.2647]])\n",
      "Loss: 1.3969686031341553\n",
      "\n",
      "> Iteration 2950/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1382]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0280]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3947]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3769896030426025\n",
      "\n",
      "> Iteration 2951/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1381]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3946]]), B=tensor([[0.2648]])\n",
      "Loss: 1.4282283782958984\n",
      "\n",
      "> Iteration 2952/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1380]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3945]]), B=tensor([[0.2648]])\n",
      "Loss: 1.387769341468811\n",
      "\n",
      "> Iteration 2953/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1379]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0279]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3944]]), B=tensor([[0.2648]])\n",
      "Loss: 1.3985357284545898\n",
      "\n",
      "> Iteration 2954/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1378]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3944]]), B=tensor([[0.2649]])\n",
      "Loss: 1.3759489059448242\n",
      "\n",
      "> Iteration 2955/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1377]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3943]]), B=tensor([[0.2649]])\n",
      "Loss: 1.415642261505127\n",
      "\n",
      "> Iteration 2956/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1376]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3942]]), B=tensor([[0.2649]])\n",
      "Loss: 1.359894871711731\n",
      "\n",
      "> Iteration 2957/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1374]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0278]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3940]]), B=tensor([[0.2648]])\n",
      "Loss: 1.4398196935653687\n",
      "\n",
      "> Iteration 2958/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1372]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6297]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4074]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1518]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9398]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3937]]), B=tensor([[0.2648]])\n",
      "Loss: 1.359831690788269\n",
      "\n",
      "> Iteration 2959/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1371]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4073]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1517]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3935]]), B=tensor([[0.2647]])\n",
      "Loss: 1.4421619176864624\n",
      "\n",
      "> Iteration 2960/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1369]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6296]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3933]]), B=tensor([[0.2646]])\n",
      "Loss: 1.349191665649414\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2961/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1366]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4072]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0277]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3930]]), B=tensor([[0.2646]])\n",
      "Loss: 1.3470569849014282\n",
      "\n",
      "> Iteration 2962/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1364]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1516]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0276]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9399]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3927]]), B=tensor([[0.2645]])\n",
      "Loss: 1.3642191886901855\n",
      "\n",
      "> Iteration 2963/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1362]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6295]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0276]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3925]]), B=tensor([[0.2644]])\n",
      "Loss: 1.3729008436203003\n",
      "\n",
      "> Iteration 2964/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1360]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4071]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0276]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3922]]), B=tensor([[0.2644]])\n",
      "Loss: 1.397123098373413\n",
      "\n",
      "> Iteration 2965/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1358]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3920]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3497525453567505\n",
      "\n",
      "> Iteration 2966/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1356]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3918]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3886823654174805\n",
      "\n",
      "> Iteration 2967/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1355]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3916]]), B=tensor([[0.2643]])\n",
      "Loss: 1.3752243518829346\n",
      "\n",
      "> Iteration 2968/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1353]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3914]]), B=tensor([[0.2643]])\n",
      "Loss: 1.385986089706421\n",
      "\n",
      "> Iteration 2969/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1351]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6294]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4070]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3912]]), B=tensor([[0.2642]])\n",
      "Loss: 1.4017393589019775\n",
      "\n",
      "> Iteration 2970/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1349]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1515]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3910]]), B=tensor([[0.2642]])\n",
      "Loss: 1.3622819185256958\n",
      "\n",
      "> Iteration 2971/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1349]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3910]]), B=tensor([[0.2642]])\n",
      "Loss: 1.3912278413772583\n",
      "\n",
      "> Iteration 2972/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1350]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3910]]), B=tensor([[0.2642]])\n",
      "Loss: 1.36089289188385\n",
      "\n",
      "> Iteration 2973/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1350]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4069]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3910]]), B=tensor([[0.2641]])\n",
      "Loss: 1.393008828163147\n",
      "\n",
      "> Iteration 2974/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1350]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3910]]), B=tensor([[0.2641]])\n",
      "Loss: 1.3985453844070435\n",
      "\n",
      "> Iteration 2975/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1349]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3909]]), B=tensor([[0.2641]])\n",
      "Loss: 1.4087331295013428\n",
      "\n",
      "> Iteration 2976/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1347]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3907]]), B=tensor([[0.2640]])\n",
      "Loss: 1.40993070602417\n",
      "\n",
      "> Iteration 2977/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1346]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3905]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3862406015396118\n",
      "\n",
      "> Iteration 2978/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1345]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3904]]), B=tensor([[0.2640]])\n",
      "Loss: 1.356091022491455\n",
      "\n",
      "> Iteration 2979/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3903]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3724538087844849\n",
      "\n",
      "> Iteration 2980/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3903]]), B=tensor([[0.2640]])\n",
      "Loss: 1.366023302078247\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 2981/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1342]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0272]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3902]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3892470598220825\n",
      "\n",
      "> Iteration 2982/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1342]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6293]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0272]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9400]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3902]]), B=tensor([[0.2641]])\n",
      "Loss: 1.4085646867752075\n",
      "\n",
      "> Iteration 2983/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1342]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0272]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3903]]), B=tensor([[0.2641]])\n",
      "Loss: 1.4055153131484985\n",
      "\n",
      "> Iteration 2984/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1514]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3903]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3721457719802856\n",
      "\n",
      "> Iteration 2985/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3903]]), B=tensor([[0.2640]])\n",
      "Loss: 1.4169983863830566\n",
      "\n",
      "> Iteration 2986/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6292]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4068]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1513]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3903]]), B=tensor([[0.2640]])\n",
      "Loss: 1.3756372928619385\n",
      "\n",
      "> Iteration 2987/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1512]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9401]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3902]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3671491146087646\n",
      "\n",
      "> Iteration 2988/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1512]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3901]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3984309434890747\n",
      "\n",
      "> Iteration 2989/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1342]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1511]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3901]]), B=tensor([[0.2639]])\n",
      "Loss: 1.3628675937652588\n",
      "\n",
      "> Iteration 2990/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1341]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4067]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1511]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3816699981689453\n",
      "\n",
      "> Iteration 2991/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1341]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1511]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3899]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3657476902008057\n",
      "\n",
      "> Iteration 2992/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1341]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6291]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1511]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0273]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3899]]), B=tensor([[0.2638]])\n",
      "Loss: 1.4034888744354248\n",
      "\n",
      "> Iteration 2993/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1342]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1511]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3558974266052246\n",
      "\n",
      "> Iteration 2994/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1343]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1510]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9402]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3901]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3948476314544678\n",
      "\n",
      "> Iteration 2995/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1344]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6290]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4066]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1510]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0274]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9403]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3902]]), B=tensor([[0.2638]])\n",
      "Loss: 1.3943428993225098\n",
      "\n",
      "> Iteration 2996/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1345]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1509]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9403]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3901]]), B=tensor([[0.2637]])\n",
      "Loss: 1.3899891376495361\n",
      "\n",
      "> Iteration 2997/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1344]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4065]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1508]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3901]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3916493654251099\n",
      "\n",
      "> Iteration 2998/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1344]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6289]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4064]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1507]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3900]]), B=tensor([[0.2636]])\n",
      "Loss: 1.3560184240341187\n",
      "\n",
      "> Iteration 2999/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1344]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6288]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4064]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1507]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3899]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3801302909851074\n",
      "\n",
      "> Iteration 3000/3000\n",
      "Parameters:\n",
      "\t> a     : tensor([[0.1344]], requires_grad=True)\n",
      "\t> b     : tensor([[0.6288]], requires_grad=True)\n",
      "\t> c     : tensor([[0.4063]], requires_grad=True)\n",
      "\t> eta   : tensor([[0.1506]], grad_fn=<CopySlices>)\n",
      "\t> alpha : tensor([[-0.0275]], grad_fn=<SubBackward0>)\n",
      "\t> beta  : tensor([[0.9404]], grad_fn=<RsubBackward1>)\n",
      "A=tensor([[0.3899]]), B=tensor([[0.2635]])\n",
      "Loss: 1.3429441452026367\n",
      "\n",
      "--- Update learning rate: [0.001] ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_visitor = LossTrainingVisitor()\n",
    "AB_visitor   = ABTrainingVisitor()\n",
    "\n",
    "optimizer        = torch.optim.Adam(gum.theta, lr=.001)\n",
    "\n",
    "def _lr_(epoch, decays=[]):\n",
    "    if epoch < len(decays):\n",
    "        return decays[epoch]\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "scheduler        = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda e: _lr_(e, decays=[.5, .2]))\n",
    "\n",
    "gum = parameters_estimation(\n",
    "    gum, \n",
    "    unknown_gum, \n",
    "    n_observations = 1000, \n",
    "    n_epochs       = 3, \n",
    "    size_batch     = 1000,     \n",
    "    optimizer      = optimizer,\n",
    "    scheduler      = scheduler,\n",
    "    visitor        = AggregateTrainingVisitor([\n",
    "        AB_visitor,\n",
    "        loss_visitor, \n",
    "        TalkativeTrainingVisitor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7413cfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T21:17:00.680147Z",
     "start_time": "2023-03-10T21:17:00.443797Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.patch.set_facecolor('xkcd:white')\n",
    "\n",
    "plt.plot(loss_visitor.losses)\n",
    "plt.xlabel(\"Iterations\", fontsize=20)\n",
    "plt.ylabel(\"log-likelihood\", fontsize=20)\n",
    "plt.title(\"Loss curve\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3354dbbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T21:17:00.898617Z",
     "start_time": "2023-03-10T21:17:00.681629Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2sAAAOaCAYAAAAGe8osAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3hT1eMG8DfdewIt0NKWMkppy96rgLIRB1sQBFScICo48AuCDAEVceAPRZYyVBAXAgItIHuUssoutEChzO6V9Pz+qL0mbcZNmzRp+36eh4cmOTn35ObmJu8995yrEEIIEBERERERkVWxsXQDiIiIiIiIqDSGNSIiIiIiIivEsEZERERERGSFGNaIiIiIiIisEMMaERERERGRFWJYIyIiIiIiskIMa0RERERERFaIYY2IiIiIiMgKMawRERERERFZIYY1IiKyKmPHjoVCoYBCocDMmTMt3ZxKY+XKldJ6i46OtnRzqryrV69K61uhUJi8fvW6r169avL6qfLjZ756YFgji1H/Qabtn6OjI/z8/NC2bVu8+uqr2Ldvn6WbTERERERUYRjWyGrl5+cjNTUVR44cwRdffIHOnTujR48euH79uqWbRibGo4NVE9/XsjF3jw1VLPYUU2XCHl3rY2fpBhABgLe3N9q2batxX25uLq5evYpr165J98XExKBDhw44dOgQ6tSpU9HNJCIiIiKqMAxrZBWioqKwdetWrY8dP34cL730Eg4dOgQAuH79Op577jn8+eefFdlEIqogK1euxMqVKy3djEpn7NixGDt2rKWbUW0EBwdDCGHpZhBRFcfTIMnqtWzZEjExMYiKipLu27JlC+Lj4y3YKiIiIiIi82JYo0rB2dkZc+fO1bjvr7/+slBriIiIiIjMj2GNKo2ePXvC3t5eun3q1KlSZRISEvDpp5/iqaeeQlhYGDw8PGBvb48aNWqgefPmeOWVV3DgwAFZy9M1yP/q1auYOXMm2rRpA39/f9ja2mqdBKCgoADbtm3D1KlT0aNHD9StWxfOzs5wdnZGQEAAHnnkEcydOxd37tyR1Z6ZM2dKbVE/1Wnbtm0YOnQoQkND4eTkBG9vb/Ts2RM//fST1nrOnDmDF154AY0aNYKTkxM8PT3RunVrLFiwAHl5ebLaoi42NhavvPIKIiMjUaNGDTg6OqJu3bp45JFHsHjxYmRmZup8bvHA+2effVa6b/fu3TpnCI2NjTVbW4rpmhTj4MGDePHFF9G0aVP4+PjonDQjIyMDS5cuRf/+/REYGAgXFxfY29vD09MTjRs3xoABAzB79mycOHHCYFuMJYTA77//jvHjxyMsLAze3t5wcnJCvXr1MHDgQHz77bfIz8+XXd/hw4fxyiuvoGXLlvDx8YGdnR2cnZ3h7++P9u3b4/nnn8cPP/xQar2W932VMyGDvs/nu+++i2bNmkmvv2nTppg+fTrS0tJK1ZOZmYlFixahY8eO8PT0hKOjI4KDgzFhwgRcvHhR9royxb6neNsLCQnRuF/Xeiu5bsoyocuNGzfw4YcfolOnTqhduzYcHR1Rq1YttGrVCu+88w4SEhJk1aPrPdu6dSsGDx6M+vXrw8nJCTVq1ECXLl2wePHiMu1vtDl16pS0bC8vL6hUKp1lp0+frrEOv/zyS51l09LSYGdnB4VCARsbGzx48EDjcUMTwQQHB0OhUGDVqlXSfR988IHO91OOtLQ0fPbZZ+jQoQP8/Pykz/fw4cOxa9cuWXUYQ9tkE7dv38bChQvRqVMn1K1bV1pHuiajuHHjBubPn4/o6GgEBATAyckJPj4+iIqKwpQpU7R+l+sjhMCPP/6IgQMHSvUFBASgR48eWL58OXJycgAY93l4+PAh1q9fjxdeeAHt27dHzZo14eDgAHd3d4SEhGDw4MH47rvvZO8/o6OjpWUXn9Kdm5uL1atXo3///qhfvz6cnZ01Hi/p999/x5NPPonAwECdr9FUYmNjtW6HISEhWrdVfadbCyHw66+/4plnnkHDhg3h4eEBV1dX1K9fH0899RRWr16NgoICk7a/yhNEFjJmzBgBQAAQ3bp1k/Wc2rVrS8/p3bu3xmMtW7aUHjP078knnxTp6el6l5WYmKjxHCGE+Pbbb4WLi4vWOtXFxMQIX19fWW1xdXUVy5YtM/jaZ8yYIT1nzJgxIisrS4waNUpv3c8//7xGHXPnzhW2trY6y0dFRYm7d+/KeSvEtWvXRK9evQy+Pn9/f/Hnn39qrUN9G5DzLyYmxmxtKbZixQqN7TIvL0+88sorWusrud3u3LlTYxs19O/QoUOy1rUc8fHxok2bNgaXGRoaanC5ubm54plnnpH9Ovr27avx/PK+r+rPnzFjhtY2avt8rl27Vri6uupcTkhIiLh+/bpUx5EjR0RgYKDO8k5OTuK3334zuO5Nte9R3/bk/Cu5bkpuu4YsWLBA5/6s+J+dnZ2YPHmyKCgo0FtXyfcsLS1NDBkyRG/dYWFhIjk52WA7DSksLBQ1atSQ6j1y5IjOsp06ddJow+DBg3WW/f3336VyzZo1K/W4tm1QXVBQkFHvZ0nqjyUmJoqDBw+KevXq6a3j1VdfFYWFhfJWnAwl2/D777/r/G5LTEzUeK5KpRIzZ84Uzs7OettsY2MjXnvtNaFUKg22JzU1VXTr1k1vfVFRUeLSpUuyPw8LFy4Ujo6Ost6jwMBAceDAAYPtVG/jihUrxLlz50RUVJTWOlesWKHx3PT0dPHYY4+Z9DUaEhMTY9S2OmbMGK31XLp0SXTo0MHg8xs3bixrPVIRTjBClYr6US0HBweNx9THsNnb26Nhw4aoUaMGbG1tkZqainPnzklHXDdt2oSUlBTs2bMHdnbyPgY//vgjJkyYAACwtbVFREQEvL29cevWLZw/f16j7PXr13Hv3j3ptre3N0JDQ+Hh4YH8/HxcvnwZKSkpAICsrCw8//zzyM/Px8svvyx7XYwbNw4bNmwAAAQEBCA0NBTZ2dk4ceKEdNRq2bJlCAoKwrvvvos5c+Zg+vTpAABPT0+Eh4fDzs4Op06dwsOHDwEAJ0+exMiRI7Ft2za9yz5z5gx69eqFmzdvSve5urqiadOmcHNzw40bN6R1cuvWLQwaNAjr1q3D4MGDNeqJjIxE7969cePGDZw+fVpaVyVnBi3m4+NjtrboMmnSJHz99dcAADc3N4SHh8PJyQmJiYka5U6ePIkBAwZoHPGsWbMmGjZsCBcXF2RmZiI5ORk3btyQHi8sLJTVBkN2796Nxx57DOnp6dJ9Xl5eCAsLg5OTE65duya19/Lly+jRowe2bduGTp06aa1v3LhxWLt2rXTbzs4OjRs3Rq1atSCEwP3793HhwgXk5uZqfR2meF+NtWXLFjz99NMQQsDFxQWRkZFwcnJCQkICUlNTAQCJiYno1asX4uPjceHCBfTs2RPp6emwsbFB06ZNUaNGDSQnJ+PSpUsAio6EDx06FGfOnEH9+vV1LttU+566deuid+/eyMnJwZ49e6T7e/furXW5DRo0KNvKAjB58mR89tlnpeoLCAjA3bt3cebMGQghoFQqsXjxYly+fBmbNm2Stb9UqVR48sknsXPnTgBA7dq10aBBA6hUKsTHxyMrKwsAcO7cOQwcOBBHjhyRvR/WRqFQoGvXrti0aROAol6C1q1blyqXk5ODI0eOaNy3e/dunfWq9/iW5dIT3bp1w+3bt3Hq1Clp/xQaGlqm9+306dMYMWIEMjMzYWNjg/DwcNSsWRN37tyR3isA+PzzzxEUFIQ33njD6GUYsm/fPowdOxZKpRIKhQJNmjSBn58f7t69i7Nnz2qULSgowMiRI/Hzzz9L9ykUCoSFhcHf3x9ZWVk4deoUcnJyUFhYiCVLliA5ORkbN27U2cuYnp6ORx99VOPzZmdnh8jISHh6eiIpKQlXrlzByZMn0atXL0yaNEnW6zp37pxGL29gYCDq1q0LV1dXZGZm4ty5c1KvfHJyMrp3745//vkHrVq1klX/vXv38OijjyI5ORkAUK9ePQQHByM7Oxvnzp3TKJuXl4eBAwdqbJfFvze8vLyQmJiIpKQko1+jIT4+PtJ+Rv37v2vXrnB2di5VPjIystR9CQkJ6NGjB27duiXdV/yd6eDggISEBOl30fnz5/HII4/gt99+Q48ePUzyGqo0y2ZFqs6M7Vm7c+eOUCgU0nPGjx+v8bivr6+YNGmS2LNnj8jPzy/1/Pv374sPP/xQ4wja3LlzdS6v5FFTd3d3AUC8/vrrpXqfLl26pHF7zZo1IioqSnz22Wfi8uXLWus/ceKE6Nu3r1S/k5OTuHLlis72qPes+fj4CACiQYMGYufOnRrlbt26JXr06CGV9fDwEH/99ZewsbGRevHU109ubq6YPHmyxmvdvn27znakp6eL+vXrS2Xr1q0r1q9fX+rI+4ULF0Tv3r012lHyyGuxsh4dNHdbit9zd3d38fXXX4vc3FyNsurv7YABA6TnNWnSRPzzzz9a6799+7ZYvny5aNeunUmOLCYnJ0vbAwARHh4u/vrrL6FSqTTKHTt2TKPnLTAwUDx48KBUfUePHtXYFqZPn661nFKpFPv27ROvvfaaGDJkiNa2lfV9LUvPmo+Pj7CzsxPz5s0T2dnZUjmVSiUWLVqkUXbZsmXSUe6RI0eKGzduaNT9999/Cw8PD6n8yJEj9bbX3PseueSu73Xr1mnU36ZNG3HixAmNMlevXtXYpvW9F0JovmfFPS/h4eGlek2zs7PFpEmTNOr99ttvZb9GXT777DOpvv79+2sts2PHDqmMem/P6dOntZZv1aqVVGbTpk2lHpf7PsnZnrVRr7t4nY4fP17cvHlTo1xCQoKIjIyUyrq6uoq0tDTZy5HbhuL94dNPP12qR/TmzZsan7s333xTep6NjY148803xa1btzSek52dLebNmyfs7Oyksh9//LHOtjz//PMa7Rk/fry4c+eORpnDhw+LZs2aCQAava36Pg/PPfec6Nu3r/j++++1nlmiUqnEn3/+KRo3bizVFxYWVmofq069Z614vbVu3brUWQ1ZWVkiJSVFuj19+nSN1zhkyJBS+6fY2FgRGhpq1Gs0hvrydX1PlpSXl6fRc+jg4CA++ugjkZWVJZUpKCgQq1atEp6enlK5mjVritTUVJO0uypjWCOLMTasffnll6V+cKnLzMyUtdzNmzdLddSuXVvrjyshSn8RAxDz5s2TtQy5bVGpVOLJJ5+U6n/jjTd0llUPa8XBpOSXX7EHDx5o/IB3cHAQtra2IjY2Vmf90dHRUvnRo0frLKd+SmBoaKjONghR9IO+X79+UvmxY8dqLVfWH/XmbgtQdBrY3r179bajoKBAODg4CABCoVCICxcuyGq/nNN+DFH/Qd2uXTu9215WVpZo0aKFVH7mzJmlysyaNUt6fNSoUbLaoOt1VGRYAyDWrFmjs85nn31W4/MAQEyYMEFnefW2Ozs7i4yMDJ1lzb3vkUvO+s7NzRW1atWSyrVs2VJn+1UqlcbpWHZ2duLatWtay5Y89bVJkyZaQ36xgQMHSmW7dOki+zXqEh8fL9Xn6empdZt8//33pTLqB6i++OKLUmXT0tKkU8YVCoXWH/EVGdYAiLfffltn2aSkJI1TWpcvXy57Oca0YeLEiQafc/jwYenAqkKhEBs2bNBbXv3ggYeHh9bThBMSEjQO1r744os667t//75o1KiRRrv17X/kfn7v3buncXDw999/11m25KmarVq1Mric5ORkad9UHNR0ndKanJws/P39Zb9GY5QlrH366acaz1u7dq3Osvv27dN4nSWHa1BpDGtkMcaEtfPnz2uED0dHx1JHF43RpUsXqa59+/ZpLVPyi7hZs2Z6j6SV1eXLl6VlNGzYUGe5kmFt/fr1euudOHGiUV+ya9eulcrWr19fa5l79+5p/CAwFGKEKPpSsbe3l963hw8flipTlh/1FdEWoGgMiCE3b96Uyvv5+clqvymcOXNG+gHj4OCgsxdX3aFDh6S2+vv7l/oxoH70eunSpeVqX0WGtT59+uit8+DBgxrlfX199f54ysvLk46IAxB79uyR3X59yrLvkUvO+v7++++lMgqFQsTFxemt89atWxrr4d1339VarmRYM7S+du7cqbE/NzQmzpDCwkKNsVTaxq0Vr3sXFxdx48YN6bOjrWf4jz/+kOqKiorSusyKDGuNGjUyuI5Gjx4tlR83bpzs5chtg5+fn0ZPiS5Dhw6VnqPvgIg69bNMtO133njjDY39lqHg89dff5klyCxfvlyq87nnntNZrmRYO3bsmMG6Z8+erRFaS/YalrRq1SqrCGuFhYWiYcOG0nMef/xxg8+ZOnWqVN7FxUXvgR0SgrNBktXKy8vD+fPnMX/+fLRr1w7379+XHnv11VdRu3btMtfdrl076e+SYxh0GT9+PGxsTP+RqV+/PmrUqAEAuHTpkjR+TB9PT0889dRTesu0adNG4/b48eP1lldfJ4mJiVpnatu4cSOys7MBAK1atULnzp0NtjUgIADdunUDUPSe7t+/3+Bz5Kiotjz33HMGyzg5OUl/p6amSuMRze2HH36QxqoMHDhQ77iqYm3btpXGzNy6davUmAn111KZrmVoaPtu0aKFxrio4cOHw9XVVWd5BwcHNG/eXLpdcj2VVVn2Pab066+/Sn937dpV4zVq4+fnhxEjRmh9vi5hYWHo0qWL3jIdOnSQ9qd5eXmlxoAaS6FQaCyz5Ayjubm5OHz4MACgY8eOqFOnDsLDwwFoH7em/vzifYYljRs3zuC4PvV9oKm2V3VPP/00XFxc9JbJzs7GL7/8It2WO6bq6aeflv6OiYkp9fiWLVukv0eNGqX3swsUjfMMDg6WtWxjlOXz27JlS7Rs2dJgOfXP1tChQ6XfBbqMGDECvr6+stpgTgkJCRoz58p5z1977TXp85+dnY0dO3aYrX1VAScYIatQPK23HH379sW8efN0Pl5QUIBdu3bhyJEjuHTpEtLT05GTkyP9qAUgTSAAQGPCB33khAFtbt++jW3btiE+Ph4pKSnIyMgoNW1t8dTnQgjcvHkTXl5eeuts3bq1wS9uf39/6W9HR0e0aNFCdnkhBNLS0lCrVi2NMnv37pX+7t69u9761EVEREg74+PHj6Nv376yn6tLRbTF09MTERERBuv09vZG/fr1ceXKFQgh8Nhjj+Hbb79Fs2bNZLerLMqzDoo/A8ePH0eTJk2kx9QnZli2bBkaNmyIF198Uesgc2vSvn17vY87ODjA29tbulSGofKA5mei5LTt2phr32NKhw4dkv6W+zkcMGAAli1bBqDoh1lGRgbc3d11lu/QoYPBOp2dneHr6yu9H3IOUhkSHR2NzZs3AygKW2+++ab02IEDB6QDUMWThURHR+PMmTNITU3F2bNnpfBW/Hz1ei1NzjoNCAiQ/jbF+ixJznfgwYMHpe+3GjVqyNp/AtAod/z4cY3HMjIyNCbxkrOvK56uX9e0+LqcOXMGu3fvxunTp3Hv3j1kZmZqXApCfQIpU/52yM/Px8mTJ6XbuiYVUmdvb4+ePXvixx9/lNUOc1Hfp7i6uqJr164Gn1O3bl00b95ceq8PHToke9Kv6ohhjSqNGjVq4K233sKbb76ptYdLpVLhs88+w7x583D37l3Z9Wq79pI2cnot1N24cQNTpkzBxo0b9V73pyztKRmitFE/Aurj4wNbW1vZ5QFovY7LmTNnpL//+OMP2dfHUf+Basx7o09FtKX4GjNyvP7663j11VcBAEePHkXz5s3RvHlz9OnTB126dEGHDh3g7e0tqy651NfB8uXL8fvvv8t6nvq6KrkOBg8ejPfeew/JyckoLCzEG2+8gQ8++AB9+vRBjx490LFjR0RERMheLxXF2M+EseX1XdfI3PseU1EqlUhKSpJua5vRTZuoqCjp78LCQiQmJmrcV5J6yNVHff0W95KXh3oP2N69e6FSqaT9nnrvmXpYK77OWmxsrBTW0tPTERcXB+C/mSYtTc46NfX6LEnOd6D6PikvLw99+vSRVbf656vkZ+j69esaM86GhYXJqlNuOaAoZE6aNEnqfZXDlL8dkpKSNGa7lhtymzZtKqucOV2+fFn6Ozw8XPYZSFFRUVJYU6+DSmNYI6ugbVpvR0dHeHp6Ijg4GO3bt0fPnj3h6Oio9flKpRJDhgyRjqoaQ+6FWfUdSS7p7NmziI6Oln3Ba2PbU/KyBaYuD0CjN6CY+uUIzp07V6ZTbUz1A7Ui2mLMe/7yyy/jwoUL+Pzzz6X7Tpw4gRMnTmD+/PmwsbFB69atMXz4cIwbNw6enp5Gt1ddYWGhxtHz4h+Xxiq5DpydnfH777+jf//+0pHj9PR0/Pjjj9IR3Bo1aqB///4YP368wdPdKoq5PxPaPg9Axex7TKVkb4uh06x0lTPUy2iq/Y2xii+E/uDBAylwFfcUF/eUubi4SN81xRcuFkIgNjYWL730EoD/gh4A6ZIOlmaq7bU85OwP1ffLGRkZBi8Do03JfVLJ24bOPDG23IYNG/D0008bdVAVgOwLZMtZbyU/U3JPb7SG0yDV9yvGfFbUy8o5c6E645g1sgpRUVHYunWrxr9ff/0Vq1evxqxZs9CvXz+dQQ0AFi1apPFjqUOHDli6dCmOHj2K1NRU6VSk4n8zZswwuo1yjxapVCoMHTpUCmqOjo4YP348Nm3aJF2vJT8/X6M9QUFBRrfHEoqvj1Qeprq2WEW0xZgxigqFAkuWLMGePXswaNCgUj+uCgsLcfjwYUyZMgUhISEa1zEri+LrE5WXtjqaNWuGhIQEfPjhh1qPCt+9exerVq1C165dMWDAAJP1llZGFbHvMZWS4VBuACi5763okCmXrnFreXl50qlaHTt2hL29PYCiH4vaxq1p64UjeftDU+yXSwbNsgZPOc+7evUqxo4dKwW1mjVr4t1338WOHTuQmJgonQZZ/Pkty9hKOeutZPAr62fTEtT3B8YcVFBvu7XuU6wFe9ao0lOpVFi0aJF0+5VXXtHo3dAmIyPDbO35/fffpVNB7O3tsWvXLnTs2NFi7TElLy8v6Yf5ypUrMWbMGLalhC5duqBLly7Izs7Gvn37sHfvXsTExODAgQPSD4IHDx7g6aefhpOTE5588skyLcfV1RX29vbS+JDY2FiTToTg7u6O9957D++99x4uXryI3bt3Y/fu3di5c6fGBCp//vkn+vTpg4MHD5brwsaVkbXtewwp2Zsrty3qF1sH5PdYWEJ0dDR+++03AP+NWzt48KB0AfeS4Ut93FpCQgKaNGlidZOLVCbq20Z0dLTWyULKUydQ1JPj4+Nj8HlyzuJYvHixtG3Ur18f+/fvh5+fn87y5vr8enh4lFqOnLMvrOG3g3o7jWmP+n7Fmvcp1oA9a1TpHT9+XDr1wsXFBR999JHB59y8edNs7fn777+lv0eOHGkwqGVnZ5tlMLg5qI/zUZ/9yRKsqS3auLi44NFHH8WsWbOwd+9e3L59G/Pnz9cYV/LWW2+Vaxk1a9aU/jbnOmjYsCEmTJiANWvW4MaNG9i9e7fGj9hjx45h3bp1Zlu+tbK2fY8hbm5uGhPFyO0lKDmeRH27szbqYaz4dEZ9k4Wo346NjUVGRobGBBcMa8Yxx345ICBAo3dK7invcsqpf1+///77eoMaYL7Pb8kxtFevXpX1vPLOomoK6vsDY9qjvl+x5n2KNWBYo0pPfcB8eHi4wamFgaKZwSqiPSWnz9fm0KFDJjs10NzUZ9DbuXOnyepV/yKWe8qLudpiLr6+vpg2bZo0oQEAXLlypVwDqy2xDoonXNi6dSsaN24s3b99+/ZSZcvyvlYm5tr3lDxtypTrTn1WWPVZ3PQ5ePCg9Le3t7dZpkQ3lWbNmklH6YvHrRWf1qg+Xq1Yt27dpMlyYmNjNcarhYeHm+RHZFX/HKhT3yfduHFDYxbHsnJ3d9fY15S8LIMu2i7JUJKx39emuvRMSX5+fhqXI5J7WQBzXP5DffIoOdur+mUJrl69itTUVIPPUalUOHr0qNY6qDSGNar01KfBlzNDXWxsLK5du2Y17Vm1apXZ2mJqvXr1kv4+ePAgTpw4YZJ61a+Zo2/WvYpoi7kNGjRI4/bt27fLXJf6Oti8eTNu3bpV5rqM5eTkpDHTm7bXUZb3tTIx176n5DWkTLnu1Md0bdy4UdYkCT/88IP0d+fOna1uJlB1NjY2Gq9x+/btUthUH69WrGbNmhrj1swxZX9V/xyoi4iI0AgdX3/9tUnq7devn/T3999/b3C2y+3bt8vq5THmM1xYWIjvv//eYJ1lpb7dbtiwwWD5q1evyj7gYgxjt9e2bdtKY9WEEFi/fr3B5/z9998aoc5aJqqyVgxrVOmpfzGcPn261PgKdQUFBXj99dcrrD379u3TW/bQoUNm3fmbWq9evTSuRTRx4kSTDAxWn5a6+FpllmpLWRhztLzkAHw5Yy90GTVqlDSjVm5uLl566aVyH7k35vnF1wcEtL+OsryvlYm59j1eXl4aFyc35bTWzz77rPT37du3sXjxYr3lN27cqPGD0NDFx62B+qmLS5YskX5w6gpfxfffvn0bq1evLnV/eal/Dqr6FOUKhULjoshLly41ajp8XSZMmCCFqZSUFLz99ts6y6alpcm+GLcx39effPIJrly5Iqveshg9erT096FDhwxeiuW9994zyz7V2O3V09MTTz31lHR73rx5escLKpVKvPvuu9Lt5s2bs2fNAIY1qvTatm0rjcPIycnBlClTtO7AMjMzMXToULP3wKj/UPjxxx91np52/PhxPPbYY0ZPF2xJCoUCCxculL40Dx06hP79+xvsHcrJycHq1avRs2dPrY9HRkZKk1Pcu3dP4wdTRbelLPbs2YMBAwYgNjZW75enSqXS+JLy9/dHo0aNyrxcV1dXzJo1S7r9yy+/4OmnnzY4yDstLQ1ffPEFhg8fXuqxESNGYP78+QYvOxEXF6dxBFXbtajK8r5WJuba99ja2mpcx+yzzz4z2anSjRs31rj47HvvvYdNmzZpLXvw4EGMGzdOut2sWTMMGDDAJO0wJ/WQpb4/MBTWSpY31fXV1H+Ibtu2DWfPnjVJvdbq5ZdfRmhoKICiWf769esnawr/w4cPY/jw4RrjyIqFhYXh+eefl25//vnneOGFF0rNRHvs2DFER0fj3LlzsqaRV/++njVrls4LXa9atUpvQDSFvn37anzux44dq3OfsXDhwnLPKKyL+vb61VdfyToIOnXqVGlff+vWLTzxxBNaA1t+fj6effZZjUvNTJ8+3QStrtqq19RdVCU5Ozvjueeew5IlSwAUXRw4ISEBEyZMQIMGDZCVlYXDhw/jm2++wfXr1+Hm5oYBAwbI6qovi2HDhuGdd97BrVu3oFQq0bdvX0yYMAF9+vSBt7c3UlJSsGXLFvzwww9QKpXo1asXEhISkJycbJb2mFq/fv0wffp0zJ49G0DRWKn69etj2LBh6N69O+rWrQs7Ozs8ePAA58+fx6FDh7Bt2zZkZWXpHLzt4eGBAQMGSFOgjx07FnPnzkVoaKjGVMAffvihxsVCzdGWshBC4M8//8Sff/6JwMBA9O3bF61bt0ZgYCDc3d2RkZGB06dPY82aNTh58qT0vLffftuoywNo8+KLL+LgwYNSEFq3bh22bt2KkSNHonPnztJR0vv37+Ps2bM4cOAAduzYgfz8fLRr165Ufbdu3cI777yD999/H927d0fnzp0REREBX19fKBQK3LhxA7t27cKaNWukL/HAwECNo8LFyvq+Vhbm3PeMHDlS6pFYvnw5/vzzT0RERGicojR8+HCtgduQL7/8Upr0RqlU4qmnnsITTzyBIUOGoG7durh79y62bNmCVatWQalUAig67XX16tXSRaatWYsWLeDp6anxY1HbeLVixePW1IN2kyZNTLaP6NmzJ2rVqoXU1FRkZWUhKioKLVq0gL+/v8b6LMu1+qyRm5sbNm3ahE6dOiEzMxP37t1Dnz590LVrVwwaNAhNmjSBu7s7MjMzcePGDRw7dgzbtm2Teq3Gjh2rtd4FCxbgwIED0j502bJl+O677xAVFQVPT08kJSVJPUEhISGYPHmy1MOma4r7V199FatXr4YQAtevX0eLFi3wyiuvoEOHDrC3t8fly5exbt066aDrhAkT8O2335pydUlsbW2xbNkydOnSBQUFBbh//z7at2+P8ePHo1evXvDy8kJiYiLWrFmDXbt2QaFQYPDgwfjpp59M2o6RI0dK19TcunUrateujebNm2vMWNmjRw+89tpr0u3mzZvj/fffly5NEhMTg6ZNm2LixIlo06YN7O3tcerUKfzf//0fEhISpOeNGDFCo1eOdBBEFjJmzBgBQAAQ3bp1K1ddmZmZolmzZlJ9uv45OjqKjRs3ihkzZkj3jRkzRmudiYmJGs81xs6dO4WDg4PB9oSHh4vU1FQRFBQk3RcTE6O1TjltVhcTEyOVDwoKktVu9bYlJibqLfvpp58KW1tbg69R/Z+fn5/O+hITE0VAQIDe5+taN6Zsy4oVK4zeLtXXtdx/L7zwglCpVLLqN0SlUok333zT6Da0a9euVF3dunUzqo5atWqJuLg4nW0ry/uqvm+YMWOGznqN/XzK+ZwZ2w5z7HuEECI/P190795db50l22TMtpuQkGDwfSn+5+7ubnB9yVlXJRn7fhijf//+Gq/hkUce0Vu+adOmGuUnTpxocBnGbIO///67cHJy0rueS1J/zND+WIiy7fMNMbYN6k6ePCmCg4ON3i/99ddfOutMTU01uI+KiooSly5dEkuXLpXuGzJkiM46P/zwQ1ntGjp0qLhy5Yqs91y9jStWrDBqvW3YsEHW99n06dPL9H0lx6hRo/QuW9e+64033pD9Pj/11FMiLy/PZG2uyngaJFUJrq6u2LNnD8aMGaPzyG+HDh1w4MCBMl/Xyhg9evRAbGwsIiMjtT7u4uKCiRMn4vDhw5V2ytrJkyfj1KlTGD58uMb4Gm3CwsIwbdo0vbN4BQcHIz4+HvPmzUPXrl1Rq1Yt2RfYNHVbjBUZGYmZM2eidevWBnseWrRogU2bNuHrr78ud69aMRsbGyxcuBAHDx5Ev3799F7vTKFQoHnz5pg9e7bWI7LTpk3DiBEjDG6X7u7umDhxIk6fPo3mzZvrLFee97UyMNe+x97eHn///TdWrlyJAQMGIDAwUGPa/fIKCwtDfHw8XnvttVITmqi3YcSIETh9+nSluzi0vin6TVHeWAMGDEB8fDwmT56Mli1bwsvLq1L0UpZHZGQkTp8+jfnz56Nu3bp6y3p7e2Po0KH4/fff8eijj+osV7NmTcTExGD9+vXo378/ateuDQcHB9SpUwfR0dH45ptvcPDgQYSGhmpMYKHvlMj33nsPy5cvLzV9fjF/f38sXrwYGzZsqJDJdYYOHYp9+/bp3K8GBARg1apV0hkl5rBmzRps2rQJgwcPRv369eHq6irrtS9atAhbtmzR+50QEhKClStX4qeffqpS3wXmpBCiio34pmrv5s2biImJwfXr12FnZ4c6deqgTZs2aNCgQYW3RQiBo0eP4ujRo3jw4AG8vb0RGBiI6OhouLm5VXh7zCU3Nxf79u3DlStXpOtOeXp6on79+oiMjESdOnWqTVsyMzNx4sQJXLp0CXfu3EFeXh7c3NxQt25dtG7dGiEhIWZdPlB0YdJ//vkHSUlJuH//PmxtbeHl5YUGDRogKipK1lgOoOg6SWfPnkVSUhIyMjJgY2MDb29vhIeHo3Xr1iYND1WBNe17jJGbm4s9e/bgypUruH//Pjw8PFCvXj1ER0eXulgvUVmdOXMGcXFxuHPnDrKysuDm5oY6deqgSZMmaNq0qckOXhUbOHAg/vjjDwBFs1K+8MILesvn5uZi7969OHPmDHJyclCrVi00aNAAnTt3tliwjo+Px5EjR3Dnzh34+vqiUaNG6Nq1q8nXlTlcvnwZBw4cwO3bt6FSqVCzZk20bNkSzZo1s3TTKh2GNSIiIiKqMm7cuIGQkBBpav74+HiNyTuIKhPrj+ZEREREVO3J6V8oKCjAuHHjpKDWokULBjWq1BjWiIiIiMjqderUCQsXLtR6/S8hBGJjY9G1a1ds375duv9///tfRTaRyOR4GiQRERERWT1/f3/pWng1a9ZEaGgo3NzckJGRgfPnz+Phw4ca5SdOnIilS5daoKVEpsPrrBERERGR1VOfWOPOnTu4c+eO1nKOjo5455132KtGVQJ71oiIiIjI6qWmpuLXX3/F7t27cfr0aWmmWjs7O2m2xB49euDZZ581eLkAosqCYY2IiIiIiMgKcYIRIiIiIiIiK8SwRkREREREZIUY1oiIiIiIiKwQwxoREREREZEVYlgjk4iNjYVCoYBCoUBwcLClm0NWIjg4WNouYmNjtZax5LbD7Zbk+OCDDwxux5XNzJkzpdc0duxYk9W7cuVKqd7o6Gid5eTsG8iyit8fhUKBq1evWro5VovbsnXp168fFAoFvL29cffuXUs3xyQY1oiIiHS4fv06FixYAADo1auX3gBCRESWNWfOHCgUCjx8+BDvv/++pZtjEgxrVAqPEhERFfnf//6H7Oxs6W8iIrJeLVq0QP/+/QEA33zzDS5cuGDhFpUfwxoREZEWV65cwZo1awAAHTp0QKdOnSzcIiIiMuStt94CAKhUKsyePdvCrSk/hjUiIiItFixYAKVSCQB49dVXLdwa05o5cyaEEBBCYOXKlRW+/KtXr0rL56ml1qn4/RFCcEwvVSpdu3ZFVFQUAGDdunWVfswlwxoREVEJDx48kHrVvL298cQTT1i4RUREJNezzz4LoKh37csvv7Rwa8qHYY2IiKiE77//Xhqr9tRTT8HJycnCLSIiIrlGjBgBG5uimLNixQoUFBRYuEVlx7BGAIpOSSmeVOTatWvS/d27d9eYvlfOlMzqEhMT8fbbbyMqKgpeXl5wc3NDWFgYXn75ZVy6dMnodt6/fx9LlixBnz59EBQUBGdnZ3h6eqJJkyZ48cUXsX//fqPrNETbhCsPHz7E4sWL0bFjR/j7+8PZ2RkhISF45plnsHfvXtl1Z2dnY/PmzZg0aRK6dOkCf39/ODo6wtXVFfXq1cOAAQOwZMkSZGZmyqpv7NixUltnzpwJACgsLMTGjRsxePBgNGzYEG5ubhqPq7t69SqWLl2KESNGIDIyEl5eXrC3t4ePjw/Cw8MxYcIEbNu2TfbrM6fY2Fi88soriIyMRI0aNeDo6Ii6devikUceweLFi2WvM2M9ePAAn3zyCXr27ImAgAA4OjrC19cXUVFRmDRpEg4fPmx0ndeuXcPbb7+NyMhIeHp6wsPDA+Hh4Xj55Zdx8uRJqZycyX90Tcm+fft2PPPMMwgLC4Onp6fOKdtv376NFStWYOzYsWjZsiV8fHxgb28PLy8vNGrUCKNGjcJPP/2EwsJCWa9NW5vT09Px+eefo1OnTtI2HxQUhPHjx2sdDF5YWIgNGzagX79+8PPzg4ODA/z9/TFo0CD8/fffstphrB9++EH6W06vWnR0tPQ65Z5WKGdqdG2faQDYunUrBg8ejPr168PJyQk1atRAly5dsHjxYuTl5Rlcdlmm7v/999/x5JNPIjAwEE5OTggICECPHj2wfPly5OTkyKqjmJxtWddlAA4ePIhnn30WjRo1gqurKzw8PNCpUycsW7YMKpWqVD1JSUl444030LRpU7i4uMDNzQ0RERGYPn060tLSjGo3ABw/fhxTp05Fq1atNLbHzp07Y86cObhz547BOnRdNsRU35nXrl3DzJkz0a1bN/j5+cHR0VHaVzVr1gzDhw/H559/juvXr+usw9ip+5VKJb7//ntpu3R1dYW7uzsaNGiAUaNG4ZdffoEQwmA95l43lel7DgCSk5Mxf/589OzZE/Xq1YOzszOcnZ1Rr1499OnTB/Pnzzf4+hMSEvDpp5/iqaeeQlhYGDw8PGBvb48aNWqgefPmeOWVV3DgwAGj2nXnzh0sXLgQjz76KOrUqQNnZ2fpu6Jp06Z48sknsXDhQly8eFFWfUqlEuvWrcPIkSPRsGFDeHh4wMXFBSEhIRg6dCg2bNgg+3sHAPz8/NCxY0cAwL179/DXX38Z9fqsiiASQiQmJgoAsv9169ZN4/kxMTHSY0FBQUIIIZYvXy6cnZ111uHg4CB++OEH2W386quvhLe3t8G2DRs2TGRmZpps3QQFBUl1x8TEiKNHj2rcp+3fxIkTRUFBgd56f/jhB+Hm5iZrffv4+IjffvvNYFvHjBkjPWfGjBni5s2bIjo6WmudM2bM0HjuE088IRQKhaz2dOnSRdy6dcvodaeNtm1Hn2vXrolevXoZbKO/v7/4888/9dZl7LJXr14tfHx8DC575MiRIiMjw2B9Qgjx3XffCVdXV5112draio8++kgIIW99zpgxQyozZswYkZaWJoYMGaK17jFjxmg8d9KkScLW1lbWNhARESHOnz9v8PWVbHNcXJwIDQ3VWa+Tk5PYuXOn9PzU1FTRtWtXvW159913Za1rua5fv66xn8rOzjb4nG7duknPWbFihazlqL+GxMRErWVKfqb1vZ/F/8LCwkRycrLeZZfcTvRJT08Xjz32mN5lRkVFiUuXLokVK1ZI95X8nlAnZ1suWZdSqRRvvfWW3nb0799f5Ofna9Sh73soMDBQXL58We/rL3b//n0xfPhwg/tJDw8P8d133+mty5zfmZ9++qlwdHSU9Tl2dnbWWY+c7bPYkSNHRJMmTQwur23btgb3G+ZcN5b6niuL/Px88dZbb8l6LxUKhdi4caPWelq2bCnr9QIQTz75pEhPTzfYtnXr1glPT0/Z9d6+fVtvfTExMaJx48YG62nRooW4ePGi7HU4e/Zs6bmjR4+W/TxrYwciAM7OzujduzcAYPfu3cjNzQUAtGnTBj4+PqXKFw/c1GXlypUYP348AMDJyQkRERFwc3PDlStXkJSUBADIz8/H6NGj0aBBA7Rt21ZvfZMnT8Znn32mcV9oaCgCAgKQn5+PM2fOID09HQCwYcMGXLt2DTExMSY/dSk5ORmTJk3CgwcPAAANGzZE3bp1kZqairNnz0rlvv76a2RlZWH16tU667py5YpG70+tWrUQHBwMd3d35OTk4MKFC9IFHe/fv4/HH38cv/zyCx577DFZbc3Ly0P//v0RFxcHoOgoU8OGDaFUKnH+/PlS5U+ePCkd9bS1tUVoaChq1aoFBwcH3Lt3DwkJCcjPzwcA7N27F506dcLx48fh4eEhqz2mcObMGfTq1Qs3b96U7nN1dUXTpk3h5uaGGzduSK/t1q1bGDRoENatW4fBgweXe9mffvoppkyZonFfYGAg6tevj/T0dJw6dUqajGLt2rW4fPkytm/frnf9FH9OhNrR5tq1a6Nhw4bIycnBmTNnkJ2djWnTpsHFxcXoNgshMHLkSPz5558AAB8fHzRu3Bg2NjZaj8SePn1a6pkoPqpdu3ZtODs74+HDh0hISJBODTx9+jQ6dOiAuLg41KtXT1Z7kpOTMWTIENy9exc2NjaIiIiAr68vkpKScPnyZQBAbm4uHnvsMcTFxaFu3bp49NFHER8fDwAICQlBUFAQ0tLSEB8fLx1lnTt3LiIiIjBixAij15E227dvl/5u3bo1nJ2dTVJvealUKjz55JPYuXMngKJtpUGDBlCpVIiPj0dWVhYA4Ny5cxg4cCCOHDkCO7vyfc3n5eVh4MCB2L17t3Sfra0tIiIi4OXlhcTERCQlJeHkyZPo1asXJk2aVK7l6fPuu+9i4cKFAICaNWsiLCwMKpUKcXFxUs/en3/+iZdffhnLli3D999/j3HjxkEIAWdnZ0RGRsLZ2RkJCQlITU0FULRNFm9v9vb2Opd948YN9OrVS2M/7+joiKZNm8LLywt37tzBmTNnUFhYiPT0dIwbNw5paWmYPHmyrNdmqu/Mb775Bq+//rrGfSEhIahXrx7s7OyQlpaGS5cu4eHDhwBgVE+FLnv37kW/fv00vs+8vb3RpEkTFBYW4uzZs9L38+HDh9G5c2fs3LkTkZGRsuo35e+JyvA9BwCZmZkYNGgQdu3apXF/3bp1ERQUBHt7e9y6dQsXL15EYWEhhBDSOi6peP8JAPb29mjYsCFq1KgBW1tbpKam4ty5c9J+f9OmTUhJScGePXt07ju2bduGp59+WmPbqVu3LkJCQuDk5ISMjAwkJiZKnzFA/3a2YcMGPPPMM9J6B4p+r4SGhsLe3h4XL16UvvPj4uLQqVMn7N27F40aNdJZZ7Fu3bpJf2/fvh1CCCgUCoPPszoWDIpkpcpylEj9SJirq6twcnISjo6OYtGiRSIrK0uj7J9//qlxRKZz58566/7iiy80jqyMHTtWXLlyRaNMfn6++OabbzR6KF599VWjXrcu6uujRo0aAoBo3bq1OHHihEa5CxculOrFWrlypc56P/zwQ9GpUyexbNkycePGDa1l9u7dK9q1a6exfH1HvdSPwru7uwsAomHDhmL79u2isLBQKpefny+SkpI0nhsZGSnGjx8vtm7dKnJyckrVnZmZKb766iuN9+7555/X2RYhTNuzlp6eLurXry+VrVu3rli/fn2pHswLFy6I3r17axzl1nVUWO6y9+3bJ2xsbKSyDRs2FLGxsRplUlNTxbhx4zTef309FpcvX9Y4Uly3bl3xxx9/aLxPmZmZYtasWcLW1lY4OTlp9MTK6Vkr3gb8/f3Fjz/+KJRKpVROpVKV+hz16dNHDB06VGzatEnrdpaXlyfWrl0r6tSpIy2jV69eOl+jEJrbQHGv5NNPPy1u3rypUW7Hjh0a29YzzzwjXnrpJenzdvToUY3yly5dEpGRkVL5wMBAoVKp9LZFLvXP0WuvvSbrORXRs+br6ysAiPDw8FLvf3Z2tpg0aZJGnd9++63OZcvtWZs+fbpGnUOGDCm1v4qNjZV6S4v3kYBpe9a8vb2FQqEQfn5+YuPGjRrvdVpamhgxYoRU1tbWVvz999/CxcVF2NnZiblz52p8D6lUKrFw4UKN17Vs2TKdbS0oKBAdOnSQynp5eYmlS5eW6nG9efOmGDVqlFTOzs5OHDx4UGud5vjOzM/P1+j5HzRokM5ew3Pnzol58+aJRo0a6XzdcrbP+/fva+wP3N3dxfLlyzV6N3NycsQnn3yi0UMUFham9XvGXOummKW+54ylvj0DEP369RNxcXGlyj18+FCsWrVKtGvXTud+x9fXV0yaNEns2bNH430pdv/+ffHhhx9qvD9z587V2baIiAipXKdOnUR8fLzWcteuXRNLliwR4eHhIiUlRWuZ48ePCwcHB6m+jh07in379pUqt2vXLtGgQQOpXMuWLbW+lpKysrI0vrvPnj1r8DnWiGGNSilvWAOKuuS3bNmis/zGjRs1yl+6dElrueTkZOHk5CSVW7hwod527N27V9jZ2QkAwsbGptSP0bIoecpjZGSkzsCUm5srOnXqJJWtWbOmyM3N1VpW7qmaOTk5on379lKdn3/+uc6y6j/sAIh69erJOo3DmPYcOXJE2NvbC6DolLU7d+7oLGvKsPbKK69I5UJDQ/W+LqVSKfr16yeVHzt2bLmW3bx5c41yur54hBDi5Zdf1ngP9u/fr7Xc8OHDpTKenp7iwoULOuv88ssvNeqUG9aAorAq53RFIeRvA4mJicLLy0taxqlTp3SWLfn5mTBhgs6yq1at0vjBbWNjI5o3b67zlNILFy5In3cAGqdPlof6jxF9P+LVVURYAyCaNGkiHjx4oLPOgQMHSmW7dOmis5ycsJacnKzxQ2rIkCEaBxNKlvX399doqynDGgDh5uYmEhIStJYtKCgQjRo1ksoWt3vNmjU626C+bvWtq0WLFknlfH19xblz53SWFUKIF154QSofHR2ttYw5vjP37NkjPR4SEiLrx6z6AZyS5Gyf6gcI7O3txe7du3XW99NPP2nUqSsQmOv3hBCW+54zxubNmzVej9wDz7r2k3Jfs/pya9eurXX7SUpK0vg86tsXFSssLNR6IK2wsFDjgNvjjz+ud/hIamqqCAwMlMrrOxiuTj3krV69WtZzrA3DGpViirA2btw4veULCws1lqPrAzR16lSpzCOPPCKrLS+++KL0nGnTpsl6jj4lf2xqO+qj7uzZsxrjfowZl6fLzp07pfoeffRRneVK/rDTdQ57eY0ePVrW6zNVWLt3755wcXGRyu3du9dgG5OTk6UvW0dHR/Hw4cMyLfuff/7RWKebN2/Wu9ycnByN1z1y5MhSZe7evavxI/jjjz82+HrUA7sxYU1O3WXx/vvvS8uYM2eOznLq68LX11fvD4e8vDzh4eGh0X5dPRPF1Mcvzpo1q8yvp5hKpZK2GwBix44dsp5XUWFtz549eutU31c4Ojrq/PEjJ6ypj/fw8PDQ+4NVCM2wbY6wNn/+fL3Lnz9/vkb5Pn366C2/f/9+g+uqoKBA1K1bVyr3/fff661TiKKj+cW9oAC0BkxzfGeuXbtWenzYsGEG22mIoe0zKytLowdq8uTJBuscOnSoVD4wMFBrWDTX7wljmfJ7zhgdO3aU6mvdurXeQG1qXbp00ftbR/0z065du3Ita8uWLRrfDdq+o0vasGGD0ctX/454++23y9VmS+FskGQWzz//vN7HFQqFNEsPUDTGQpvvv/9e+lvuWIinn35a+jsmJkbWc+Rq3ry5Rru1adKkCXr06CHd3rx5c7mX265dO+nvI0eOyHpOrVq1MGjQoHIv21TtKY+NGzdKY6VatWqFzp07G3xOQECAdL56Xl5emWcK/fXXX6W/g4ODDY4ZdHJywgsvvCDd/v3330udr79z507p/Hx7e3vpejD6qNcpl52dneyZ/oxVlm1g+PDhcHV11fm4g4MDmjVrJt1u2rSpxnIMtUPXfsQYN2/e1JjiuW7duuWu01TCwsLQpUsXvWU6dOggTVedl5eHxMTEMi9PfdsfOnQoatSoobf8iBEj4OvrW+bl6aNQKDBu3Di9Zdq0aaNxu3icky6tWrWCra0tAN3rKiYmBjdu3ABQtE8dPny4wba6uLhozCAq53vIFN+Z6mO0T506ZZLxaPrExsZqzKb52muvGXyO+hi+5ORkaUy1Pqb6PWGsiv6eA4rWifp31TvvvCNtoxXB0GtW38YuXrwofS+Xhfrvu7Fjx8LT09Pgc5544glp/PbRo0eRkZFh8Dnq+3D12c4rE04wQibn4OCAVq1aGSwXEBAg/V082FndlStXNCaSkHu5gIiICOnvuLg4kw4oLZ6ExZC+fftKU4rL2cknJiZi586dOHnyJO7cuYOMjAxpsoqSHj58iOzsbIMTTrRv375MO/nCwkL8888/OHjwIM6fPy8tT6hNglH846Xk3+aifjmE7t27y35eREQEduzYAaBouu2+ffsavexDhw5Jf/fp00fWtjRgwAC8++67AICMjAwkJCSgadOm0uPq20Tz5s3h7e1tsE5jXnex8PBwrRMEyXHkyBHs27cPZ8+exYMHD5CVlaXx4+/+/fvS33K3gfbt2xss4+/vX+byxRP/lEfxpD7FvLy8yl2nqXTo0MFgGWdnZ/j6+krTx2vbt8qRn5+vcdkIOfs+e3t79OzZEz/++GOZlqlP/fr1UbNmTb1l1LcFwPD24+DgAB8fH2ldadt+1Pc9Xbt2lb1PVf8eOn78uMF2mOI7U72Os2fP4tlnn8VHH31Uar2Yivq+MSwsDCEhIQaf06FDB/j6+uLevXtSHa1bt9ZZ3lTrpiRr/J4DNLc3BwcHDBgwwGR1FxQUYNeuXThy5AguXbqE9PR05OTkaLxm9YmntL3m8PBwuLi4IDs7G/fv38dTTz2FL7/8EvXr1ze6PWX5Xre3t0ejRo1w4sQJqFQqnDx5Ep06ddL7HPXvVzmX1bBGDGtkcr6+vrJmIFMPG9qOzpw5c0b6287Orkwz+hUUFCA9PV3WERs51L+A9VH/YX716lUolUqt6+TcuXOYNGkS/v77b1nXnymWlpZmMKyVZee5evVqTJ8+HcnJyUa1xdzUt4U//vgDp06dkvU89S+ekj/C5SqepRCA7NnLmjRpAjs7OylwX758WWObKJ7BDCj6kSNH8XUFjbmeVVm2gS1btuCNN94w6ui03G2gVq1aBsuob9fGljf2Wl/aFM+oWMxaZoIESocRXQztW+VISkrSmJ2tLPs+UzJ2WyjLc7RtP+r7niNHjqBPnz4G6wQ0f+ga2veY6juzXr16eOqpp7Bx40YARfvztWvXolu3bnj00UfRqVMntGnTBo6OjrJegyFl2TcWly2+tp56HdqYat2os9bvOQAaMzVHRkbCwcGh3HWqVCp89tlnmDdvnlHfg9pes6OjI1588UV8/PHHAIqu99igQQN06NABvXr1QufOndG+fXu9Z1AARQcx1df/3Llz8fnnn8tql3rvmJzXo74PL7l/rywY1sjkyrJz0RZUio+8AUUXSyzrRSrT0tJMFtbknuJTstzDhw9LnUK0Z88e9O3bt0w/puRc9Nbd3d2oOl999VV88cUXZmlLealvC+fOnSvTaS5l/bJVP0pr6DSwYnZ2dvDy8pK+SEoesVdvizE9N56enkYFEmO3gUWLFuGtt94y6jmA/G3A2H2DseWNOeBhyTrLylT7VjlKbrNl3feZSlleuym2H/V9z7Vr18p0GpWhfY8p39dvvvkGqampUq+FUqnEzp07pcs9ODs7o3v37hg9ejSGDBlSrlPsyrJvLFnWUG+4qbd5a/6eAzTPWDDUkyyHUqnEkCFDyjQcQ9drnjt3Lq5evSodFBBCYP/+/dLpm/b29ujUqRNGjhyJ0aNHa72EkvrnCkCZhynI+V63pn14WXHMGlktUx0BMeV5+3K/OEoeuSy500tPT8eQIUOkoObu7o5Jkybhzz//xMWLF6XTIEXRJEBl2tkUj1uRY/369RpfYE2bNsUnn3yCAwcOICUlBdnZ2dK1XIQQWLFihdHtKQ9TbAtl3Q7U3ztjfjiobwMl3/+yfnkY+zxjtoGDBw9i6tSp0u2goCDMmTMHe/bsQXJyMrKysqBSqaRtwNTjQa1FySPCxdecrG7Ue9WAsu/7KjtL7nvKwtvbG7GxsVi9ejXat29f6rTtnJwcbNmyBSNGjEBERISsMWO6mGPfaE7W/j0HaK4PU3yWFi1apBHUOnTogKVLl+Lo0aNITU2VToMs/jdjxgyDdTo4OODnn3/Gr7/+ip49e5YK/AUFBYiNjcXzzz+PBg0aSEMR1FXk7zv1A5yGevysFXvWyGqp9zgEBweXa6C8qcgZzKqtXMmeve+++066YKS3tzcOHTqEhg0blnu5ZTV//nzp78cffxw//vij3gvEmrs9Jan3Uq1cuRJjxoypsGV7enpKRwGNed3qFygt2XumftuYMUW6LnpqCvPnz5fCYPv27fH333/Dzc1NZ/mK3gYqSskeggcPHphlzE9F/oAvi5IXAc7IyJB1hkJV2y7UP6szZ86U9WPW0mxsbDB69GiMHj0aqamp2L17N/bu3YudO3dqXNT73Llz6NGjB44dO1amU6bVtwdT7RvNydq/5wDN9VHeUy9VKhUWLVok3X7llVcMnmpozGt+7LHH8NhjjyEtLQ179uzB3r17sWvXLhw/flz6Lrlx4wb69++P3bt3a4whLfm+X716FUFBQbKXbQz13ltT9FZaAnvWyGqpjzdITk62iiPcV69elVVOPVi6uLiU+tFbPPkIUDSDlr6gBkBjohVTS01NRXx8vHT7008/1fsFZu72aKO+LVy8eLFCl62+c5d7wODu3bsaX3olvyDq1asn/S33lM5r166ZZEyWNkIIjaOf8+bN0xvUgIrfBipKnTp1NLZ/uRMLqPcsqM8mqYspJkMxp5Ljvcqy76sKLLnvMYVatWphyJAhWLJkCc6cOYPz58/jueeekx5/+PAh5syZU6a6y7JvBDTHqVXUj+fK8D0HaI5LLe/2dvz4celAo4uLCz766CODzynLa/b09MTAgQOxYMECHD16FElJSZg2bZrU45afn4/33ntP4zk1atTQOPPDnJ8t9X24uQKhuTGsUSnqHyBLnuvbpk0bqS0qlQq7d++2WFuKyZ2+V71cixYtSj2uPsFEyemmtSnr+dxyqA/yrVGjBoKDgy3aHm3Uj8gVj72oKC1btpT+Vp/9TJ8DBw5IfysUilLbgPp7Hh8fL+uHe/GAfHO4f/++xmkplt4mLcnGxgaNGzeWbhuaAKGY+vhAOe/n6dOnjW9cBfLz80Pt2rWl22XZ91UF6vuemJiYSj/+pVGjRli2bJnG2Qnbt28vU13q+8bjx4/LOkjx8OFDjUk01Oswp8rwPQdobm83btzAlStXylyX+u+M4lkcDVH/7iqrgIAAzJ8/H9OnT5fu27NnT6lTPJs3by7dNuf3uvo6NNcESObGsEalqJ/Ta64j+XJ4eXmhbdu20u2vv/7aYm0p9scffxicEESpVOLnn3+Wbmu7Jpj6l5qcqeBXrVplRCuNY2xbrly5ojHlbkXo1auX9PfBgwdx4sSJClu2+nWtduzYIWv2qR9++EH6OyIiotQpHz179pR6YvLz87Fy5UqDdS5btkxeg8ug5I8sQ9tBRkYGfvnlF7O1x9LUpwqXG6rUe0vlzFaqfg0za6W+7W/YsMFg+atXr8o+oFFZPProo9LfN2/exG+//WbB1pjO448/Lv19+/btMtWhvn2kpaXhr7/+Mvic9evXQ6VSAQBsbW1lXY7CFCrD9xwAtG7dWuP74ptvvilzXca+5tjYWJNeh0x9G1MqlaUmFVH/Xl+1apVZzp7Kzs7W6PWVcxkIa8SwRqWod8PLPapsLuoX0Pz1119NcoHp8khPTzd4KsHSpUtx/fp16ba28VXqR6z37dunt76ff/7ZrL2K6m25c+eOwdMRJk2aVOFHl3v16oXw8HDp9sSJEytsYPrw4cOlI5L5+fn43//+p7f80aNH8dNPP0m3tV2Y19fXF08++aR0e/bs2Xo/a//3f/9n1qO8vr6+GqcEGdom33nnnSo3Nkld165dpb/l9hSp9xBs2bJF70Gdq1ev4ttvvy17AyvI6NGjpb8PHTqE33//XW/59957r9L3PJUUFhamMV3/lClTrPYUVmPWfWZmpvR3Wa/FGBYWphG23n//fb29axkZGZg9e7Z0e+DAgbIur2AKleF7Dig6nVr9IuCLFy/WuHyEMdRf8+nTp/WOeS4oKMDrr79usM6ybmMASl1P9OWXX5a+d1JSUvDOO+/IrluuY8eOSeODa9WqJftSOdaGYY1KUf/R8d1331XY9UW0GTJkiPRlIITAyJEjsWbNGoPPO3v2LF544QVZPRbGmjNnjs6Lvm7fvl1j6vNBgwahSZMmpcp169ZN+vuLL77QefR++/btGDt2bPkabEBQUJDGedyvvvpqqZnggKKd+UsvvYQ//vjDrO3RRqFQYOHChdLRwUOHDqF///4Gjwjn5ORg9erV6NmzZ5mX7eXlhZdeekm6vXTpUixZskRr2QsXLuCJJ56Qvhzq1KmDZ599VmvZOXPmSCHwwYMH6N69O/766y+NL8OsrCzMmTMHL7/8MpycnAyOIysre3t7dOzYUbo9depUrV/sQgjMnTsXX375pVnaYS3Uj/geO3as1I8Obfr37y/1lt6/fx/Tpk3TWu7mzZt47LHHKkXY7du3L6KioqTbY8eO1dmrvXDhQqxdu7aCWlax5s6dK83Md+XKFfTo0UPjGo7aFBQUYPPmzejQoUOFjbdesGABXnrpJY3TDLW5f/++xkFH9YMTxlIfi3Ty5EmMGTNG64G09PR0PPnkk9KYKFtbW7P8ONelMnzPFXvjjTeksXy5ubl49NFHDR402rp1K/755x+N+9q2bStdYywnJwdTpkzRGrYyMzMxdOhQWWes/PDDD3j66adx7NgxveVycnIwc+ZM6XabNm1KXbMyICBAIyAuXrwYb7zxhsEDsampqZgzZw5ee+01g+1VP9Ddq1cvWT2M1oizQVIpI0aMwMKFCyGEwIkTJ1C3bl20bNkS3t7e0oYeERGBDz/80OxtsbGxwU8//YQ2bdogJSUFOTk5eOaZZ/Dpp59i8ODBaNasGTw9PZGdnY1bt24hLi4OO3bskMKPtvFi5TF06FD8+OOPGDZsGDZs2IAhQ4agTp06SE1Nxa+//oq1a9dKP9S9vLx0zrz0/PPPY8GCBcjJyUF6ejo6dOiAF198Ed27d4erqyuSkpLwyy+/YNOmTQCACRMmmPVI/KRJkzBlyhQAwLZt29CqVSu8+OKLaNq0KfLz8xEfH4/ly5fj3LlzsLW1xejRo80ShPXp168fpk+fLh2Z3blzJ+rXr49hw4ahe/fuqFu3Luzs7PDgwQOcP38ehw4dwrZt25CVlQU/P79yLXv27NnYsmWLNJPapEmT8Ouvv2LUqFGoX78+0tPTsWvXLixbtkzqUbGxscHy5ctLzapXrH79+vjyyy8xbtw4CCGQnJyMfv36oU6dOmjYsCFyc3Nx6tQpqb7Fixdj0aJFUnAw9RTpkyZNkr7YTpw4gaioKLzyyito1aoVhBBISEjAqlWrpB8N5t4mLSkgIABt27bF4cOHoVQqsWvXLjz22GN6n+Pj44Nx48ZJp2t/8cUXuHDhAsaOHYuAgAA8ePAAu3fvxjfffIOMjAyMGjUK33//fUW8nDKztbXFsmXL0KVLFxQUFOD+/fto3749xo8fj169esHLywuJiYlYs2YNdu3aBYVCgcGDB2v0LFcFLVq0wFdffSX1kp84cQJNmjTBk08+iUcffRTBwcFwdHTEw4cPceXKFRw5cgR//fWXxjWzKkJOTg6WLl2KpUuXokWLFujZsydatGiBWrVqwdnZGXfv3sWhQ4fw3XffSQe6bG1tdR5YkKN///549tlnpWnu161bh+PHj+P5559HVFQUCgsLcezYMXz99dcaY6imTp2qMcyhIlSG7zmgqAdo9erVGDhwIJRKJVJSUtCuXTsMGjQIAwYMQHBwMOzs7JCSkoKjR49i8+bNuHz5MlasWKEx7MLZ2RnPPfecdHBx+fLlSEhIwIQJE9CgQQNkZWXh8OHD+Oabb3D9+nW4ublhwIABWL9+vc62KZVKrF27FmvXrkWjRo3Qu3dvtGrVCrVr14arqysePnyIuLg4fPfddxqnH7777rta65szZw6OHj2KXbt2AQA++eQTrF+/HiNHjkT79u1Rs2ZNKJVK3L17F6dPn8Y///yDPXv2QKVSYdiwYQbXpfr1eZ966imD5a2WINJi+vTpAoDOf926ddMoHxMTIz0WFBQkaxkzZsyQnjNmzBi9Za9duyaaN2+ut03a/i1durRsK0BNUFCQVF9MTIwYNWqUweW6urqKffv26a139erVQqFQGKyrS5cuIicnR+O+xMRErXWOGTNGKjNjxgzZr1GpVIrevXsbbIuNjY1YsmSJWLFihc5tQd+608bYbefTTz8Vtra2Rm0Hfn5+5V72zZs3RUREhKzl2dvbi3Xr1hl8LUIIsXz5cuHq6qqzLltbWzF//nwhhBB+fn7S/WfOnNFanzGfq5ImTJgg6/W9+eabstednG1AnbHbsNxt0VifffaZVO/YsWNlPefBgweiadOmsj7T2dnZZvtMy1nnxmwnGzZskPWZmz59ukn3Dca+t4mJiRrtkcOY7XP9+vXC2dnZqH0PAJGTk1OqLnN8Z6o/LuefnZ2dWLVqlc7lydk+hRAiPz9fDB8+XPZyX331VVFYWKizPnP9nrDk91xZbNmyRbi5uclerytWrChVR2ZmpmjWrJnB5zo6OoqNGzcaXI/q60Tuvzlz5uh9nTk5OeLpp582ut5hw4bprff27dvCxsZGABDe3t4iLy/PmNVvVXgaJGk1e/Zs7Nq1C6NGjULjxo3h5uZm0e7jevXq4fDhw/j6668NTnPv5uaGgQMHYt26dWY5hXDNmjVYsGABfH19tT7evXt3HD9+XOO0Mm1Gjx6NX3/9FSEhIVof9/b2xnvvvYddu3bBycmp3O3Wx9bWFr/99humTJmis8cmIiIC27Ztw6uvvmrWthgyefJknDp1CsOHDze4XsLCwjBt2jSTzKRYu3ZtHDp0CDNmzCh17n0xGxsb9OvXD3FxcRg+fLiseseNG4czZ85g6tSpaNq0Kdzc3ODu7o4mTZrgxRdfxPHjxzFt2jQolUqNI/UlrwdmCsuWLcO8efP09gauXbsWCxcuNPmyrc3o0aOl01Q3b94sa7IlLy8vxMTE6Dzi6+bmhmnTpmHnzp2lTgmyZkOHDsW+ffs0Zm9TFxAQgFWrVmmMR6qKhg0bhvPnz2PixIkas39qExwcjFdeeQVHjhwx+/672ODBg/HKK68gNDRUbzlbW1v0798fx44dwzPPPFPu5drb22Pt2rX4/vvv9S47MjISv/32G5YsWWKR3xOV6XsOKDoNOSEhAePHj9e7v/Dy8sL48ePRvXv3Uo+5urpiz549GDNmTKmLVxfr0KEDDhw4oDGOWpfu3btj6tSpiIiI0PseKhQKdO3aFbt379bZq1bMyckJ33//PbZu3YouXbpozEhekq2tLTp27IhPPvlE4wLn2qxfv14602ns2LFGXbjd2iiEqGKjgalauHLlCg4dOoTU1FRkZGTA1dUVfn5+CAsLQ2RkpMHrpxgjODhYmiEpJiYG0dHRAIomm9i1axcSExORlpYGf39/dO7cGQ0aNDCqfqVSiQMHDiA+Ph7p6enStMLR0dEW2bncu3cPMTEx0ikMtWvXRlRUlMbYFWuRm5uLffv24cqVK9JMU56enqhfvz4iIyNRp04dsyxXqVRi//79OHfuHO7duwdXV1fUqVMH0dHRZglRQNHYqdatWwMomgQoJSXFLMsBiiYCiI2NxcWLF5Gfnw9/f380adIE7dq1M9syrdHEiRPxf//3fwCA77//Hk8//bTs516/fh0xMTG4efMmnJ2dERQUhJ49e5pt3GFFiY+Px5EjR3Dnzh34+vqiUaNG6Nq1q94fWFWRUqnEoUOHcP78edy9excqlQoeHh4ICgpCRESErKnhzen27duIj49HYmIiHjx4gMLCQnh4eCA0NBRt27bVebDRFE6dOoXjx48jNTUVCoUCfn5+aN++vcEDrRWpMn3PAUXfdf/88w+uXLmCu3fvws7ODrVq1ULTpk3RokUL2NkZHtV08+ZNxMTE4Pr167Czs0OdOnXQpk0bo3+zFHvw4AFOnDiBy5cv4969e1AqlXBzc0NwcDDatGlT5u/fe/fu4Z9//sGNGzfw4MEDODg4wMfHB40aNUKzZs10HkwsqXnz5oiPj4eNjQ0uXrxYpgu/WwuGNSIDdIU1oor08ssv46uvvgIAPPHEE9J4RjKfy5cvIywsDEqlEu3btzfJNYiIiMi8/vnnH+nSEiNGjKj0kx9Vr0NhRERWRO6xsl27dkk9PADMPkMoFQkNDZWmrz948KDBSxoQEZHlFZ+qb2NjgxkzZli4NeXHsEZEZCHLly/HsGHDsGXLFq3Te9+7dw8ffvgh+vbtK11ItlWrVujfv39FN7Xa+uCDD6Sxa7NmzbJwa4iISJ8TJ05I14ScMGECGjdubOEWlR+n7icishClUokff/wRP/74I+zt7dGwYUPpIrG3bt3C+fPnNXrffHx8sHr1ap0Dxcn0AgMDMW3aNMyYMQPbt29HbGwsT4UmIrJS7733HoQQ8PT0rJBLTFUEjlkjMoBj1shcli1bhhdeeEFW2ebNm2P9+vVV4ighERERycOeNSIiC5kwYQLCwsKwdetWHDp0CJcuXcLdu3eRl5cHDw8P+Pn5oUOHDhg0aBAee+wxi14+g4iIiCoee9aIiIiIiIisEHvWLKiwsBA3b96Eu7s7j5gTEREREVUTQghkZGSgTp06eq9VybBmQTdv3kRgYKClm0FERERERBaQnJyMgIAAnY8zrFmQu7s7gKI3Se4V2YmIiIiIqHJLT09HYGCglAd0YVizoOJTHz08PBjWiIiIiIiqGUNDoXhRbCIiIiIiIivEsEZERERERGSFGNaIiIiIiIisEMesWTkhBJRKJVQqlaWbQmQW9vb2sLW1tXQziIiIiKwOw5oVy8/PR0pKCrKzsy3dFCKzUSgUCAgIgJubm6WbQkRERGRVGNasVGFhIRITE2Fra4s6derAwcGBF86mKkcIgTt37uD69eto2LAhe9iIiIiI1DCsWan8/HwUFhYiMDAQLi4ulm4OkdnUrFkTV69eRUFBAcMaERERkRpOMGLlbGz4FlHVxh5jIiIiIu2YBIiIiIiIiKwQw1p1UFgI5GcV/V8JrVy5El5eXpZuhlEqY5uJiIiIyLowrFVlt04Bm18C5tb579/ml4ruN5OxY8dCoVCU+tenTx9Zzw8ODsbixYs17hs2bBguXLhghtZqskTAiomJwYABA1CzZk04OTkhNDQUw4YNw549e2S1y8vLCytXrpRuF6/vgwcPapTLy8uDr68vFAoFYmNjzfBKiIiIiMjUGNaqqlM/A8uigcS9QJcpwJPfFv2fuLfo/lM/m23Rffr0QUpKisa/devWlbk+Z2dn1KpVy4QttA5fffUVevbsCV9fX2zYsAEJCQlYs2YNOnbsiNdff73M9QYGBmLFihUa9/3yyy+cGp+IiIiokmFYq4punQJ+eQGIHAK8dhzoNhWIGlL0/2vHi+7/5QWz9bA5OjrC399f45+3t7f0+MyZM1GvXj04OjqiTp06eO211wAA0dHRuHbtGl5//XWphwgo3bM0c+ZMNG/eHN999x3q1asHNzc3vPjii1CpVFiwYAH8/f1Rq1YtzJkzR6Ndn3zyCSIjI+Hq6orAwEC89NJLyMzMBADExsbi2WefRVpamrTsmTNnAiiamXPq1KmoW7cuXF1d0a5du1K9UytXrkS9evXg4uKCJ554Avfu3dO7jpKSkjB58mRMnjwZq1atQo8ePRASEoKOHTti0qRJOHr0aFlWPQBgzJgxWL9+PXJycqT7vvvuO4wZM6bMdRIRERFRxWNYqyyEKBp3Juff/iWAuz/QZz6gytd8TJVfdL+bH7D/c3n1CWGyl/Hzzz/j008/xf/93//h4sWL2Lx5MyIjIwEAmzZtQkBAAGbNmiX1yOly+fJl/PXXX9i6dSvWrVuH7777Dv3798f169exe/dufPTRR5g+fbrG6YA2NjZYsmQJTp8+jVWrVmHXrl2YOnUqAKBjx45YvHgxPDw8pGW/+eabAIBnn30W+/btw/r163Hy5EkMGTIEffr0wcWLFwEAhw4dwrhx4/DSSy/hxIkT6N69Oz788EO962Hjxo0oKCiQll9SeWZIbNWqFUJCQrBx40YAQHJyMvbs2YPRo0eXuU4iIiIiqni8zlplUZBdNObMGB8F6X/85Iaif4a8exNwcJW92D/++KPUKXfTpk3D+++/j6SkJPj7++ORRx6Bvb096tWrh7Zt2wIAfHx8YGtrC3d3d/j7++tdRmFhIb777ju4u7sjPDwc3bt3x/nz57FlyxbY2NigcePG+OijjxAbG4v27dsDACZPniw9PyQkBLNnz8aLL76Ir776Cg4ODvD09IRCodBY9uXLl7Fu3Tpcv34ddeoUrf8333wTW7duxYoVKzB37lx89tln6N27N95++20AQKNGjbB//35s3bpVZ/svXLgADw8PjWVt3LhRo/frwIEDUpA11rPPPovvvvsOo0aNwooVK9CvXz/UrFmzTHURERERkWUwrJHJde/eHUuXLtW4z8fHBwAwZMgQLF68GPXr10efPn3Qr18/DBw4EHZ2xm2KwcHBcHd3l277+fnB1tZW47p0fn5+SE1NlW7HxMRg7ty5OHv2LNLT06FUKpGbm4usrCy4umoPo8ePH4cQAo0aNdK4v3jCDgBISEjAE088ofF4hw4d9IY1oHTvWe/evXHixAncuHED0dHRUKlUep+vz6hRo/D222/jypUrWLlyJZYsWVLmuoiIiIjIMhjWKgt7l6IeLkNEIbCwAdDxVaCznkkq/vm06DTIty4Dhk65s3cxqqmurq5o0KCB1scCAwNx/vx5/P3339ixYwdeeuklLFy4ELt374a9vb3sZZQsq1AotN5X+O/lCq5du4Z+/fph4sSJmD17Nnx8fPDPP/9g/PjxKCgo0LmcwsJC2Nra4tixY7C1tdV4rLj3UJThNNGGDRsiLS0Nt27dknrX3Nzc0KBBg1LB1cPDA5mZmVCpVBptUKlUyMzMhKenZ6n6fX19MWDAAIwfPx65ubno27cvMjIyjG4nEREREVkOx6xVFgpF0amIhv45ugMRTwHxGwBbB+1lbB2A+PVAxGDA0c1wneUYP6WNs7MzHnvsMSxZsgSxsbE4cOAATp0qmuzEwcGhXD1Kuhw9ehRKpRIff/wx2rdvj0aNGuHmTc3wq23ZLVq0gEqlQmpqKho0aKDxrzhkhYeHl5oqv+TtkgYPHgx7e3t89NFHBtseFhYGlUqFuLg4jfuPHz8OlUqFxo0ba33euHHjEBsbi2eeeaZU0CQiIiIi68eetaqo/YtFY9F+exV47HPAVq3HSVUA/PoKkJECtJ9olsXn5eXh1q1bGvfZ2dmhRo0aWLlyJVQqFdq1awcXFxesWbMGzs7OCAoqGl8XHByMPXv2YPjw4XB0dESNGjVM0qbQ0FAolUp8/vnnGDhwIPbt24evv/5ao0xwcDAyMzOxc+dONGvWDC4uLmjUqBGefvppPPPMM/j444/RokUL3L17F7t27UJkZCT69euH1157DR07dsSCBQvw+OOPY/v27QZPgaxXrx4+/vhjTJo0Cffv38fYsWMREhKC+/fv4/vvvwcAKWCFh4ejb9++GDduHD755BOEhobi8uXLmDJlCvr27Yvw8HCty+jTpw/u3LkDDw8PE6xBIiIiIqpo7FmrivwjgSf+Dzj1E7CkBbB7AXDyx6L/l7QATv9c9Lh/2SavMGTr1q2oXbu2xr/OnTsDKLqI8zfffINOnTohKioKO3fuxO+//y6N/5o1axauXr2K0NBQk06I0bx5c3zyySf46KOPEBERgR9++AHz5s3TKNOxY0dMnDgRw4YNQ82aNbFgwQIAwIoVK/DMM8/gjTfeQOPGjfHYY4/h0KFDCAwMBAC0b98e3377LT7//HM0b94c27dvx/Tp0w226dVXX8X27dtx584dDB48GA0bNkS/fv2QmJiIrVu3akwusn79ejzyyCN48cUXER4ejhdffBE9e/bUe/06hUKBGjVqwMHBoSyrjIiIiIgsTCHKMuCGTCI9PR2enp5IS0sr1fuRm5uLxMREhISEwMnJqWwLuHUKOPg1cHojoMwB7JyLTpFsP9FsQY3IWCbZ1omIiIgqEX05QB1Pg6zK/COBx78sOhVSmVM0UYiJx58REREREZF5MKxVBzY2Rl0njYiIiIiILI9j1oiIiIiIiKwQwxoREREREZEVYlgjIiIiIiKyQgxrREREREREVqhahLU9e/Zg4MCBqFOnDhQKBTZv3mzwObt370arVq3g5OSE+vXrl7qAMgBs3LgR4eHhcHR0RHh4OH755RcztJ6IiIiIiKqjahHWsrKy0KxZM3zxxReyyicmJqJfv37o0qUL4uLi8O677+K1117Dxo0bpTIHDhzAsGHDMHr0aMTHx2P06NEYOnQoDh06ZK6XQURERERE1Ui1uyi2QqHAL7/8gscff1xnmWnTpuG3335DQkKCdN/EiRMRHx+PAwcOAACGDRuG9PR0/PXXX1KZPn36wNvbG+vWrZPVFlNfFPvTvy/A1kaB13o2lFUeAJbsvAhVocDrjzaS/RwiU+JFsYmIiKi6kXtR7GrRs2asAwcOoFevXhr39e7dG0ePHkVBQYHeMvv376+wdpZka6PAJ39fwJKdF2WVX7LzIj75N+BZo+joaEyePNnSzSAiIiKiSk4Igbd+ise6w0lQqgot3RzZGNa0uHXrFvz8/DTu8/Pzg1KpxN27d/WWuXXrls568/LykJ6ervHPlF7r2RBTHm0kK7AVB7UpjzYyqifOEF0Ba/PmzVAorDMUkmHBwcFYvHixpZtBREREVCbf7UvET8eu451Np3DlbpalmyObnaUbYK1KBovis0XV79dWRl8gmTdvHj744AMTtrK04uD1yd8XNG6rM1dQo8pFCAGVSgU7u4rbDeTn58PBwaHClkdERET009FkfPhH0fAmbxd7NKjpZuEWyceeNS38/f1L9ZClpqbCzs4Ovr6+esuU7G1T98477yAtLU36l5ycbPrGQ38Pm7UEtZkzZ6J58+ZYs2YNgoOD4enpieHDhyMjI0Pnc7Zu3QpPT0+sXr0aADB27Fg8/vjjWLRoEWrXrg1fX1+8/PLL0qmqAPDgwQM888wz8Pb2houLC/r27YuLF4vWiRACNWvW1Jg4pnnz5qhVq5Z0+8CBA7C3t0dmZiaAooD+7bff4oknnoCLiwsaNmyI3377Te9rTUlJQf/+/eHs7IyQkBCsXbu2VE9VWloann/+edSqVQseHh7o0aMH4uPjjVpfQggsWLAA9evXh7OzM5o1a4aff/5Zejw2NhYKhQLbtm1D69at4ejoiL179+Ly5csYNGgQ/Pz84ObmhjZt2mDHjh3S86Kjo3Ht2jW8/vrrUCgUGgckNm7ciKZNm8LR0RHBwcH4+OOPNV57cHAwPvzwQ4wdOxaenp547rnn9K4rIiIiIlPJVxbiu38SMfXnkyiepKN3Uz/YWOkQIG0Y1rTo0KED/v77b437tm/fjtatW8Pe3l5vmY4dO+qs19HRER4eHhr/zEVbYLOWoFbs8uXL2Lx5M/744w/88ccf2L17N+bPn6+17Pr16zF06FCsXr0azzzzjHR/TEwMLl++jJiYGKxatQorV67EypUrpcfHjh2Lo0eP4rfffsOBAwcghEC/fv1QUFAAhUKBrl27IjY2FkBRsDt79iwKCgpw9uxZAEUBp1WrVnBz++8IzAcffIChQ4fi5MmT6NevH55++mncv39f5+t85plncPPmTcTGxmLjxo1YtmwZUlNTpceFEOjfvz9u3bqFLVu24NixY2jZsiV69uypUa+h9TV9+nSsWLECS5cuxZkzZ/D6669j1KhR2L17t0Z7pk6dinnz5iEhIQFRUVHIzMxEv379sGPHDsTFxaF3794YOHAgkpKSAACbNm1CQEAAZs2ahZSUFKSkpAAAjh07hqFDh2L48OE4deoUZs6ciffff19j/QPAwoULERERgWPHjuH999/XuZ6IiIiITCFfWYgbD3Pwf7svY/YfZyEAeDgVnUnULNDbso0zlqgGMjIyRFxcnIiLixMAxCeffCLi4uLEtWvXhBBCvP3222L06NFS+StXrggXFxfx+uuvi7Nnz4rly5cLe3t78fPPP0tl9u3bJ2xtbcX8+fNFQkKCmD9/vrCzsxMHDx6U3a60tDQBQKSlpZV6LCcnR5w9e1bk5OSU45UL8dmOCyJo2h+i4btbRNC0P8RnOy6Uqz5DunXrJiZNmlTq/l9++UWob24zZswQLi4uIj09XbrvrbfeEu3atStV15dffik8PT3Frl27NOocM2aMCAoKEkqlUrpvyJAhYtiwYUIIIS5cuCAAiH379kmP3717Vzg7O4sff/xRCCHEkiVLREREhBBCiM2bN4vWrVuLJ598Unz55ZdCCCF69eolpk2bJj0fgJg+fbp0OzMzUygUCvHXX39pXR8JCQkCgDhy5Ih038WLFwUA8emnnwohhNi5c6fw8PAQubm5Gs8NDQ0V//d//ydrfWVmZgonJyexf/9+jTrGjx8vRowYIYQQIiYmRgAQmzdv1tpWdeHh4eLzzz+XbgcFBUntLTZy5Ejx6KOPatz31ltvifDwcI3nPf7443qXZaptnYiIiKq3vAKVuP4gW5y6/lB8tuOCCJ72hwia9ocY/n8HRKP3/hRB0/4Qp64/tHQzhRD6c4C6atGzdvToUbRo0QItWrQAAEyZMgUtWrTA//73PwBFp6kV9yIAQEhICLZs2YLY2Fg0b94cs2fPxpIlS/DUU09JZTp27Ij169djxYoViIqKwsqVK7Fhwwa0a9euYl+cAa/1bAgHWxvkqwrhYGtjFT1qxYKDg+Hu7i7drl27tkaPE1B0mt3kyZOxfft2dO/evVQdTZs2ha2trdY6EhISYGdnp/Ge+Pr6onHjxtJlGaKjo3HmzBncvXsXu3fvRnR0NKKjo7F7924olUrs378f3bp101hmVFSU9Lerqyvc3d1LtbvY+fPnYWdnh5YtW0r3NWjQAN7e/x3VOXbsGDIzM+Hr6ws3NzfpX2JiIi5fvixrfZ09exa5ubl49NFHNepYvXq1Rh0A0Lp1a43bWVlZmDp1KsLDw+Hl5QU3NzecO3dO4zOhTUJCAjp16qRxX6dOnXDx4kWoVCqdyyMiIiIypeKetAu3M3A/Mx+7zqXi078vQADoUN8X0Y1qIk8pYGejQGN/d4P1WZNqMcFIdHS0NEGINiVP2wKAbt264fjx43rrHTx4MAYPHlze5pnVkp0XpaCWryrEkp0XzRrYPDw8kJaWVur+hw8fljrts/iU0mIKhQKFhZpTqTZv3hzHjx/HihUr0KZNm1ITuOirQ9d7LtQmgomIiICvry92796N3bt3Y9asWQgMDMScOXNw5MgR5OTkoHPnzka3W31Zhu4vLCxE7dq1pdMx1Xl5eclabvH/f/75J+rWratRztHRUeO2q6urxu233noL27Ztw6JFi9CgQQM4Oztj8ODByM/P19p29degayIefcsjIiIiMoV8ZSHuZObhQVY+in+CxJzXDGqDWwXgRPJDAECAtzPsbStXX1W1CGvVVckxasW3Ae2zRJpCWFiYxoXCix05cgSNGzc2ur7Q0FB8/PHHiI6Ohq2tLb744gvZzw0PD4dSqcShQ4eksYT37t3DhQsX0KRJEwCQxq39+uuvOH36NLp06QJ3d3cUFBTg66+/RsuWLTV6s4wVFhYGpVKJuLg4tGrVCgBw6dIlPHz4UCrTsmVL3Lp1C3Z2dggODi7TcsLDw+Ho6IikpKRSPYGG7N27F2PHjsUTTzwBAMjMzMTVq1c1yjg4OGj0lhUv859//tG4b//+/WjUqJFGbycRERGRKWkLaYD2oGajUCDpfjYAIMin8h1ArlzRkmTTNpmIMddhK6uXXnoJly9fxssvv4z4+HhcuHABX375JZYvX4633nqrTHU2atQIMTEx0imRcjVs2BCDBg3Cc889h3/++Qfx8fEYNWoU6tati0GDBknloqOjsXbtWkRFRcHDw0MKcD/88AOio6PL1OZiYWFheOSRR/D888/j8OHDiIuLw/PPPw9nZ2epV+qRRx5Bhw4d8Pjjj2Pbtm24evUq9u/fj+nTp+Po0aOyluPu7o4333wTr7/+OlatWoXLly8jLi4OX375JVatWqX3uQ0aNMCmTZtw4sQJxMfHY+TIkaV6CoODg7Fnzx7cuHFDutbgG2+8gZ07d2L27Nm4cOECVq1ahS+++AJvvvlmGdYUERERkX4lT3eUE9QAILk4rPm6WKDV5cOwVgXpm/XR3IEtODhYmg6+V69eaNOmjTRD45AhQ8pcb+PGjbFr1y6sW7cOb7zxhuznrVixAq1atcKAAQPQoUMHCCGwZcsWjVMKu3fvDpVKpRHMunXrBpVKZXQvlTarV6+Gn58funbtiieeeALPPfcc3N3d4eTkBKCod2/Lli3o2rUrxo0bh0aNGmH48OG4evWq3ktBlDR79mz873//w7x589CkSRP07t0bv//+O0JCQvQ+79NPP4W3tzc6duyIgQMHonfv3hpj7ABg1qxZuHr1KkJDQ1GzZk0ART2CP/74I9avX4+IiAj873//w6xZszB27FjjVhARERGRHvpCGqA/qBUWClx/mAMAqOdT+cKaQugbzEVmlZ6eDk9PT6SlpZUaz5Wbm4vExESEhIRIP+rlkDs9v7VN41+dXL9+HYGBgdixYwd69uxp6eZYXFm3dSIiIqradJ3uqE5fUAOAlLQcLNh2Hg52NvhkcDMMaF6nYhpvgL4coI5j1qoQYwJY8ePmHsNGwK5du5CZmYnIyEikpKRg6tSpCA4ORteuXS3dNCIiIiKrIyekAYaDGgAkPyjqVQvwcq5UF8MuxrBWhagKhVE9ZcXlVIXsXDWngoICvPvuu7hy5Qrc3d3RsWNH/PDDD6VmdyQiIiKqzuSGNEBeUAP+G69WGU+BBBjWqpTXH21k9HPYo2Z+vXv3Ru/evS3dDCIiIiKrZExIA+QHNeC/sBbIsEZERERERCSPsSENMC6oKVVFE5MAQKA3wxoREREREZFeZQlpgHFBDQBupedCWSjgbG+LGm4O5W+4BTCsWTlO1klVHbdxIiKi6qGsIQ0wPqgBkC6GHejz3/VtKxuGNStVPPlEdnY2nJ2dLdwaIvPJz88HANja2lq4JURERGQO5QlpQNmCGvDfTJCV9RRIgGHNatna2sLLywupqakAABcXl0p7RIBIl8LCQty5cwcuLi6ws+PuiIiIqCopb0gDyh7UgMo/uQjAsGbV/P39AUAKbERVkY2NDerVq8eDEURERFWEKUIaUL6glq8sREpaUc9aPfaskTkoFArUrl0btWrVQkFBgaWbQ2QWDg4OsLGxsXQziIiIqJxMFdKA8gU1AEhJy0GhANwc7eDlUnmvbcuwVgnY2tpyPA8RERERWSVThjSg/EENUJ9cpHIPJWJYIyIiIiIio5k6pAGmCWoAkPzg37DmXbkn6mNYIyIiIiIi2cwR0gDTBTUASLr/73i1Sjy5CMCwRkREREREMpgrpAGmDWp5BSqkpucCqNzT9gMMa0REREREpIc5Qxpg2qAGANcf5kAA8HK2h4dz5Z1cBGBYIyIiIiIiLcwd0gDTBzWgalxfrRjDGhERERERSSoipAHmCWqA2kyQlfwUSIBhjYiIiIiIUHEhDTBfUAPUZoL0qdwzQQIMa0RERERE1VpFhjTAvEEtO1+Ju5n5ANizRkRERERElVRFhzTAvEENAJIfFE3Z7+vqAFfHyh91Kv8rICIiIiIi2SwR0gDzBzWgak0uAjCsERERERFVC5YKaUDFBDXgv7BWrwqcAgkwrBERERERVWmWDGlAxQU1oGpNLgIwrBERERERVUmWDmlAxQa1jNwCPMgugAJAAHvWiIiIiIjI2lhDSAMqNqgBwLV/T4Gs5eEIJ3tbsy2nIjGsERERERFVAdYS0oCKD2oAkHSvKKwF+biadTkViWGNiIiIiKgSs6aQBlgmqAHAtftZAIB6VWQmSIBhjYiIiIioUrK2kAZYLqgVCoGkf0+DDPJlWCMiIiIiIguwxpAGWC6oAcCdjDzkFhTC3laB2p5VYyZIgGGNiIiIiKhSsNaQBlg2qAHAtXtFp0AGeLvA1qbilmtuDGtERERERFbMmkMaYPmgBvw3E2RwFToFEmBYIyIiIiKyStYe0gDrCGoAcO3fmSDrVaGZIAGGNSIiIiIiq1IZQhpgPUEtX1mIlLQcAFVrchGAYY2IiIiIyCpUlpAGWE9QA4DrD7JRKAAPJzt4OdtbpA3mwrBGRERERGRBlSmkAdYV1ID/xqsF+bpCYcF2mAPDGhERERGRBVS2kAZYX1AD/psJsipdDLsYwxoRERERUQWqjCENsM6gBqj3rDGsERERERFRGVTWkAZYb1BLyynAw+wCKAAEejOsERERERGRESpzSAOsN6gBQNL9olMg/T2d4GRva+HWmB7DGhERERGRGVT2kAZYd1AD1K+vVvV61QCGNSIiIiIik6oKIQ2w/qAGqI1Xq2IXwy7GsEZEREREZAJVJaQBlSOoFRYKJFfhyUUAhjUiIiIionKpSiENqBxBDQBuZ+QiT1kIBzsb+Hs4Wbo5ZsGwRkRERERUBlUtpAGVJ6gBauPVvF1gY2OdbSwvhjUiIiIiIiNUxZAGVK6gBvw3Xq1eFT0FEmBYIyIiIiKSpaqGNKDyBTUASLpXNG1/UBWdCRJgWCMiIiIi0qsqhzSgcga1vAIVUtJzAQBBvlVzJkiAYY2IiIiISKuqHtKAyhnUACD5QTaEALyc7eHpbG/p5pgNwxoRERERkZrqENKAyhvUALXJRarweDWAYY2IiIiICED1CWlA5Q5qQNW/GHYxhjUiIiIiqtaqU0gDKn9QA4CkKn4x7GIMa0RERERULVW3kAZUjaD2MDsfaTkFsFEAAd7Olm6OWTGsEREREVG1Uh1DGlA1ghrw3ymQ/p5OcLSztXBrzIthjYiIiIiqheoa0oCqE9SA/yYXqerj1QCGNSIiIiKq4qpzSAOqVlADgKT7/14Mu4qPVwMY1oiIiIioiqruIQ2oekFNVSiQfD8HAHvWiIiIiIgqHYa0IlUtqAHArbQc5KsK4WRvg1oejpZujtkxrBERERFRlcCQ9p+qGNQAIFFtvFpVeD2GMKwRERERUaXGkKapqgY1ALh6t2i8WnCNqn8KJMCwRkRERESVFENaaVU5qAFA4r2isBbiy7BGRERERGR1GNK0q+pBLS2nAPez8qFA9ZgJEmBYIyIiIqJKgiFNt6oe1ADg2r+9arU9neBkX7Uvhl2MYY2IiIiIrBpDmn7VIagBQGI1G68GMKwRERERkZViSDOsugQ1ALj6b89acDUZrwYwrBERERGRlWFIk6c6BbUCVSGSHxRdDDuEPWtERERERBWLIU2+6hTUAOD6gxyoCgXcHO3g6+pg6eZUGIY1IiIiIrIohjTjVLegBvx3fbWQGq5QVPHXqo5hjYiIiIgsgiHNeNUxqAH/XV+tOo1XAxjWiIiIiKiCMaSVTXUNakKI/yYXqVE9rq9WjGGNiIiIiCoEQ1rZVdegBgD3svKRkauErY0Cgd4Ma0REREREJsOQVj7VOagB/03ZH+DtDHtbGwu3pmIxrBERERGRWTCklV91D2qA2uQi1Wy8GsCwRkREREQmxpBmGgxqRaTJRarR9dWKVZt+xK+++gohISFwcnJCq1atsHfvXp1lx44dC4VCUepf06ZNpTIrV67UWiY3N7ciXg4RERGR1clXFuLGwxxcuJ2B+5kMauXBoFYkt0CFlLSi39fVbSZIoJqEtQ0bNmDy5Ml47733EBcXhy5duqBv375ISkrSWv6zzz5DSkqK9C85ORk+Pj4YMmSIRjkPDw+NcikpKXBycqqIl0RERERkNRjSTItB7T/X7mdDCMDH1QGezvaWbk6FqxanQX7yyScYP348JkyYAABYvHgxtm3bhqVLl2LevHmlynt6esLT01O6vXnzZjx48ADPPvusRjmFQgF/f3/zNp6IiIjISvF0R9NjUNNUPF6tOvaqAdWgZy0/Px/Hjh1Dr169NO7v1asX9u/fL6uO5cuX45FHHkFQUJDG/ZmZmQgKCkJAQAAGDBiAuLg4vfXk5eUhPT1d4x8RERFRZcOeNPNgUCuteCbIEN/qNWV/sSof1u7evQuVSgU/Pz+N+/38/HDr1i2Dz09JScFff/0l9coVCwsLw8qVK/Hbb79h3bp1cHJyQqdOnXDx4kWddc2bN0/qtfP09ERgYGDZXhQRERGRBTCkmQ+DWmmFGhfDZs9alaYosbELIUrdp83KlSvh5eWFxx9/XOP+9u3bY9SoUWjWrBm6dOmCH3/8EY0aNcLnn3+us6533nkHaWlp0r/k5OQyvRYiIiKiisSQZl4MatrdTs9FbkEhHOxsUNvT2dLNsYgqP2atRo0asLW1LdWLlpqaWqq3rSQhBL777juMHj0aDg4Oesva2NigTZs2envWHB0d4ejoKL/xRERERBbEMWnmx6CmW+K/49WCfFxga1M910mV71lzcHBAq1at8Pfff2vc//fff6Njx456n7t7925cunQJ48ePN7gcIQROnDiB2rVrl6u9RERERJbGnrSKwaCmn3QKZDWdXASoBj1rADBlyhSMHj0arVu3RocOHbBs2TIkJSVh4sSJAIpOT7xx4wZWr16t8bzly5ejXbt2iIiIKFXnBx98gPbt26Nhw4ZIT0/HkiVLcOLECXz55ZcV8pqIiIiITI09aRWHQc2wq3ezAQAh1XS8GlBNwtqwYcNw7949zJo1CykpKYiIiMCWLVuk2R1TUlJKXXMtLS0NGzduxGeffaa1zocPH+L555/HrVu34OnpiRYtWmDPnj1o27at2V8PERERkSkxpFUsBjXDMnOVuJOZBwAIqqYzQQKAQgh+JC0lPT0dnp6eSEtLg4eHh6WbQ0RERNUMQ1rFY1CT5/SNNCzflwg/Dye83SfMJHU62tmgX5R1DFmSmwOqRc8aEREREf2HIc0yGNTkS6zm11crxrBGREREVE0wpFkOg5pxrt6t3tdXK8awRkRERFTFMaRZFoOacZSqQiQ/+HdykWo8EyTAsEZERERUZTGkWR6DmvGuP8xBgUrAxcEWNd2r9zWKGdaIiIiIqhiGNOvAoFY2V+4UnQJZv4YrFNV8fTGsEREREVURDGnWg0Gt7K7czQQA1K/hZuGWWB7DGhEREVElx5BmXRjUyq5QCCT+O7lI/ZrVe7wawLBGREREVGkxpFkfBrXyuZ2ei+x8FRxsbRDgXb2n7QcY1oiIiIgqHYY068SgVn7F49WCfF1ga8N1x7BGREREVEkwpFkvBjXTkMar1eR4NYBhjYiIiMjqMaRZNwY10xBC4PK/PWuh1fxi2MUY1oiIiIisFEOa9WNQM537WflIyymAjQIIquYXwy7GsEZERERkZRjSKgcGNdO68u8skIHeLnCws7Fwa6wDwxoRERGRlWBIqzwY1Ezvyp3i8WrsVSvGsEZERERkYQxplQuDmnkU96zxYtj/YVgjIiIishCGtMqHQc08MnILkJqRBwAI4eQiEoY1IiIiogrGkFY5MaiZT+K/vWr+Hk5wdWREKcY1QURERFRBGNIqLwY185Km7Od4NQ0Ma0RERERmxpBWuTGomR8vhq0dwxoRERGRmTCkVX4MauaXW6DCjYc5AID6HK+mgWGNiIiIyMQY0qoGBrWKcfVeFoQAfFwd4OXiYOnmWBWGNSIiIiITYUirOswW1EQhbFW5UNk6AQpe+BkArtwpnrKfvWolMawRERERlRNDWtVijqDmkXYODa6sRt2bW2GnyoXS1gk36vTBpfrPIN0zzDQNr6Sk8Wq8vlopDGtEREREZcSQVvWYI6gFXP8TreLeQY5TLVxo8ByyXAPhmpWMoKSNCLz+B461mIfrAf1N8wIqGaWqENfuZQMA6nMmyFIY1oiIiIiMxJBWNZmrR61V3Du4Xrc/jjefBWFjLz12oeEEtDzxP7SKewfp7qHVsoct6UE2lIUCbo52qOXuaOnmWB2eKEtEREQkU76yEDce5uDC7Qzcz2RQq0rMNUatwZXVyHGqJQU1u4IMhJ37HIrCAggbexxvPgu5TjXR4Mqa8r+ISqh4vFpIDVcoOHlLKQxrRERERAYwpFVt5pxMpO7NrbhW7ykIG3t4pp1F992D0eTCUoQnfFZUxMYeV+sNRt2bf6E6blhX7vJi2PrwNEgiIiIiHXi6Y9Vnzun5bVW5sFPlIsslAMFX1yPq9HzYFuYj27k2btZ+VCqX5RoAO1Vu0SyRds4mWXZlUFgokMjJRfRiWCMiIiIqgSGtejD3ddRUtk5Q2Tii0aXl8My4AABI8YvGsRZzUeDgJZVzzboOpa1T0XT+1UhKei5yCwrhaGeDOl7VJ6Qag2GNiIiI6F8MadVHRVzw2iP9Igpt7OGZcQGFsMHZ8NdxMfRZjeurKQoLEJz0M27U6QtUszFbV+4U9aoF+7rC1qZ6vXa5GNaIiIio2mNIq17MHtSEQFDSJjQ79SFsC/MgAKTW7IhL9Z8pFdRanngfTrl3cKn+aNMtv5IoHq/GKft1Y1gjIiKiaoshrfoxd1CzVWaj+clZqHf9NwDArVqdkeL/CJqd+hC9dvbB1XqDkeUaANes6whO+hlOuXdwrMW8ajdtvxACl+9wvJohDGtERERU7TCkVU/mDmru6RfR9ujr8Mi8AgEbnG3yGi40mAAobHDfOwoNrqxBo0vfwE6VC6WtE27U6YtL9UdXu6AGAHcz85GRq4StjQL1fFws3RyrxbBGRERE1QZDWvVl7qBWL2kzmp2aDTtVDnIca+JI649xz7e19Hi6ZxiOt5iD481nF836aOtc7caoqSvuVQvycYGDHa8mpgvDGhEREVV5DGnVm1mn51fmIOr0HAQnbQIApNbogCOtFiDf0Vf7ExQ2UNmxJ+lialFYa1CLp0Dqw7BGREREVRZDGpkzqLllXEHbo6/DM+MiBBRIaPwyzjd6AVDYmqT+qkp9vBrDmn4Ma0RERFTlMKQRYN6gFnD9D7SInwE7VQ5yHX1xpOVC3K3Z3iR1V3V3MvOQllMAWxsFgnw4E6Q+DGtERERUZTCkUTFzBTUbVR6iTs9FyLWfAAB3arTDkZYLkOdUs9x1VxeXUouvr8bxaoYwrBEREVGlx5BG6swV1Fwzr6Lt0dfhlX4eAgqcbzQRCY1f4mmPRioOaw1q8hRIQxjWiIiIqNJiSKOSzBXU6t74Cy3i/wd7ZRbyHHxwtOVHSK3VqfwNrmaEELgkjVdzt3BrrB/DGhEREVU6DGmkjTmCmo0qHxFnFiD06loAwF2fVjjSahFynf1M0OLqJzUjDxm5StjZKBDky1kxDWFYIyIiokqDIY10MUdQc8lKRtujU+CddgYAcL7hc0ho/CqEDX9Cl5U0Xq2GK+xtOV7NEG5pREREZPUY0kgfcwS12ik70DLuPTgoM5Dn4IVjLebjtl9X0zS4GpNOgeR4NVkY1oiIiMhqMaSRIaYOaorCfESc/QQNrqwGANzzboEjrRchx7m2iVpcfQkh/ptchNdXk4VhjYiIiKwOQxrJYeqg5px9E22PvQGfB/EAgIuhY3GmyesQNvYmanH1djs9D5l5StjbKhDkw/FqcjCsERERkdVgSCO5TB3U/G7vRuvjb8OhIA359h443nwOUmr3NF2DSToFMtjXFXYcryYLwxoRERFZHEMaGcOUQU1RqEST85+j8cVvAAAPvCJwuNUnyHYNMGGLCQAupWYA4CmQxmBYIyIiIothSCNjmTKoOebeQZtjb6LmvSMAgMvBI3G66VQU2jqYsMUElLy+GsOaXAxrREREVOEY0qgsTBnUatw5iDbH3oJT/j0U2Logrvls3Kjb17QNJsmt9Fxk5angYGuDet4cryYXwxoRERFVGIY0KiuTBTVRiMYX/g9Nzn8JBQqR5t4Ih9t8iky3EJO3mf6jfn01jleTj2GNiIiIzI4hjcrDVEHNIe8+Wh+fBr87+wAAV+s9iZMR70Fl52ziFlNJ0pT9vL6aURjWiIiIyGwY0qi8TBXUfO4dR9tjb8A59zaUtk6Ij3wfSfWeMH2DqZRCIXD53/FqDTlezSgMa0RERGRyDGlkCiYJakKgwZVVaHr2E9gIJTLcQnC49adI92hkljZTabfScpGVr4KDnQ0CeX01ozCsERERkckwpJGpmCKo2Reko2XcdNS5tQMAcL1OX8Q1nwWlnasZWky6FJ8CWb+GK2xtyn4tvOqIYY2IiIjKjSGNTMkUQc0z7SzaHnkdbtnJKFTY4WTE20gMHgGU48LZVDbFU/aHcrya0RjWiIiIqMwY0sjUyh3UhEDwtZ8QdXoubAvzkeVcB4dbf4qH3pFmazPpxvFq5cOwRkREREZjSCNzKG9Qs1Vmo/nJD1Dv+u8AgBS/aBxrMRcFDl7maTAZlPIwB9n5Kjja2SCA11czGsMaERERycaQRuZS3qDmnnEZbY9OhkfGZRQqbHE2bBIuNhgHKHhNL0sqPgWS49XKhmGNiIiIDGJII3Mqb1ALuP4HWsTPgJ0qBzmONXGk9ce459vafA0m2YonFwnlKZBlwrBGREREOjGkkbmVJ6jZqPIQeXo+6l/bAABIrdEOR1suRJ5TDTO2mOQqLBS4fCcLANCwlruFW1M5MawRERFRKQxpVBHKE9RcM6+h7bEp8EpLgIAC5xu9gITGLwMKW/M2mmS78TAHOQVF49XqejlbujmVEsMaERERSRjSqKKUJ6jVubkNLU9Mh70yC3kO3jja8iOk1ups3gaT0S7czgBQNAskx6uVDcMaERERMaRRhSprULNR5SPi7AKEJq4FANz1aYkjrRYh19nfzC2msriQ+m9Y8+MpkGXFsEZERFSNMaRRRStrUHPKuYV2RybB5+EpAMCFBhNwNuw1CBv+nLVG+cpCXPl3vFpjhrUy49ZNRERUDTGkkSWUNaj53juKtkdeh1P+PeTbe+Boy49w26+b+RtMZZZ4LwvKQgFPZ3vUcne0dHMqLYY1IiKiaoQhjSylTEFNCIRcXYeo0/NhI5RI82iMg22WINs1sELaTGVXPF6tkZ8bFEZchoE0MawRERFVAwxpZEllCWo2qjw0P/kBgpI3AwCS6/ZDXLNZUNm5mL/BVG7/hTWeAlkeDGtERERVGEMaWVpZgpr6+DQBG5wOfwOXQscC7KGpFLLylLjxIAcAr69WXgxrREREVRBDGlmDsgQ1zfFpnjjc+mPcqdmxYhpMJnExNRMCQG1PJ3g621u6OZUawxoREVEVwpBG1sLooCYEQq5uQNTpubARSjz0aIxDbT5HtmtAhbWZTOO/66uxV628GNaIiIiqAIY0sibGBjUbVT6anfoQwUk/AwCu1+mL481nc3xaJaU+uQiVD8MaERFRJcaQRtbG2KDmlJtaND7tQTwEFDjTZAouNhjH8WmV1N3MPNzLyoeNAgitybBWXgxrRERElRBDGlkjY4Oaz/0TaHtkEpzz7iDf3gNHWi1Caq3OFddgMrmLtzMBAEG+rnCyt7Vwayo/hjUiIqJKhCGNrJVRQU0IBF/7Ec1OzYGNUCLdvQEOtvkcWW5BFdpmMr0LqUWnQDbmlP0mwbBGRERUCTCkkTUzJqjZqPL+HZ+2EQBwo3YvHG8xB0o71wpsMZlDoRC4yOurmRTDGhERkRVjSCNrZ0xQ07x+mgJnm0zGhQYTOD6tirjxMAdZ+So42tmgng8nhzEFG0s3oKJ89dVXCAkJgZOTE1q1aoW9e/fqLBsbGwuFQlHq37lz5zTKbdy4EeHh4XB0dER4eDh++eUXc78MIiKqJvKVhbjxMAcXbmfgfiaDGlknY4Ka772j6L57CHwenkK+vQf2t1+GCw2fY1CrQopngWxQyw22NnxfTaFahLUNGzZg8uTJeO+99xAXF4cuXbqgb9++SEpK0vu88+fPIyUlRfrXsGFD6bEDBw5g2LBhGD16NOLj4zF69GgMHToUhw4dMvfLISKiKowhjSoLY4Ja8NX16Lx/HJzy7+GhR2PEdP0JqbU6VWyDyewu/Du5SCNeX81kFEJU/a+Bdu3aoWXLlli6dKl0X5MmTfD4449j3rx5pcrHxsaie/fuePDgAby8vLTWOWzYMKSnp+Ovv/6S7uvTpw+8vb2xbt06We1KT0+Hp6cn0tLS4OHhYdyLIiKiKoWnO1JlIjeoKQqViDwzH6GJawEAyXX7I67ZLKjsnCu4xWRuBapCvLf5FApUAm/3CYOfh5Olm1SKo50N+kXVtnQzAMjPAVW+Zy0/Px/Hjh1Dr169NO7v1asX9u/fr/e5LVq0QO3atdGzZ0/ExMRoPHbgwIFSdfbu3dtgnUREROrYk0aVjdygZp+fho4HX5CC2pkmk3G05QIGtSoq8W4WClQCns72qOXuaOnmVBlVfoKRu3fvQqVSwc/PT+N+Pz8/3Lp1S+tzateujWXLlqFVq1bIy8vDmjVr0LNnT8TGxqJr164AgFu3bhlVJwDk5eUhLy9Pup2enl7Wl0VERJUce9KoMpIb1NwyE9Hh0Etwy7oGpa0zjrb8CCm1H6n4BlOFKR6v1rCWGxQch2gyVT6sFSu50QghdG5IjRs3RuPGjaXbHTp0QHJyMhYtWiSFNWPrBIB58+bhgw8+KEvziYioimBIo8pKblCrmbofbY9NgUNBOrKda+NA2y+R7hlW8Q2mCnWBU/abRZU/DbJGjRqwtbUt1eOVmppaqmdMn/bt2+PixYvSbX9/f6PrfOedd5CWlib9S05Olr18IiKq3Hi6I1VmsoKaEKif+AM6HnoBDgXpuOfdArFdNjCoVQNZeUpcf5ADgGHN1Kp8WHNwcECrVq3w999/a9z/999/o2PHjrLriYuLQ+3a/w1I7NChQ6k6t2/frrdOR0dHeHh4aPwjIqKqjSGNKjs5QU1RWIDmJz9As1NzYCNUSAoYhH86rkCeUw3LNJoq1MXUTAgA/h5O8HS2t3RzqpRqcRrklClTMHr0aLRu3RodOnTAsmXLkJSUhIkTJwIo6vG6ceMGVq9eDQBYvHgxgoOD0bRpU+Tn5+P777/Hxo0bsXHjRqnOSZMmoWvXrvjoo48waNAg/Prrr9ixYwf++ecfi7xGIiKyLjzdkaoCOUHNPv8h2h19HTXvHoKAAmfCp+Bi6DheP60a+e8USDcLt6TqqRZhbdiwYbh37x5mzZqFlJQUREREYMuWLQgKCgIApKSkaFxzLT8/H2+++SZu3LgBZ2dnNG3aFH/++Sf69esnlenYsSPWr1+P6dOn4/3330doaCg2bNiAdu3aVfjrIyIi68GQRlWFnKDmkpWMjocmwj0zEQW2LjjaaiFu+Xe3TIPJYi6mcryauVSL66xZK15njYio6mBIo6pETlDzfhCPDodehmP+fWQ7++NAu6+R7tHIMg0mi7mXmYcPtyTARgHMeTwSTva2lm6STpXxOmvVomeNiIjIXBjSqKqRE9Rqp+xAm2NvwbYwDw88w3Gw3VfIdaplmQaTRZ27VdSrFlLD1aqDWmXFsEZERFQGDGlUFRkMakIg9MoaRJ75CAoI3PLrhsOtFkFl52qxNpNlnbtVdN3gMH+eJWYODGtERERGYEijqspQUFMUKhF1eh7qX10HALgSPBwnI96FsOHPyepKqSrEhdRMAECYP8ermQM/XURERDIwpFFVZiio2RVkoM2xN+Cf+g8EFDgd/gYuhT7LGR+rucS7WchXFsLdyQ51vZwt3ZwqiWGNiIhID4Y0quoMBTW3zES0P/wK3DMTobR1xtGWHyGl9iOWazBZjYR/x6uF+btDweBuFgxrREREWjCkUXVgKKj534pF6+NTYa/MRLaTPw61/RwPvZparsFkVThezfwY1oiIiNQwpFF1oTeoCYFGF/8P4ec+hwICd31a4nDrxchzqmHRNpP1eJidj5S0XCgUQGNeX81sGNaIiIjAkEbVi76gZqvMQqu491A3ZTuA4olE3oawcbBgi8naFE/ZX8/HBa6OjBTmwjVLRETVGkMaVTf6gppLVjLaH34VnhkXUKiwQ3zU+7gaNMSyDSarxFMgKwbDGhERVUsMaVQd6QtqNe8cQNujU+BQkIZcR18cav0Z7vu2tGyDySqpCgXO3y7qWWvCKfvNimGNiIiqFYY0qq50BjUhUD/xe0SeWQAbocJ9r0gcavMZcp39Ld1kslLX7mUht6AQrg62CPR2sXRzqjSGNSIiqhYY0qg60xXUFIX5aH7yQwQn/QwASAp4DHHNPkChraNlG0xWrXi8WmN/d9jYcMp+c2JYIyKiKo0hjao7XUHNIe8B2h2dhBr3jkLA5t8LXY/lha7JoASOV6swDGtERFQlMaQR6Q5q7hmX0P7Qy3DLTkaBnRuOtFqE235dLd1cqgQycgtw/UEOgKKLYZN5MawREVGVwpBGVERXUPO7vQdtjr0Je2UmMl0CcbDdl8hwb2Dp5lIlUXwKZIC3M9yd7C3cmqqPYY2IiKoEhjSi/2gNagBCL69G5JkFUKAQd31a4VCbJch39LZ0c6kSKQ5rPAWyYjCsERFRpcaQRqRJW1CzFQVodmoOQq79BAC4Wu9JnIj6Hy90TUYpLBQ4L41X4ymQFYFhjYiIKiWGNKLStAU1p4I0tD0yGTXvHYaAAqebvoVL9cdwIhEyWvKDbGTlq+Bkb4NgX1dLN6daYFgjIqJKhSGNSDttQc0jMxEdDr1YNJGIrQuOtlqEW/7Rlm4qVVLFp0A28nOHLafsrxAMa0REVCkwpBHppi2o+d09gLZHXoeDMgNZLnVxsO2XSPdoZOmmUiV27t9TIJtwvFqFYVgjIiKrxpBGpJ+2oBaStAnNT34AG6HEPZ+WONjmM+Q7+lq6qVSJZeUpce1+NgCOV6tIDGtERGSVGNKIDCsV1FrWRfj5L9DkwlIAQFLdAYhr/iEKbTmRCJXPhdsZEAKo7ekELxduTxWFYY2IiKwKQxqRPCWD2pAW/mh5ciaCk34GAJxr9CISGr/CiUTIJP6bsp+9ahWJYY2IiKwCQxqRfCWD2rBmvmh3dDJq346BgA1ORP0PV4OHWrqZVEUUCoEEacp+jlerSAxrRERkUQxpRMYpGdRGRrqh48EJ8H1wAiobRxxptRAptR+xdDOpCrnxMAcZuUo42Nmgfg1O2V+RGNaIiMgiGNKIjFcyqI1uYoPO+0bDI/MK8u09cLDtl7jn28rSzaQq5uzNol61xn7usLO1sXBrqheGNSIiqlD/3959x7dV3/sff8uyJG95r8Rx9h5kkMVeYUOBEig0pbeUXnpL2S2kAwJtGe2FTuht+TFaCoUyUqBAIECYGZC993Ac78R7yLJ0fn84Njh2Ei/pHEmv5+ORR7F8LH+kKo5ePud8D5EG9M6RofadEY06+bPvKbapTA0x2Vo68y+qTRph9pgIQ5uKqiVJ43I4BDLYiDUAQFAQaUDvHRlq3xtSoZOWfl9Ob41qEofrs5l/VVNsttljIgxVN3q1v7JRkjSGWAs6Yg0AEFBEGtA3R4ba9/MLNXv5TYr2NepgymQtm/GYvM5ks8dEmGo7BHJQapySYh0mTxN5iDUAQEAQaUDfHRlqNw3cpZkrbpXd36zSjNlaceIf5IuOM3tMhLHNxYcPgcxlr5oZiDUAQL8i0oD+cWSo3ZKzSdO/+LGijBYVZZ+pL6Y+ysWuEVDNLX5tK229vtq4XLfJ00QmYg0A0C+INKD/HBlqd2R+oamr7pFNfu0fcKFWTX5ARhSHpCGwdpTVyuszlBznUK47xuxxIhKxBgDoEyIN6F9HhtrdKUs0ae1DkqS9g67QmkkLJJvd1BkRGTYdPl9tXE6SbDabydNEJmINANArRBrQ/44MtZ8nvK6xmx6TJO0Y9m1tHPsjiTfNCALDMLS5+HCscQikaYg1AECPEGlAYHQ69DHjC41d1xpqm0bfrO0j/ptQQ9AcqGpUdaNXTnuUhmcmmD1OxCLWAADdQqQBgXNkqH13WI0mf/YLSdKWUT/Q9pE3mjsgIk7bIZAjsxLksEeZPE3kItYAAMdEpAGBdWSoXTs+RrM+vk52v0clmadq68jvmz0iItCmorYl+zkE0kzEGgCgS0QaEHhHhtpVk9I0a/l3FddUotqEIVo59deSjb0aCK7qRq/2VzZKksbmcH01MxFrAIAOiDQgOI4MtbknZGrWFz9UWuVaNTuStHz6Y/I6eKOM4NtyeGGRQalxSorlEhFmItYAAJKINCCYjgy1Kydna8aqO5RV/qla7LFaNv1x1SUMNntMRKj2QyDZq2Y6Yg0AIhyRBgTXkaH29Sm5mrb2p8oteU++KIeWT/+jDqVNMXtMRKjmFr+2ldZKksblEmtmI9YAIEIRaUDwdQ61ATph4wMaVPi6/Da7vpj6qMozZps9JiLYjrJaeX2GkuMcyk2ONXuciEesAUCEIdIAc3QKtakDNXb74xq293kZsmnV5AdUnHOW2WMiwrUt2T8uJ0k2rutnOmINACIEkQaYp6tQG7b3nxqzrfWi1+sn/ESFAy82d0hEPMMwtPnw4iJjWbLfEog1AAhzRBpgrq5Cbei+F3XChl9KkraM/L52D7nW3CEBSQeqGlXd6JXTHqURmQlmjwMRawAQtog0wHxd71F7USdsuF+StHPoddo66iZzhwQOazsEcmRWghx2ru9nBcQaAIQZIg2whuOF2o5h39bGsT+SOC8IFtG+ZD+HQFoGsQYAYYJIA6yjy0MfC17+Sqj9lzaOvZNQg2VUN3q1v7JRkjSG66tZBrEGACGOSAOspatQG1i8WCesWyCpbY8aoQZr2Xigda/aoNQ4uWMdJk+DNsQaAIQoIg2wnq5CLb1ynaatvks2GdqdfxWHPsKS1h+OtYkDOQTSSog1AAgxRBpgTV2FWlxTqWZ+/gPZ/R4VZ52hdRN/RqjBcuo9LdpZVitJmjgg2dxh0AGxBgAhgkgDrKurUIuSdML6X8jVXKkq9xh9MfU3ks1u9qhAJ5uLa+Q3pBx3jDISXWaPg68g1gDA4og0wNq6DDWbTYP3vqic0iXy26K1avKD8kXHmT0q0KUNhw+BnDCAQyCthlgDAIsi0gDrO1qouas2a+LGByVJm8bcppqkkeYOChyFp8WnrSWt11ebODDZ3GHQCbEGABZDpAGh4Wih5vDWaMbKW2T3N6s463TtHHad2aMCR7W1pFZen6HUeKdy3TFmj4MjEGsAYBFEGhA6jhZqkjRu8yOKbzigurg8rZrykGSLMndY4Bg2FB5eBXKAWzYWv7EcYg0ATEakAaHlWKGWemitBu97RZK05oT75XVwcWFYV4vPr03FbUv2J5s7DLpErAGASYg0IPQcK9Rs/hZN3PBL2eRXwcCLVZE+w9xhgePYUVanJq9fiTHRyk9jARwrItYAIMiINCA0HSvUJGlQ4WtKqd6s5ujE1gtfAxb31VUgozgE0pKINQAIEiINCF3HCzWn56DGb/qNJGnHiO/KE5Nu0qRA9/j9RnuscSFs6yLWACDAiDQgtB0v1CQps3ypnN4a1cUP0g5Wf0QI2HOwXnWeFsU67BqemWD2ODgKYg0AAoRIA0Jfd0JNkvIK/yNJKs4+U0aUM8hTAj3XtldtXG6S7FEcAmlVxBoA9DMiDQgP3Q01SUqp2iBJKsw9P4gTAr1jGIbWF1ZJkiYOdJs7DI6JWAOAfkKkAeGjJ6EmSdEt9ZKkFgeHk8H6CqsaVdngldMepVFZXF7CyrhKIwD0UXOLXweqGrW9tFaH6gg1INT1NNQkqSEuT5IU11AUhAmBvmm7EPbonEQ5o8kBK+P/HQDoJSINCD+9CTVJavvrn3z4cEjAyta3rwLJIZBWR6wBQA8RaUB46m2oSdKB3PMkSaN2PCF39eYATgn0TVlNk0prmmSPsmlsDodAWh2xBgDdRKQB4asvoSZJO4b/lyQp2teojPLlAZoS6Lu2vWojMhMU62T5Cqsj1gDgOIg0ILz1NdQkyRcdr4rUqZKklKpNsvm9AZgU6Lu2JfsncAhkSCCnAeAoWN0RCH/9EWptdg2dp/RDqzSw6G2lHVqtwgEXaPuIG9TsTO7XmYHeqqxvVsGhBtlErIUK9qwBwBHYkwZEhv4MNUkqyp2jdeN/IkmKbSrViF1Pa+yW3/XPsEA/WLu/SpI0NCNeiTEOc4dBt7BnDQAOY08aEDn6O9Ta7B5yrZqdyTpx9Y8lSdVJo465/aKNxYqy2TRnXHa3v8e7m0rkNwydNz6nT7Mi8qzZXylJmpyXYvIk6C5iDUDEO1ak+Q1D1Q1eVdR5VFHfrIN1HjV5/TIMQ37DkN9Q639L8vsNxTrscsc5lBzrUHKcU+5Yh9yxDsU47KY8NgCdBSrUJEk2mwzblwcueR2JivI1yW+P6XLzKJtNb28qkaRuBdu7m0r09qYSnd+DuAMkqaLOo/2VjbLZpIkDOQQyVBBrACLWVyOtucWvvRX12l5aq+2ldSquadLBOo8O1jfL5+/7brYYR5SyEmM0OD1eg9PiNTgtToPT45UW75Stv94kAjiugIbaYQdyz5dW3SlJOnH1j9Vij9X68T/RvvwrOm3bFmjdCbavhlpP9sQB0peHQI7ITOAQyBBCrAGIOM0tfm0urtHSnRXaVlKr7aW12lVer2afv8vto2xSSpxTaQlOpcW7FO+yy2azKUpSVJSt/b9lk5q8PlU1eFXV6FV1o1dVDc3y+gw1ef3ad6hB+w416COVt993gitag9PiNDIrUZMGJmtsbhJ74YAACUaoSZJsNn0+9RGN3fp7JdQXKNrXqCnrfq68wje0N//rKhx4UYfNOwaboQtGJanFHqvolnq5miuVWbZUi7ZW6O3ak3XB2HSdQ6ihFzgEMjQRawAixs7SWr265oDe21Kq7aV1nT6f4IrWyKxEjcpK0KC0eKUnOJWe4FJKnFP2qN69oTMMQ41enyrrvTpQ1ai9B+u172C99hxs0IHKBtV5WrSxqEYbi2r06poDio6yaUxOkiblJeuEgckanpnQ6+8N4EtBC7XDDgw4XwcGnC9nc5VO/uzbctduV8bBz5Vx8HOlVG2Q3dekGE+FXE0VSqnapEttUfpT9MV6dNOVGrPtT7o5emH7ff2h5TL9X8uVuj36Jc1IPU1FGhiwuRGeymqaVFTVpCgbq0CGGmINQFjbWVar/6wr1psbirWj7MtAs0kalpGgUdmJhwMtUbnJMf1+SKLNZlOcM1pxzmgNSInV9CGp7Z9rbvGrsLJBuyvqtamoWmv3V6uizqMNB6q14UC1/qF9infaNXlQik4dka6p+alyRrOIL9BTwQ61r2p2JuuD01/VxW+dqGhfoyRp+O5nO21nM/ztgfZoy5WSpJujF+oPLZfp0cOhNi9tmz7N/ElQ5kZ4WXP4EMhR2UmKd/H2P5TYDCMy1jx7/PHH9Zvf/EbFxcUaN26cfve73+mUU07pcttXX31Vf/7zn7V27Vp5PB6NGzdOCxYs0Lnnntu+zTPPPKP/+q//6vS1jY2Nionp+iTiI9XU1Mjtdqu6ulpJSUm9e2AAOmny+vTa2gN66rO92lZS2357228UTxqerplD0pQS7zRxys4Mw1BxdZPW7q/S2v1V2nCgWnWelvbPxzntmjU0TaeOyNCkvGT2uAHdYGaofVVswwHlFyyU01slyaaW6Hi1RMepIXaAqpLHytlcJW90gpqdyXpv4369ujtKTnnVLIe+Njxap00ZH/SZER4Mw9DD72xVaY1H10wfpBMHpx7/i8KUKzpKF0y0xiqq3e2AiEjrF198Ubfeeqsef/xxnXTSSfrLX/6i888/X5s3b9agQYM6bf/xxx/rnHPO0QMPPKDk5GQ9/fTTuvjii7VixQpNnjy5fbukpCRt27atw9d2N9QA9L/yWo+eXb5P/1i+T4fqmyVJ0VE2TcpL1knD0jR9SJrcsdY9qdpmsyk3OVa5ybG6YEKOfH5Du8rr9OnOCn2yo1wVdc16f2uZ3t9aJnesQ7OHpenM0ZkalZXIIiVAF6wSapLUGDdAW0ff1K1tT5mWodf2rlOz3yF7lI1QQ58UVzeptMYje5RN43M5BDLURMSetRkzZmjKlCn685//3H7bmDFj9LWvfU0PPvhgt+5j3Lhxuuqqq3TPPfdIat2zduutt6qqqqrXc7FnDegfm4tq9OSne/T6ugPy+lp/pGUkunTxxBydMyZbCTGh/3spv2FoS3GNPt5Roc92Vqi60dv+ueEZCbpoYo5OGZHBYZLAYVYKtZ5qW/XRHmWTz2+w+iP65K0NxVq8pVTjc5N0/clDzR7HVOxZs6Dm5matWrVKd999d4fb58yZo6VLl3brPvx+v2pra5Wa2nG3cV1dnfLz8+Xz+XTCCSfoF7/4RYc9b0fyeDzyeDztH9fU1PTgkQA40sYD1Xro7a36dGdF+22jsxN16QkDNGtoWlgdJhhls2lcrlvjct363ilDtW5/lT7cXqZPd1ZoZ3mdfvf+Dj352R6dOzZb50/IVmYie/kRucIh1NoCre1jqXvXYQO+yjCML1eBHMQqkKEo7GOtoqJCPp9PWVlZHW7PyspSSUlJt+7jkUceUX19vebOndt+2+jRo/XMM89owoQJqqmp0e9//3uddNJJWrdunUaMGNHl/Tz44IO67777ev9gAEiSiqsb9Zt3tmnhmgMyjNZz0U4anq5LJw3QqOxEs8cLOHuUTVPyUzQlP0XXnzxUizeX6q2NxSqv9ejl1YV6dU2hZgxJ0yWTcjWeVb8QYcIp1KSeXYcNOFJhVaMq6prlsNs0LoejuEJR2MdamyPP5zAMo1vnePzzn//UggUL9NprrykzM7P99pkzZ2rmzJntH5900kmaMmWK/vjHP+oPf/hDl/c1f/583X777e0f19TUKC8vr6cPBYhY9Z4W/eWjXfrrJ7vV5G29JtrpIzP0zZn5ykqKzD1J7liHvj51oC6bPECf7z2k/6wv0vrCai3bfVDLdh/U+NwkfWP6IE0Y4Oa8NoS9cAu1NgQbemttQZUkaWxOklxcwzMkhX2spaeny263d9qLVlZW1mlv25FefPFFXX/99XrppZd09tlnH3PbqKgonXjiidqxY8dRt3G5XHK5XN0fHoAkyec39PKq/frfd7arvK71UOKxOUm6/uQhGpkV/nvSusMeZdOsoWmaNTRNBYca9Ma6Ir23pVQbi2r0039v1LjcJF194iBNGki0ITyFa6i1IdjQUxwCGR7CPtacTqemTp2qxYsX67LLLmu/ffHixbr00kuP+nX//Oc/9Z3vfEf//Oc/deGFFx73+xiGobVr12rChAn9MjeAVpuLanTHS2u1pbh1Cf7spBh9e/ZgzR6WRnQcxaDUOP3gjOGaOy1Pr6wu1DubSrSpqEY/f22jxmQn6urpgzQ5L5nnD2Ej3EOtDcGGnth3qEGVDV65oqM0JptDIENV2MeaJN1+++2aN2+epk2bplmzZumvf/2rCgoKdOONN0pqPTzxwIED+vvf/y6pNdS+9a1v6fe//71mzpzZvlcuNjZWbnfr+R/33XefZs6cqREjRqimpkZ/+MMftHbtWj322GPmPEggzLT4/Hr8w136w/s71OI3FO+06+oTB+nCiTly2FnxsDsyEl268bRhunLqQL2yulCLNpVoS0mt7n19k8bkJOn6k4ZExDl+CG+hHGpS60qvPVntsW07f/gv5o0+WnP4EMhxuW5WCg5hERFrV111lQ4ePKj7779fxcXFGj9+vN566y3l5+dLkoqLi1VQUNC+/V/+8he1tLToBz/4gX7wgx+0337dddfpmWeekSRVVVXpe9/7nkpKSuR2uzV58mR9/PHHmj59elAfGxCOtpfU6vaX1mrjgdYVU2cMSdUPzhiulDhrXcQ6VKQluPS9U4fp61Nb97Qt2liiLcU1uvPldTptZIa+NSuf1SMRkkI91CTpvPE9X0acPWo4Hr9haF1hlSRpcl6yqbOgbyLiOmtWxXXWgI48Xp/++slu/emDnfK0+BXntOt7pwzVmaMzOWSvHx2sa714+Adby2RIctqjdOkJufr61IGKc0bE7/AQBsIh1IBA2VVepz8t2akYR5R+ccl4RXNEiiSuswYAvdLc4temomrd/8ZmrdlfJUmaONCtW84awR6fAEhLcOnWs0fqoom5euqzPdpwoFovrSrU4s2lunZGvs4ZmxVW16hD+CHUgGNbe/jf0gkD3IRaiCPWAJimucWv8jqP3t9Sql8v2qY6T4uc9ih9e/ZgXTgxhzdfATY8M0G/+tp4rdhzSE9/tkdF1U167MOdenNDkW46YwTns8GSCDXg2Pz+rx4CySqQoY5YAxB0bZF2qM6jf68t0tOf7ZHfaI2HO84ZqYEpcWaPGDFsNptmDk3T1PwUvb2xRP/8vEB7DzboRy+v0/kTcvStmfmKd/FPBayBUAOOb3tZrWqbWhTntHN5mzDAv8AAgqYt0irrm+Xx+vXYhzv1wdYySdLZYzL1P6cPZ6VHkzjsUbpkUq5OG5mhpz7bow+2lumtDcVavuugbjh1qE7iUgkwGaEGdM/KfYevrZaXwiHtYYBYAxBwX400w2hd4OLBt7dqW2mtomzS9ScP0cUTc4kBC3DHOnTb2SN15uhMPb5kp4qqm/Twoq2alp+iG08bpqwkziFE8BFqQPd4vD5tKKyWJE0bzCGQ4YBfYQMImOYWvw5UNWp7aa0O1bWG2raSWt3+r3XaVlqrBFe07rtkvC6ZNIBQs5hJA5P1x29M0TdOzFN0lE0r91XqB8+v1sI1hfL5WUQYwUOoAd23/kC1mn1+ZSS4lJ/KKQXhgD1rAPrdkXvS2nywtUx/WrJDXp+hQalx+tmFY5TjjjVvUByTMzpK18zI1ykjM/T4kp3aWFSjpz7bq+W7D+nWs0fw/x0CjlADemblvkOSpGn5KfwSNEywZw1Av+lqT1qbV1cX6rfvbZfXZ2jGkFT95usTebMfIvJS4vTAZRN00xnDFeuwa3NxjX74zzV6a0OxuFQnAoVQA3qmqqFZO0rrJElT8zkEMlywZw1Anx1tT5okGYah5z8v0Atf7JckXTFlgL41azBvukKMzWbTueOydUJesn7//g5tOFCtP3+0S8t3H9TNZ41QeoLL7BERRgg1oOdWF1TJkDQkPV5p/EwOG+xZA9Brx9qTJrWG2pOf7mkPtW/NzNe3Zw/hTVcIy0qK0S+/Nl43nDJETnuU1uyv0k3Pr9YHW8vYy4Z+QagBPWcYRodDIBE+iDUAPXa8SJMkn9/Qn5bs1GvriiRJ/33qUF05LS/IkyIQomw2XTJpgH539QkamZWg+maffvvedj20aKvqmlrMHg8hjFADeqeoqlHF1U2KjrLphLxks8dBPyLWAHRbdyJNklp8fj26eLve3VyqKJt0y5kjdNHE3OAOi4DLS4nTr6+YpG/OzJc9yqaluw7q5hfXaEtxjdmjIQQRakDvtV1bbVxukuKcnOUUTog1AMfV3Uhr2/ahRVv18Y5y2aNs+tG5o3X22KzgDYugskfZdNW0PP3v1ycpxx2j8lqP7n51vV5auV9+DotENxFqQO/5/IZWFbTG2rT8VJOnQX8j1gAcVU8irW37X7y5WSv2HJLDbtNPLxijk4enB2dYmGp4ZoJ+d9UJOm1khvyG9Pfl+3Tv65tUWd9s9miwOEIN6JvtpbWqbWpRvNOu0dmJZo+DfkasAeikp5Emtf5m79fvbNXa/VWKcURpwcXjdOJgfsMXSeKc0brjnJG65cwRckVHae3+Kt38whqtPvwbX+BIhBrQd22HQE4elKJoO2/tww3/jwJo15tIk1pXoXpsyc72PWr3XDROEwcmB3RWWJPNZtPZY7P027knaHBanKoavbr39U3629K98vk5LBJfItSAvmvy+rThQJUkVoEMV8QagF5HWpu/LdurxVtaFxP50bmjNWGAOzCDImTkpcbpf6+cpPPHZ0uSXl5dqHte36jqRq/Jk8EKCDWgf6w/UC2vz1BGokuDUuPMHgcBQKwBEayvkSZJr64u1CurD0iSfnjGCM0amtbPUyJUuaLt+p/Th+vH545SjCNK6wurdeuLa7S9tNbs0WAiQg3oPyv3tl5b7cT8FNn4exSWiDUgAvVHpEnSe1tK9fTSvZKk/5o9mFUf0aVTRmTokStP0IDkWFXUNeuuV9Zr0cYSLqIdgQg1oP9UNTRrZ1mdJGkqh0CGLWINiCD9FWmStGLPQf3xgx2SpMsnD9DlUwb205QIR4NS4/To3EmaNTRNLX5Dj324U3/4YIc8LT6zR0OQEGpA/1q1r1KGpGEZ8UqNd5k9DgKEWAMiQH9GmiRtOFCthxdtld+Qzh6TqW/PHtwvcyK8xTmjNf/80bpu1mBF2aT3tpTprlfWq7SmyezREGCEGtC/DMPQF/u4tlokINaAMNbfkSZJ+w816JdvbpbXZ2jGkFTddMYIjpNHt9lsNn196kDdf8l4JcVEa1d5vW7711ptKKwyezQECKEG9L/9lY0qrWlSdJRNkwayqFc4I9aAMBSISJOkOk+LfvnmZjU0+zQ2J0k/OneU7FG86ULPTcpL1u+umqzhmQmqbWrRz1/fpEUbS8weC/2MUAMCY/nug5KkiQOTFeuMNnkaBBKxBoSRQEWa1HrR69+8s01F1U3KSHRp/vmj5Yq29983QMTJSHTpocsn6NQR6fIdPo/tLx/t4npsYYJQAwLD0+LT6oLWQyBnDuUQyHBHrAFhIJCR1ubZ5fu0uqBSzugo/fSCMUqOc/b/N0HEcUXbdeecUZo3M1+S9J8Nxbr39Y2qbeJ6bKGMUAMCZ93+Knla/EpPcGp4RoLZ4yDAiDUghAUj0iTpo+3lemV1oSTpljNHaBj/OKAf2Ww2zZ2Wp59cMEYxjiitK6zWHS+t0/7KBrNHQy8QakBgLd/Tem21GUPSOGc8AhBrQAgKVqRJ0s6yOv3h/dYl+q+YMlCnjswI3DdDRJs1NE2/vmKSMhNdKq5u0p0vrdOqw6udITQQakBgldY0aU9FvaJs0omDOQQyEhBrQAgJZqRJrRfc/NVbW9Ts82tqfkr7oWpAoAxJj9cjV07S2JwkNTT7dP9/NumtDcVmj4VuINSAwGtbWGRsTpLcsQ6Tp0EwEGtACAh2pEmS1+fXQ4u2qqLOowHJsbpzDis/IjiS45z65dfG66zRmfIb0p8/2qWnPtsjfzBe+OgVQg0IvBafX1/saz0EcubQNJOnQbCw1idgYc0tfpXXeVRZH5xA+6onPtmtTUU1inPa9dMLxyjBxY8LBI/DHqVbzhqhHHeM/rGiQAvXHFBZTZNuO2ckq5BaDKEGBMfGohrVe3xyxzo0OjvJ7HEQJOxZAyzIjD1pX/XhtjK9vbFENkl3nDNKeSlxwR0AUOvCI1edOEh3nDNS0VE2fbbroH66cKOqG1kp0ioINSB42g6BnD44lSNdIgixBliI2ZEmSSXVTXr8w12SpLkn5mn6EE5ghrlOH5Wp+y8drwRXtLaV1urOl9apkJUiTUeoAcFzqN6j7aW1ksS/yxGGWAMswAqRJrUeD/+/725To9enMTlJ+saJg8wZBDjChAFu/ebrE5WdFKOSmib96OX12nig2uyxIhahBgTXij2HZEgakZmg9ASX2eMgiIg1wERWibQ2z39eoG2ltYp32nXnOSM5zAKWMjAlTv975SSNykpUnadFP39toz7dWWH2WBGHUAOCy+839Pnha6vNYmGRiEOsASawWqRJ0rrCKr28qvXC1zedOUKZSTEmTwR05o516FeXjdesoWlq8Rv69aKtemNdkdljRQxCDQi+baW1qmr0Ks5p14QBbrPHQZARa0AQWTHSJKm60atHD78BO2dslk4enm72SMBRuaLtuuu80bpgQo4MSX/9ZLf+vmyvDKv8hQpThBpgjraFRablpyrazlv3SMNa3EAQmLkE//EYhqE/frBDh+qbNSA5Vt87ZajZIwHHZY+y6cZThyo13ql/LN+nl1YV6mB9s354xnDezAQAoQaYo7bJq41FrefnzhzKwiKRiFgDAsjKkdbmrQ3FWrHnkKKjbPrxuaMU4+AaVggNNptNV03LU2qcQ39aslMfbC1TdaNXd583mtdxPyLUAPN8vveQ/IaUnxanHHes2ePABPz6EQgAqx7ueKS9FfV68rM9kqT/OmmwhmYkmDwR0HPnjM3WTy8YK2d0lFbtq9RPFm7gWmz9hFADzGMYhlbsbl1YZOYQFhaJVMQa0I9CJdKk1ll/8+42eX2GpuWn6OKJuWaPBPTa9CGp+tXXxivRFa0dZXX68cvrVFbTZPZYIY1QA8y1s7xO5XUeuaKjNDkv2exxYBJiDegHoRRpbV74okAFhxqUHOfQLWeNkI03YQhxo7OT9PDXJyoj0aWi6ibd9ep67T/ExbN7g1ADzPfpjtZLk0zLT5GLQ7sjFrEG9EEoRpok7Syr0yurW5fp/5/Thys5zmnyRED/yEuJ06+vmKi8lFhV1DXrrlfXa3tprdljhRRCDTBfZX2zNhxeWOQkVmiOaMQa0AuhGmmS1OLz648f7JDfaP0HgAtsItykJ7j04OUTNSIzQbVNLfrZvzdqXWGV2WOFBEINsIaluw/KMKThmQksLBLhiDWgB0I50tq8suaAdlfUK9EVrf8+lWX6EZ7csQ798mvjNWmgW41enxa8vknLDl+rCF0j1ABraPH526+tdgp71SIesQZ0QzhEmiTtP9SgFz4vkCTdcOpQpXD4I8JYnDNa9148TrOGpqnFb+iht7fovc2lZo9lSYQaYB1rC6tU52lRcqxD43LdZo8DkxFrwDGES6RJks9v6A8f7FCLv3X1x9NHZpg9EhBwDnuU7jpvtM4ZkyW/If3+gx3695oDZo9lKYQaYC1tC4vMHpYmexR/FyMdF8UGuhAKF7PuqTc3FGlrSa1iHXb9z+nDWf0REcMeZdMPzxyuhJhoLVxzQE9+tkeNXp+uPjEv4v8eEGqAtRQcatC+Qw2yR9k0k3PKIWIN6CAcI02SSqqb9Pdl+yS1Xvw6I9Fl8kRAcNlsNn3npCFKcEXr2eX79PznBWpo9uk7Jw2O2GAj1ADr+XRnuSTphLxkJcY4TJ4GVkCsAQrfSJMkwzD0pyU75Gnxa3xuks4dl232SIBp5k7LU4zDric+2a1/rz2gJq9P3z99WMRFCqEGWE+dp0VrCqoksbAIvkSsIaKFc6S1WbylVOsKq+W0R+mHZ47gDRki3iWTchXriNIfP9ipRZtK1NTi061njYyYc0MINcCaVuw+qBa/obyUWA1KjTN7HFgEsYaIFAmRJrVeVPOpT/dIkq6dMUi5yVyrBZCkc8ZmyxVt16PvbdeH28rl8fr1o3NHyWEP73W3CDXAmvx+Q5/tal1Y5OTh6RF7eDY6C+9/lYAjhNPqjt3xzLK9qm/2aXhGgi49YYDZ4wCWcurIDM0/f7Sio2xatvugfvnmZjV5fWaPFTCEGmBdm4prVNngVbzTrhPyUsweBxZCrCEiRFqkSdLW4hp9sLVMknTjacMi5hAvoCdmDEnTvRePkys6SqsLqrTgjU1qaG4xe6x+R6gB1vbpzta9ajOGpskZzdtzfIlXA8JaJEaa1HpNtb98vFuSdPaYTI3KTjR5IsC6TshL1v2Xjlec065NRTW657VNqvOET7ARaoC1ldY0aXtprWw26aRhLNePjog1hKVIjbQ2720p1c7yOsU57frWrMFmjwNY3ticJP3y0vFKcEVrW2mtfv7vjapt8po9Vp8RaoD1fXZ4r9q4nCSlxnNpHXRErCGsRHqkSVJtk1d/W7ZXknTN9EFKiXOaOxAQIkZkJepXXxuvpJho7Syv00//vVHVjaEbbIQaYH1NXp8+33tIknTy8AyTp4EVEWsIC0Tal55bUaDaphblpcbpwgk5Zo8DhJShGQl64LIJSo5zaE9FveYv3KDK+mazx+oxQg0IDct3H5Snxa/MRJdGZCWYPQ4siFhDSCPSOtpTUae3NxZLkv771KGKDvNlyIFAyE+L10OXTVRavFP7DzVo/sINqqjzmD1WtxFqQGjw+Q19vKNcknTGqEz+nqJLvJNDSCLSOjOM1kVF/IZ00vB0TRqYbPZIQMgakBKrhy6fqMxElw5UNWr+qxtUVtNk9ljHRagBoWPt/ipVNniVGBOtqfks14+uEWsIKUTa0X28o0KbimrkjI7Sd04abPY4QMjLdsfowcsmKMcdo5KaJt29cINKqq0bbIQaEDoMw9CSba2X1zlleLocHAmDo+CVgZBApB1bY7NPT322R5I0d+pAZSbGmDwREB4yk1qDbUByrMprPZq/cL2KqhrNHqsTQg0ILTvK6nSgqlFOe5RmD0s3exxYGLEGSyPSuudfK/frUH2zspNidNnkgWaPA4SVtASXHrhsgvJSYlVR16z5CzfoQKV1go1QA0JP21616UNSFe+KNnkaWBmxBksi0rqvpLpJ/157QJJ0wylD5IzmrzXQ31LjnfrVZRM0KDVOh+qbNX/heu2vbDB7LEINCEFFVY3aWtJ6EezTRrJcP46Nd3WwFCKt5/6xYp9a/IYm5yXrxMGpZo8DhK2UOKceuGyCBqfFqbLBq58s3KB9B+tNm4dQA0JT2161iQOSlZ7ARbBxbMQaLIFI651d5XX6aHvrsr/fnj1YNt6oAQHljnXol1+boKHp8apq8Oqn/96ovRXBDzZCDQhNVQ3NWl1QKUk6c3SmydMgFBBrMBWR1jd/W7pXknT6yAwNzeBimkAwtAbbeA3LiFd1o1c//fcG7amoC9r3J9SA0PXxjgr5DWlYRrwGpcaZPQ5CALEGUxBpfbd2f5XW7K9SdJRN187MN3scIKIkxjj0y0snaERmgmqaWvTThRu1qzzwwUaoAaGryevTst0Vklovgg10B7GGoCLS+offMNr3qp0/PlvZSSzVDwRbQky0fnHpeI3KSlStp0U/+3dgg41QA0Lbst0H1eT1KzPRpTE5SWaPgxBBrCEoiLT+9dnOCu0sr1Osw66rThxk9jhAxIp3Reu+S8ZpVFai6g4H286y/g82Qg0IbT6/oY8Pn2N+xqhM/v6i24g1BBSR1v+8Pr+eXb5PknT5lAFyxzpMngiIbPGuaN1/6TiNzj4cbK9t6NdgI9SA0Ldmf6WqGr1KjInW1PwUs8dBCCHWEBBEWuC8u6lExdVNSo5z6NJJA8weB4CkOGfrHrYx2Ymq9/j0s9c2aEdpbZ/vl1ADQp9hGO3L9Z8yPEMOO2+/0X28WtCviLTAamz26YUv9kuSvnHiIMU67SZPBKBNnDNaCy4ZpzE5Sar3+PTz1zZqex+CjVADwsPWkloVVTXJaY/S7GFpZo+DEEOsoV8QacHx77UHVNXoVY47RnPGZpk9DoAjxDmjteDisRqbk6T6Zp/u6WWwEWpAeDAMQ+9sKpEknTQ8TfGuaJMnQqgh1tAnRFrwVDU0a+GaA5KkeTPzFc1hFIAlxTmjde/FYzUutzXYerqHjVADwse20lrtO9Qgh93Gcv3oFd7toVeItOB7ceV+NXp9Gp6ZoJOGp5s9DoBjiHNG696LxmlcbpIaehBshBoQPlr3qpVKkmYPS1diDAuCoeeCGmtVVVUqLy+XwTv7kEWkmaO0pkmLNrYeRvHt2YN58waEgFinvUfBRqgB4WVHWZ32HqxXdBR71dB7fY41n8+njRs3auXKlSorK+v0eY/HowULFigvL09paWnKzs5WQkKCrrjiCm3atKmv377bHn/8cQ0ZMkQxMTGaOnWqPvnkk2Nu/9FHH2nq1KmKiYnR0KFD9X//93+dtnnllVc0duxYuVwujR07VgsXLgzU+KYj0sz10qpCtfgNTRro1qSByWaPA6Cbugq2bSWdg41QA8LLV89VmzU0jcvsoNd6HWuGYei+++5Tenq6Jk2apBkzZignJ0ezZs3SihUrJEler1fnn3++fvGLX6ioqEiGYcgwDDU2Nurf//63pk+frvfff7/fHszRvPjii7r11lv105/+VGvWrNEpp5yi888/XwUFBV1uv2fPHl1wwQU65ZRTtGbNGv3kJz/RzTffrFdeeaV9m2XLlumqq67SvHnztG7dOs2bN09z585tf+zhgkgzX1ltk97f0noYxTemcwFsINQcGWz3vN4x2Ag1IPzsKq/T7op62aNsOnM0e9XQezajl8ckfve739XTTz8tSZ0Oa0xOTtby5cv1xBNP6JFHHpEkpaamasSIEWppadHmzZvV2NgoScrMzNS2bdvkdrv78jiOacaMGZoyZYr+/Oc/t982ZswYfe1rX9ODDz7Yafu77rpLr7/+urZs2dJ+24033qh169Zp2bJlkqSrrrpKNTU1evvtt9u3Oe+885SSkqJ//vOf3ZqrpqZGbrdb1dXVSkpK6u3DC4jmFr/K6zyqrCfQzPb4hzv19sYSTRzg1q8um2D2OAB6qbHZp/v+s0mbimoU57Tr/kvGq6i6kVADwtBjS3ZqZ3mdThqWrq9PHWj2ODjMFR2lCybmmD2GpO53QK/WD/3oo4/01FNPyWazKSYmRhdeeKEGDx6svXv36s0331R1dbV+97vf6cUXX5TD4dBjjz2m66+/XrbD/wA1Njbqnnvu0SOPPKLy8nI988wzuuWWW3r3SI+jublZq1at0t13393h9jlz5mjp0qVdfs2yZcs0Z86cDrede+65evLJJ+X1euVwOLRs2TLddtttnbb53e9+16/zB4NhGGr0+iS1RlpFXbOqGog0KzhY59G7hw+juHzKQDUd/v8JQOix2aS7zh2tB9/eoi0ltZr/6jp5/a2fmz44VRdPypHX5zd3SAB9tqeiXjvL6xRlk04ZkS5PC/92W4ehhuYWxTrs7V1idb2KtbY9apmZmfrggw80ZsyY9s9t3bpVZ555pp544gn5/X796Ec/0ne/+90OXx8bG6vf/OY32rhxo9555x29+eabAYu1iooK+Xw+ZWV1vCZVVlaWSkpKuvyakpKSLrdvaWlRRUWFcnJyjrrN0e5Taj1/z+PxtH9cU1PT04cTEI1en8be847ZY+A4FrwRvHM8AQSe9ytd9vneQ/p87yHzhgHQ7/yG9NCirWaPgSP9a50233+u4pyhcc27Xp2ztmzZMtlsNt12220dQk2SRo8erdtuu00+X+tvEebNm3fU+7nuuuskKSgLjRxZz4ZhHLOou9r+yNt7ep8PPvig3G53+5+8vLxuzw8AAAAgsvQqKYuKiiRJs2bN6vLzX719+PDhR72fESNGSJIOHQrcbxPT09Nlt9s77fEqKyvrtGesTXZ2dpfbR0dHKy0t7ZjbHO0+JWn+/Pm6/fbb2z+uqamxRLDFOuxa+bOz9N7mzqt5wjxvrCvWZ7sqlJ8WpxtPHRoyu+sBHN2agkq9uLJQknTmqEwVVTVoa2mdYh1R+tmFYzUyK9HkCQH0xS/f3Kx1hdU6c3Smvn/aMLPHiXjRdpvSE1xKiXMoKurL91GxDruJU/VMr2Ktvr5eNptNqampXX4+OTm5/b9dLtdR7ycmJkZS63llgeJ0OjV16lQtXrxYl112Wfvtixcv1qWXXtrl18yaNUtvvPFGh9veffddTZs2TQ6Ho32bxYsXdzhv7d1339Xs2bOPOovL5Trm82EWm82mOGe0XNGh88INdzWNXn2+96Ak6fxxOYpxhMauegBHt3LfIf3rcKidNy5b3z99mJpb/FrwRuuiI796a4vuu2ScRmdba8EpAN2zraRW6wqrFWWTvnHiIMWEUBCEm2i7TRmJLqXGOTtEWijq03XWjvabfqvtAbj99tv1//7f/9NTTz2lLVu26LbbblNBQYFuvPFGSa17vL71rW+1b3/jjTdq3759uv3227VlyxY99dRTevLJJ3XnnXe2b3PLLbfo3Xff1cMPP6ytW7fq4Ycf1nvvvadbb7012A8PYWjJtjJ5fYby0+I0MivB7HEA9NHKfYf0/IoCGfoy1KJsNsU47Fpw8TiNP7ys/72vb9LWEmuczwygZ174ovWSUGeOzlS2O8bkaSJTtN2mnOQYjcpKVHqCK+RDTeqHi2KHgquuukq/+93vdP/99+uEE07Qxx9/rLfeekv5+fmSpOLi4g7XXBsyZIjeeustffjhhzrhhBP0i1/8Qn/4wx90xRVXtG8ze/ZsvfDCC3r66ac1ceJEPfPMM3rxxRc1Y8aMoD8+hJfaJq8+21UhSTp3bLblfvkBoGeOFmptYhx23UuwASFta0mNVu6rVJRNunKq+ae4RJpwjLQ2vbrOWlRUlGw2mzZs2KCxY8d2+vymTZs0YcIE2Wy29oVGutLd7cKVla6z1tDconc2lpo6A1q9sa5IH2wr06DUON161ghiDQhhxwu1r2ry+nTfG5u08fB12DgkEggNhmFo/sIN2lRUo3PGZOnms0aYPVLECOXDHQN6nbU2jz/+uDIzO1+Vvazsy4Uq7r///qN+/Ve3AyDVeVr06eG9anPGZhFqQAjrSahJX+5hawu2e17bpPsvJdgAq1u5r1KbimrktEfpG9MHmT1ORAjlSOupPu1Z6w9ty92zZ409a5DeXF+k97aWaWBKrG4/eySxBoSonobaVzV5fbr/P5u14UC1Yh123X/JOI3OIdgAK/L5Dd3ywhrtO9SgK6YM0LdnDzF7pLAWTpHW3Q7o9TlrhmH0yx8ArRqaW/TJzra9apyrBoSqvoSa1LqH7Z6LxmriALcavT7d8/ombSnmHDbAij7cVqZ9hxoU77Lr61M4Vy1QwvmctOPp1WGQS5Ys6e85gIi3dNdBeVr8ynXHaHwuv0UHQlFfQ61NjMOun180Vr94c7PWF1br3tc3acEl4zSWPWyAZTS3+PWPFa0L1M2dmqeEGC6z09/CaU9ab/XqVXXaaaf19xxARGvx+fXxjnJJ0hmjM9mrBoSg/gq1NjEOu35+4dj2i+ze+/pGLbh4nMbluvtvaAC99uaGIlXUeZSe4NSFE3PMHiesEGlfioil+wGrW7WvUrVNLUqOdWhyXorZ4wDoof4OtTZte9hOyEtWk7ftAtrVfR8YQJ/UeVr00uGL3F87PV+uaC6A3R8i+XDHoyHWAJP5DUNLtreujHrqyAzZ+cEEhJRAhVobV7RdP7twTIdg23CAYAPM9OrqQtV6WjQoNU5njO68Mjp6hkg7OmINMNmW4hqV1ngU44jSrKFpZo8DoAcCHWpt2oJt8uFgu++NTVpfWNXv3wfA8R2s8+i1dUWSpOtm5fNL1j4g0o6PWANMtmRb6161WUPTFePgMAogVAQr1Nq0BttYTRmUIk+LX/f9Z7PW7q8K2PcD0LV/fl6g5ha/xuQk6cTBqWaPE5KItO4j1gATFRxq0K7yekXZpFNHpJs9DoBuCnaotXFGR+mnF4zRtPwUNbf49Yv/bNbqfZUB/74AWu2vbNDiLa3Xpf327MEsCNZDRFrPEWuAidr2qk0ZlKLkOKfJ0wDoDrNCrY0zOko/uWCMZgxJVbPPr1++tVkr9x4K2vcHItmzy/bJb0gzhqRyKY0eINJ6j1gDTHKwzqN1h885OWMUJycDocDsUGvjsEfprvNGa9bQNHl9hn711hZ9vudg0OcAIsnGA9VatvugomzSt2YNNnuckECk9R2xBpjko+3lMgxpdHaicpNjzR4HwHFYJdTaOOxR+vG5o3TSsDS1+A09+PZWLdtNsAGB4PMb+svHuyRJ547L1qDUOJMnsjYirf8Qa4AJ6j0tWrGn9bCl09mrBlie1UKtTbQ9SnfOGaVTRqSrxW/o4UVb9dnOCrPHAsLOoo3F2nuwQYmuaH1zRr7Z41gWkdb/iDXABEt3VajZ59eA5FiNzEwwexwAx2DVUGsTbY/SHeeM0mkjM+TzG/r1O1v10fZys8cCwkZ1o1fPrtgnSfrmzHwlxTpMnsh6iLTAiTZ7ACDSeH1+fXL4N99njMpgJSnAwqweam3sUTbddvZI2aNs+mBrmR55d5tafH6dNSbL7NGAkPfssr2q9/g0ND1e547LNnscS4m225SR6FJqnJNACxBiDQiyVfsqVdvUouQ4h07ISzF7HABHESqh1sYeZdMtZ42QI8qmdzaX6vfv75DXZ+i88by5BHprR2mt3t3culT/904dygWwDyPSgodYA4LIbxjty/WfNiKDH/qARYVaqLWJstn0gzOGy2GP0n82FOuxD3eqxe/XRRNzzR4NCDl+w9BfPt4tQ9LpIzM0Ltdt9kimI9KCj1gDgmhbSa3Kaj2KcURp5tA0s8cB0IVQDbU2NptN3zt1qKLtUfr32gP6y8e75fX5ddnkgWaPBoSUD7aWaVtprWIddn179mCzxzEVkWYeYg0Iok8Pn6s2fXCaYhx2k6cBcKRQD7U2NptN3zlpsJzRUfrXyv166rO9avYZumpantmjASGh3tOivy3dK0m66sQ8pSW4zB3IJESa+Yg1IEgq6jzaUlwjSTp5eLrJ0wA4UriEWhubzaZ5M/PltNv0jxUF+sfyffK2+HXtjEEsbAQcxz8/L1BVo1cDkmN1yaTIO4yYSLMOYg0IkqW7KmSo9SLYGYmR+Rs6wKrCLdS+6qoTB8lhj9LTS/fqxZX71eT16fqThxBswFEUHGrQfzYUS5K+d8pQOeyRc6UrIs16iDUgCJpb/O0XwWavGmAt4RxqbS6fMlDO6Cj95ePdem1dkZq8Pn3/9OEscgQcwTAM/fXjXfL5Dc0Ykqop+ZGxajORZl3EGhAEqwsq1dDsU2q8U2Oyk8weB8BhkRBqbS6amKuYaLv+uGSH3tlcqqYWv249a4SiI2ivAXA8S7aVa11htRx2m7578lCzxwk4Is36iDUgwAzDaF9Y5ORh6fwwBCwikkKtzdljs+RyROmRxdv10fZyNXl9uuu80RF1mBdwNJUNzfp/n+yW1Hr4cLY7xuSJAodICx38dAYCbO/BBh2oapTDbtP0IalmjwNAkRlqbU4ZkaGfXjBGDrtNK/Yc0v3/2awmr8/ssQDT/eXj3ar1tGhoeryumDzA7HECItpuU05yjEZlJSo9wUWohQBiDQiwT3eWS5KmDEpRvIud2YDZIjnU2pw4OFX3XjxOMY4ord1fpXtf36R6T4vZYwGmWbarQp/trFCUTbo5DA8PJtJCV3i9EgGLqW70au3+KknSSSwsApiOUPvSpIHJ+sUl4xXvtGtzcY1+9u+Nqm70mj0WEHR1TS3680e7JElXTBmoYRkJJk/Uf4i00EesAQG0fPdB+Q1pcFq88lLizB4HiGiEWmejc5L0q8smKCkmWjvL63T3q+tVXusxeywgqJ78bLcqG1qvqXb1iYPMHqdfEGnhg1gDAsTnN7Rs90FJ0ikj2KsGmIlQO7phGQl66IqJSk9wqbCyUT9+Zb0KKxvMHgsIitUFlXpvS5lsaj380Rkd2m+NibTwE9qvSMDCNhyoVnWjV4kx0Zo4wG32OEDEItSOLy8lTg9fMUEDkmNVUefR3a9u0M6yOrPHAgKqsdmnx5bslCRdODFHY3NC99I6RFr4ItaAAGlbWGTW0LSwO1EZCBWEWvdlJsbo4SsmalhGvKobvfrJwg3acKDa7LGAgPn78r0qq/UoM9Glb80cbPY4vUKkhT/eQQIBUFTVqF3l9YqytcYagOAj1HrOHevQA5dN0PjcJDV6fbr39Y1aseeg2WMB/W5zcY3eXF8sSbrpjOGKddpNnqhniLTIQawBAdB2EewJA5KVHOc0eRog8hBqvRfnjNaCS8ZpxpBUeX2GHnhriz7YWmb2WEC/afL69If3d8iQdPaYTE0elGL2SN1GpEUeYg3oZ43NPq3aVymJhUUAMxBqfeeKtmv++WN05qhM+Q3pt+9t16urC2UYhtmjAX321Gd7dKCqUSlxDl1/0lCzx+kWIi1ycYVeoJ+tLqhUs8+v7KQYDU2PN3scIKIQav3HHmXTLWePUGJMtF5bV6Snl+7VofpmfefkITynCFnLdlXo7Y0lkqTbzh6phBhrvxWOttuUkehSapyTQItQ1n6FAiFo+eHzO2YOTZONNzRA0BBq/S/KZtN3TxmqtASnnvpsr15bV6TKhmbdevZIOVg4CSGmvNajP3zQuvrj5ZMHWPrwRyINbYg1oB/tr2xQYWWj7FE2Tcu37j8CQLgh1ALrsskDlRLn1O/e36GPd1S0rhZ5wRjFOXkbgdDg8xt6dPE21XlaNDwzQd+cmW/2SF0i0nAkfi0G9KMVhy+CPXGAW/Eu3sQAwUCoBcfpozJ1z0VjFeuwa11htea/ukGV9c1mjwV0y8ur9mtjUY1iHFH60ZxRltszzDlpOBprvVKBENbc4teqgtaFRWayXD8QFIRacE0ZlKIHLpug5FiHdlfU60evrNOBykazxwKOaUtxjZ7/vECS9P3Thik3Odbkib5EpOF4iDWgn6wrrFKT16/UeKeGZyaYPQ4Q9gg1cwzPTNCvvz5ROe4YldZ49KNX1mlzcY3ZYwFdqvO06H/f3Sa/IZ02MkNnjMo0eyRJRBq6j1gD+snyw4dAzhySyhtGIMAINXPluGP16ysmakRmgmqbWvSzf2/QJzvKzR4L6MAwDD3+4U6V1XqUleTS/5w+zPSFv4g09BSxBvSDspom7a6ol80mnTg41exxgLBGqFlDcpxTD1w2of3i2b9+Z5teXsW12GAd728p0yc7KmSPsulHc0abuiAOkYbeItaAfrB8zyFJ0pjsJCXHOU2eBghfhJq1xDhaL559yaRcSdLflu3VY0t2qsXnN3kyRLq9FfX6v493SZKunT5Io7ITTZmDSENfsVwd0EctPr++2NsaaywsAgQOoWZN9iibbjhlqLKTYvT/Pt2tdzaXqrzOo7vOM3dPBiJXTaNXv3xrszwtfp2Ql6zLpwwM+gwswY/+wp41oI82F9eoztOixJhojc1JMnscICwRatZ38aRc/eSCMXJFR2l1QZXuemW9yms9Zo+FCOPzG/r1O1tVWtN6ntqP5oySPYixxJ409DdiDeijtoVFpg9ODeo/CECkINRCx4whaXro8olKiXNo78EG3fnSOm0vrTV7LESQpz7bo3WF1YpxROlnF4xVUqwjKN+XSEOgEGtAH1Q2NGtrSesbkRlDOAQS6G+EWugZnpmg//36JA1KjdOhhmbd/ep6fbitzOyxEAHe31Kq19cVSZJuPWukBqfHB/x7EmkINGIN6IPP9xySIWl4RoIyEl1mjwOEFUItdGUmxeg3X5+o6YNbV4p8ZPF2/W3pXvlZKRIBsq2kVo99uFOSdNWJeTppeHpAvx+RhmAh1oBe8vsNrdhz+NpqLCwC9CtCLfTFOaP1kwvG6OuHF3d4eXWhHnhrixqaW0yeDOHmUH2zHnh7i7w+QzOGpOqa6YMC9r2INAQbsQb00vayWlU2eBXntGviQLfZ4wBhg1ALH/Yom66bPVi3nzNSDrtNK/Yc0o9fXq+SmiazR0OY8Pr8euCtLTpU36y81Djdfs7IgPy8INJgFmIN6KW2hUWmDkqRw85fJaA/EGrh6YxRmXrwstaFR/YdatAd/1qrjQeqzR4LIc4wDP35w13aVlqreJddP7tgTL9fLoJIg9l4hwn0Qp2nRRuLaiRxCCTQXwi18DYqO1GPzj1BwzLiVdPUop+9tlGvrzsgg/PY0EsvfLFfi7eUKsom/fjc0cpNju23+ybSYBXEGtALawoq5fMbGpgS26//OACRilCLDOkJLj10+USdOiJDPr+hJz7Zo0cWb1eT12f2aAgxb24o1vOfF0iSvnfKUE0ZlNIv90ukwWqINaAXVu6rlCRNy081eRIg9BFqkSXGYdedc0bqhlOGyB5l00fby3XnS+tUVNVo9mgIER9vL9dfPtolSfrGiXm6cGJun++TSINVEWtAD5XVNKngUIOibNKUQclmjwOENEItMtlsNl0yaYB+9bXx7eex3favte0r7AJHs7qgUr99b7sMSRdOyNE3+rjyI5EGqyPWgB5q26s2OjtJiTEOk6cBQhehhnG5bv3uqskak5OkhmaffvnmFj27fJ98fs5jQ2fbSmr1wFtb1OI3dMqIdH3v1KGy9fJnBpGGUEGsAT3gNwyt3HdIkjQtv3+OjwciEaGGNqnxTj3wtfG6eGKOJOlfK/drwRubVNnQbPJksJKCQw26741N8rT4NTkvWbed3bsl+ok0hBpiDeiB3eX1qmzwKsYRpXG5XFsN6A1CDUeKtkfpe6cO0x3njJQrOkpr91fp5hfWaE1BpdmjwQLKapt07+sbVetp0aisRM0/f0yPL5lDpCFUEWtAD7TtVZs0MFnOaP76AD1FqOFYTh+VqUfnnqD81DhVNXh17+ub9Lele9Xi85s9GkxysM6je17bpIq6ZuWlxOqei8Yq1mnv9tcTaQh1vNsEuqm5xa91hVWSWAUS6A1CDd0xKDVOj8ydpPPHZ8uQ9PLqQs1fuEGlNU1mj4YgK61p0t2vbtCBqkZlJLp0/6XjlRTbvXPFiTSEC2IN6KZNxdVq8vqVEufQ0Ix4s8cBQgqhhp5wRdv1P6cP193njVa8066tJbW65cU1+mxnhdmjIUj2VzborlfWq6SmSdlJMXrwsglKT3Ad9+uINIQbYg3oppV7W8+dmJqfyptMoAcINfTWScPT9furJ2tUVqLqPT49tGirHluyU43NXEQ7nO2pqNP8VzfoYH2z8lLj9NDlE5SVFHPMryHSEK6INaAbapu82lpSI4lVIIGeINTQV1lJMXro8gn6+pSBkqRFm0p0y4trtKmo2uTJEAhbS2o0f+EGVTd6NSwjXg9eNkFpx9ijRqQh3BFrQDesKaiS32g9l+J4v90D0IpQQ3+JtkfputmD9ctLxys9waXi6ibNf3WDnv5sj5pbWHwkXKwvrNLPX9uoeo9PY3KS9KuvTZD7KOeoEWmIFMQa0A1fHF4Fcip71YBuIdQQCJPykvWnb0zWWaMzZUh6dc0B3favtdpZVmf2aOijlXsP6b43NqvJ69cJecm6/5JxindFd9qOSEOkIdaA4yitaVJhZaOibNKUPGINOB5CDYEU74rWrWeP1M8uHKPkWIcKDjXozpfX6Z+fF7DEfwgyDENvbijWL9/aomafXzOGpOrnF45VjKPj8vxEGiJV519ZAOhg5d7WvWpjcpKUEMNfGeBYCDUEy4whaRp9TZL+/OFOfbbroJ7/vEDL9xzUD88YoeGZCWaPh27w+vz6y8e79c6mEknS6aMydMuZIxT9lQteR9ttykh0KTXOSaAhIvHOEzgGv2FoZUHrKpBcWw04NkINweaOdeiu80bro+3l+r+Pd2l3eb3ueGmtLpqYq2tnDFKck7c5VlXV0KwH396qzcU1skm6bvZgXT55gGyHf2YQaUArfooBx7CrvE5VDV7FOKI0LjfJ7HEAyyLUYBabzabTR2Vq0sBk/b9P9+jjHeV6fV2Rlu6q0PdOHaZZQ9PMHhFH2FVep1++uUUVdR7FOe360ZxRmja49ReiRBrQEbEGHEPbtdVOyEuRw84pnkBXCDVYQUq8Uz86d5TOGp2pxz/aqdIajx54a4tmDEnVf586TBmJx7+gMgLvkx3l+t37O9Tc4teA5Fj99MIxykuJI9KAoyDWgKNobvFrXWGVJK6tBhwNoQarmZKfoj99Y4r+tXK/Xl1zQCv2HNL6wmp9Y3qeLpqYyy/eTNLi8+u5FQV6eXWhJGnKoBT96NxRSo5zEGnAMRBrwFFsLq6Rp8WvlDiHhqTHmz0OYDmEGqwqxmHXt2YN1mkjM/TYkp3aUlKrpz7bq7c3lui/Zg/WzKFp7edGIfD2H2rQo+9tb7/EwuWTB+g7Jw9RtjuGSAOOg1gDjmLt/tZDICfnpfAGFDgCoYZQkJ8Wr4eumKj3tpTq2eX7VFzdpAfe3qpxuUn67slDWTUywPyGoTfWFenvy/ap2edXvMuuH545Ql+fOpBIA7qJWAO64PH6tLm4RpJ0wqBkc4cBLIZQQyiJstk0Z2y2Th6erldXH9DCNQe0qahGt/1rrc4clal5s/KVnsD5bP2ttKZJv39/hzYcqJYkTc1P0S8uHafR2UlEGtADxBrQhU3FNfL6DKUnODUwOdbscQDLINQQquKc0frmzHydOy5bf1++Vx9uK9cH28r06a4KXTopV187YYCSYh1mjxnyDMPQ+1vK9NdPdqvR65MrOkq3nTNSN5w8RHbOFwR6jFgDurCm4MtDIDmvAWhFqCEcZCS6dMc5o3TxxFw9+ekebS6u0UurCvWf9cW6aGKOLj1hgNxEW68UVTXq/326W18cXkl54kC3fnfVCRqaweGmQG+F/a84KisrNW/ePLndbrndbs2bN09VVVVH3d7r9equu+7ShAkTFB8fr9zcXH3rW99SUVFRh+1OP/102Wy2Dn+uvvrqAD8aBENjc4u2lNRKkiZzCCQgiVBD+BmZlaiHLp+gn5w/WkPS49Xo9emlVYX67t+/0DNL96q60Wv2iCGjrqlF/++T3frB86v1xd5KOew23XXeKC38n5MINaCPwn7P2jXXXKPCwkItWrRIkvS9731P8+bN0xtvvNHl9g0NDVq9erV+/vOfa9KkSaqsrNStt96qSy65RCtXruyw7Q033KD777+//ePYWA6XCwcbimrk8xvKSopRjpv/TwFCDeHKZrNp1rB0zRiaphV7DumFLwq0u7xer6wu1H/WF+mCCTm6bPIApcQ5zR7Vkrw+v97eWKwXPt+vWk+LJOnUEen62UVjNTIr0eTpgPAQ1rG2ZcsWLVq0SMuXL9eMGTMkSU888YRmzZqlbdu2adSoUZ2+xu12a/HixR1u++Mf/6jp06eroKBAgwYNar89Li5O2dnZgX0QCLq17YdAJps7CGABhBoiQZTNpllD0zRzSKq+2HtI//xiv3aW1WnhmgN6Y12RThmRrosm5hIghxmGoeV7DumZz/aoqLpJkjQyM0E/vWisThuZYfJ0QHgJ61hbtmyZ3G53e6hJ0syZM+V2u7V06dIuY60r1dXVstlsSk5O7nD7c889p3/84x/KysrS+eefr3vvvVeJiUf/Qe7xeOTxeNo/rqmp6dkDQsDVe1q0rfTwIZDEGiIcoYZIY7PZNH1Imk4cnKpV+yr1whf7ta20Vku2lWvJtnKNyEzQRRNzdcqI9Ii8uLbfMLRy7yG9enhFTUlKi3fqjjmjNHfaQEVH4HMCBFpYx1pJSYkyMzM73Z6ZmamSkpJu3UdTU5PuvvtuXXPNNUpKSmq//dprr9WQIUOUnZ2tjRs3av78+Vq3bl2nvXJf9eCDD+q+++7r+QNB0KwvrJbfkAYkxyozKcbscQDTEGqIZDabTdMGp2ra4FRtL63Vm+uL9fGOcu0oq9Nv39uupz7bozljs3TeuOyI+Leisdmn97eW6vV1RSo+vCfNFR2l754yRN8/fbgSXGH9dhIwVUj+7VqwYMFxo+eLL76QpC5X8jMMo1sr/Hm9Xl199dXy+/16/PHHO3zuhhtuaP/v8ePHa8SIEZo2bZpWr16tKVOmdHl/8+fP1+23397+cU1NjfLy8o47B4JnzeELYZ/AXjVEMEIN+NLIrESNPCdR3zl5iN7ZVKK3Nxaroq5ZL60q1EurCjUmJ0mnjkjXScPTw+7ctrLaJr25vljvbC5RvccnSUqMidY10wfp2ycN5rxuIAhCMtZuuumm4668OHjwYK1fv16lpaWdPldeXq6srKxjfr3X69XcuXO1Z88effDBBx32qnVlypQpcjgc2rFjx1FjzeVyyeXiwptWVdvk1c7yOknEGiIXoQZ0zR3r0NxpebpiykCt2HNQb24o1obCam0prtGW4ho98cluTRyYrFNHpGvWsPSQ3dvU2OzTyn2H9OnOCi3ffVB+o/X2/LQ4XX/yEF0xZaDiQ/SxAaEoJP+2paenKz09/bjbzZo1S9XV1fr88881ffp0SdKKFStUXV2t2bNnH/Xr2kJtx44dWrJkidLS0o77vTZt2iSv16ucnJzuPxBYyrrCahmGNCg1TukJRDUiD6EGHJ89yqbZw9I1e1i6DtZ59MnOCn28vfUQybX7q7R2f5Ue/3CXJg5M1gl5bp2Ql6z8tHhL/12q97To872HtHRXhVbvq1Kzz9/+uVlD03T9yUN05uhMRUVZ9zEA4cpmGIZh9hCBdP7556uoqEh/+ctfJLUu3Z+fn99h6f7Ro0frwQcf1GWXXaaWlhZdccUVWr16tf7zn/902AOXmpoqp9OpXbt26bnnntMFF1yg9PR0bd68WXfccYdiY2P1xRdfyG63d2u2mpoaud1uVVdXH3fPXaA1NLfonY2d90JGkj8t2aFd5fW6ZFKuzhjV+VxHIJwRakDfFFc36uMdreFWcKihw+fcsQ5NGujWpLxknTAw2fTz3Hx+QwWH6rW1pFaf7zmktfur1OL/8u3gwJRYXTghR1+bPEBjcsx9fwKEq+52QEjuWeuJ5557TjfffLPmzJkjSbrkkkv0pz/9qcM227ZtU3V1tSSpsLBQr7/+uiTphBNO6LDdkiVLdPrpp8vpdOr999/X73//e9XV1SkvL08XXnih7r333m6HGqylqqFZu8vrJXEIJCIPoQb0XY47VldNy9NV0/K072C91hRUaW1hlTYVVau60dsacjsqJLWuoDg4PV6D0+I0OC1eg9PiNSAlNmArTB6s82hbaa22l9ZqW0mtdpbXqcnr77BNXmqs5ozN1uWTB2hsblK3zu0HEHhhv2fNytizZh0fbS/Tv9cWaUh6vG4+c4TZ4wBBQ6gBgeX1+bW9tFZr91dp3f4qbSutlb+Ld172KJvyUmKV7Y6RO9ap5FiHkuMccsc6lBzrkDvOKWd0lAzDkGG0LqPf9r9+o/W864q6Zh2s86i8zqODdc2qqPeootajmqaWTt8v1mHXiKwETRqYrIsn5WhafiqHOQJBxJ41oAfW7K+SxF41RBZCDQg8hz1K43LdGpfr1rUz8tXQ3KJ9Bxu092C99lTUt/93Q7NPew82aO/BhuPfaQ9F2VrPxx6VlaiR2YkalZWowenxynbHKDXOSaQBFkasIeIdqm/WvoMNskmaNDDZ7HGAoCDUAHPEOaM1Jiepw7lghmGovM6jvRUNOljvUVWDV1WNXlU3NKuq0Xv442a1+AxF2Wyy2dTpf+Od0cpIdCkt3qn0BJfSElr/Nz3BqeykWMU6W0/TiLbblJHoItKAEEGsIeKtPbxXbVhGgtyxDnOHAYKAUAOsxWazKTMxRpmJgVt4hEgDQhOxhojXdiHsyYOSzR0ECAJCDYgsRBoQ2og1RLSKOo8KKxsVZZMmcggkwhyhBkQOIg0ID8QaItqGwtZLNgzLSFCCi78OCF+EGhAZiDQgvPDuFBFt/YEqSexVQ3gj1IDwR6QB4YlYQ8SqbvS2L5E8YYDb5GmAwCDUgPBGpAHhjVhDxNp4oPUQyMFpcawCibBEqAHhi0gDIgOxhojVdgjkhAHJps4BBAKhBoQnIg2ILMQaIlK9p0U7y+okcQgkwg+hBoQfIg2ITMQaItLm4hr5DSnXHaOMRJfZ4wD9hlADwguRBkQ2Yg0RaX1hlST2qiG8EGpA+CDSAEjEGiKQx+vTttJaSSzZj/BBqAHhgUgD8FXEGiLOlpJaeX2G0hOcynHHmD0O0GeEGhD6iDQAXSHWEHE2HF6yf8IAt2y8oUWII9SA0EakATgWYg0RpcXn1+bi1libyJL9CHGEGhC6iDQA3UGsIaLsKKtTk9evpJhoDUqLM3scoNcINSA0EWkAeoJYQ0T56iqQvLFFqCLUgNBDpAHoDWINEcPvN7SxqEaSNIFVIBGiCDUgtBBpAPqCWEPE2HOwXnWeFsU57RqekWD2OECPEWpA6CDSAPQHYg0Ro+0QyLE5SbLzDydCDKEGhAYiDUB/ItYQEQzDaF+ynwthI9QQaoD1EWkAAoFYQ0QorGxUZYNXTnuURmUlmj0O0G2EGmBtRBqAQCLWEBHWH96rNiYnUc7oKJOnAbqHUAOsi0gDEAzEGiLCl0v2J5s6B9BdhBpgTUQagGAi1hD2ymqaVFbrkT3KprE5SWaPAxwXoQZYD5EGwAzEGsLepsPXVhuWkaBYp93kaYBjI9QAayHSAJiJWEPY21Tcer7auFz2qsHaCDXAOog0AFZArCGs1XtatKeiXpI0nliDhRFqgDUQaQCshFhDWNtaUiO/IeW4Y5Qa7zJ7HKBLhBpgPiINgBURawhrbeerjWNhEVgUoQaYi0gDYGXEGsKWz29oS0lrrI3NdZs8DdAZoQaYh0gDEAqINYSt3RV1avL6Fe+yKz81zuxxgA4INcAcRBqAUEKsIWy1HQI5NieJf5BhKYQaEHxEGoBQRKwhbG1uO1+NQyBhIYQaEFxEGoBQRqwhLJXVNKm8ziN7lE2jshLNHgeQRKgBwUSkAQgHxBrCUtshkMMyEhTjsJs8DUCoAcFCpAEIJ8QawtKm4mpJ0jguhA0LINSAwCPSAIQjYg1hp6G5RXsq6iVxfTWYj1ADAotIAxDOiDWEnS3FtfIbUnZSjNISXGaPgwhGqAGBQ6QBiATEGsLOpiIOgYT5CDUgMIg0AJGEWENY8fkNbS2plcSS/TAPoQb0PyINQCQi1hBW9lTUqdHrU7zLrvzUOLPHQQQi1ID+RaQBiGTEGsJK25L9Y7KT+EcdQUeoAf2HSAMAYg1hpi3WOAQSwUaoAf2DSAOALxFrCBtltU0qr/PIHmXT6OxEs8dBBCHUgL4j0gCgM2INYaNtr9qwjATFOOwmT4NIQagBfUOkAcDREWsIG+1L9nMhbAQJoQb0HpEGAMdHrCEsNDS3aE9FvSSur4bgINSA3iHSAKD7iDWEhW2ltfIbUlaSS2kJLrPHQZgj1ICeI9IAoOeINYSFrcWtF8Iek81eNQQWoQb0DJEGAL1HrCHkGYahrSWti4uM5nw1BBChBnQfkQYAfUesIeQVVzeppqlFTnuUhqbHmz0OwhShBnQPkQYA/YdYQ8jbUty6V214ZoIc9iiTp0E4ItSA4yPSAKD/EWsIeVtL2s5X40LY6H+EGnBsRBoABA6xhpDW5PVpd0WdJM5XQ/8j1ICjI9IAIPCINYS0HWV18htSeoJT6SzZj35EqAFdI9IAIHiINYS0tvPVWLIf/YlQAzoj0gAg+Ig1hCyW7EcgEGpAR0QaAJiHWEPIKqv1qLLBq+gom4ZlsGQ/+o5QA75EpAGA+Yg1hKy2QyCHZiTIFW03eRqEOkINaEWkAYB1EGsIWSzZj/5CqAFEGgBYEbGGkNTc4teu8tYl+8dwvhr6gFBDpCPSAMC6iDWEpF3ldWrxG0qJcygzkSX70TuEGiIZkQYA1kesISS1na82OjtJNt5coxcINUQqIg0AQgexhpDUfr5aDueroecINUQiIg0AQg+xhpBTUedReZ1HUTZpRCaxhp4h1BBpiDQACF3EGkLO1sOHQA5Jj1eMgyX70X2EGiIJkQYAoY9YQ8jZ0r5kP6tAovsINUQKIg0AwgexhpDS4vNrZ1nrkv2jWbIf3USoIRIQaQAQfog1hJTdFfVq9vmVFBOtXHeM2eMgBBBqCHdEGgCEL2INIYUl+9EThBrCGZEGAOGPWENIaVuyfzRL9uM4CDWEKyINACIHsYaQUdXQrJKaJtls0kiW7McxEGoIR0QaAEQeYg0hY3tp6161QSlxinfx0kXXCDWEGyINACJXlNkDBFplZaXmzZsnt9stt9utefPmqaqq6phf8+1vf1s2m63Dn5kzZ3bYxuPx6Ic//KHS09MVHx+vSy65RIWFhQF8JNhW2roK5Mgs9qqha4Qawkm03aac5BiNykpUeoKLUAOACBT2sXbNNddo7dq1WrRokRYtWqS1a9dq3rx5x/268847T8XFxe1/3nrrrQ6fv/XWW7Vw4UK98MIL+vTTT1VXV6eLLrpIPp8vUA8lohmG0b5njVhDVwg1hAsiDQDQJqyPJduyZYsWLVqk5cuXa8aMGZKkJ554QrNmzdK2bds0atSoo36ty+VSdnZ2l5+rrq7Wk08+qWeffVZnn322JOkf//iH8vLy9N577+ncc8/t/wcT4Yqrm1TnaZHTHqXBaXFmjwOLIdQQDjjcEQBwpLDes7Zs2TK53e72UJOkmTNnyu12a+nSpcf82g8//FCZmZkaOXKkbrjhBpWVlbV/btWqVfJ6vZozZ077bbm5uRo/fvxx7xe907ZXbWhGvKLtYf2yRQ8Ragh17EkDABxNWO9ZKykpUWZmZqfbMzMzVVJSctSvO//883XllVcqPz9fe/bs0c9//nOdeeaZWrVqlVwul0pKSuR0OpWSktLh67Kyso55vx6PRx6Pp/3jmpqaXjyqyLS9rPV8tVEcAomvINQQytiTBgA4npDcRbFgwYJOC4Ac+WflypWS1OWFkw3DOOYFla+66ipdeOGFGj9+vC6++GK9/fbb2r59u958881jznW8+33wwQfbFzpxu93Ky8vr5iOObC0+v3aVs7gIOiLUEKrYkwYA6K6Q3LN200036eqrrz7mNoMHD9b69etVWlra6XPl5eXKysrq9vfLyclRfn6+duzYIUnKzs5Wc3OzKisrO+xdKysr0+zZs496P/Pnz9ftt9/e/nFNTQ3B1g37DjWoucWvBFe0st0xZo8DCyDUEIrYkwYA6KmQjLX09HSlp6cfd7tZs2apurpan3/+uaZPny5JWrFihaqrq48ZVUc6ePCg9u/fr5ycHEnS1KlT5XA4tHjxYs2dO1eSVFxcrI0bN+rXv/71Ue/H5XLJ5XJ1+/ui1ZerQCbwhhyEGkIOkQYA6K2QPAyyu8aMGaPzzjtPN9xwg5YvX67ly5frhhtu0EUXXdRhJcjRo0dr4cKFkqS6ujrdeeedWrZsmfbu3asPP/xQF198sdLT03XZZZdJktxut66//nrdcccdev/997VmzRp985vf1IQJE9pXh0T/Ycl+tCHUEEo43BEA0FchuWetJ5577jndfPPN7Ss3XnLJJfrTn/7UYZtt27apurpakmS327Vhwwb9/e9/V1VVlXJycnTGGWfoxRdfVGLil7Hw29/+VtHR0Zo7d64aGxt11lln6ZlnnpHdbg/eg4sAjc0+FRxqkCSNzCTWIhmhhlDBnjQAQH+xGYZhmD1EpKqpqZHb7VZ1dbWSkpJMnaWhuUXvbOx8fp/ZNhyo1lOf7VFGoks/OX+M2ePAJIQaQgGRBgDoru52QNjvWUNoaz8Ekr1qEYtQg9URaQCAQCHWYGlfXVwEkYdQg5URaQCAQCPWYFlVDc0qq/XIZpNGZBJrkYZQg1URaQCAYCHWYFlte9UGpcQp1slLNZIQarAiIg0AEGy8A4ZlbS+tk8SS/ZGGUIPVEGkAALMQa7AkwzC0vYzz1SINoQYrIdIAAGYj1mBJJTVNqm1qkdMepcFp8WaPgyAg1GAVRBoAwCqINVhS2/lqQzPiFW2PMnkaBBqhBisg0gAAVkOswZK2cb5axCDUYDYiDQBgVcQaLKfF59eucmItEhBqMBORBgCwOmINlrPvUIOaW/xKcEUrxx1j9jgIEEINZiHSAAChgliD5bSdrzYiM4E372GKUIMZiDQAQKgh1mA5bbE2ikMgwxKhhmAj0gAAoYpYg6U0eX0qONQgifPVwhGhhmAi0gAAoY5Yg6XsKq+T35DSE5xKiXeaPQ76EaGGYCHSAADhgliDpewsa10Fcngme9XCCaGGYCDSAADhhliDpew8vGT/8IwEkydBfyHUEGhEGgAgXBFrsIzG5hYdqGqUJA3PJNbCAaGGQCLSAADhjliDZeyqqJdhSBmJLrljHWaPgz4i1BAoRBoAIFIQa7CM9vPVOAQy5BFqCAQiDQAQaYg1WMaXi4sQa6GMUEN/I9IAAJGKWIMl1HtaVNR2vhp71kIWoYb+RKQBACIdsQZL2F1RL0NSZqJLSZyvFpIINfQXIg0AgFbEGixhZ1mtJGkEh0CGJEIN/YFIAwCgI2INltB2vtowYi3kEGroKyINAICuEWswXb2nRUXVTZI4Xy3UEGroCyINAIBjI9Zgul3lrXvVspJilBjD+WqhglBDbxFpAAB0D7EG07UdAsn5aqGDUENvEGkAAPQMsQbT7Ty8Z20Yh0CGBEINPUWkAQDQO8QaTFXX1KJizlcLGYQaeoJIAwCgb4g1mKptr1qOO0YJMbwcrYxQQ3cRaQAA9A/eHcNUbYuLsFfN2gg1dAeRBgBA/yLWYKodhxcXGc7iIpZFqOF4iDQAAAKDWINpapu8Kq1pPV+NxUWsiVDDsRBpAAAEFrEG07Sdr5brjlG8i5ei1RBqOBoiDQCA4OAdMkyzi0MgLYtQQ1eINAAAgotYg2k4X82aCDUciUgDAMAcxBpMUdPoVVmtRzZJQ9OJNasg1PBVRBoAAOYi1mCK9vPVkmM5X80iCDW0IdIAALAG3iXDFDs5BNJSCDVIRBoAAFZDrMEUO7kYtmUQaiDSAACwJmINQVfd6FX54fPVhmXEmz1ORCPUIhuRBgCAtRFrCLrdXzlfLdbJS9AshFrkItIAAAgNvFNG0O2uqJckDWWvmmkItchEpAEAEFqINQRd2541luw3B6EWeYg0AABCE7GGoGpsblFxdZMkzlczA6EWWYg0AABCG7GGoNpdUS9DUkaCS4kxDrPHiSiEWuQg0gAACA/EGoKK89XMQahFBiINAIDwQqwhqHaXH461dGItWAi18EekAQAQnog1BE1zi1/7KxskSUO5GHZQEGrhjUgDACC8EWsImoJD9fL5DSXFRCst3mn2OGGPUAtfRBoAAJGBWEPQtB8CmZEgG9EQUIRaeCLSAACILMQagmZXBeerBQOhFn6INAAAIhOxhqDw+Q3tO9gaa8M4Xy1gCLXwQqQBABDZiDUExYGqRnla/Ip12JXtjjF7nLBEqIUPIg0AAEjEGoJkd3mdJGlIejwBEQCEWngg0gAAwFcRawiK3ZyvFjCEWugj0gAAQFeINQScYRjaU/HlSpDoP4RaaCPSAADAsRBrCLiyWo/qPC1y2G3KS4k1e5ywQaiFLiINAAB0B7GGgNt1+Hy1QalxirZHmTxNeCDUQhORBgAAeoJYQ8B9eb4ah0D2B0It9BBpAACgN4g1BFzbSpBDM1hcpK8ItdBCpAEAgL4g1hBQlQ3NqmzwymaTBqcRa31BqIUOIg0AAPQHYg0Btbu89RDIAcmxinHYTZ4mdBFqoYFIAwAA/YlYQ0Dtrmg9BHIYS/b3GqFmfUQaAAAIBGINAdW2Z42LYfcOoWZtRBoAAAgkYg0BU+9pUUlNkyRWguwNQs26iDQAABAMxBoCZs/hJfszE11KiOGl1hOEmjURaQAAIJh4B42A2dW+ZD971XqCULMeIg0AAJiBWEPAfHkxbM5X6y5CzVqINAAAYCZiDQHhafGpsLJBkjSMi2F3C6FmHUQaAACwAmINAVFwqEF+Q3LHOpQS5zR7HMsj1KyBSAMAAFZCrCEg9h4+BHJIerxsRMcxEWrmI9IAAIAVEWsIiD0HWw+BHJzGIZDHQqiZi0gDAABWRqyh3/kNQ/sOfrlnDV0j1MxDpAEAgFBArKHfldd61NDsk8Nu04DkWLPHsSRCzRxEGgAACCXEGvpd2/lqg1LjZOcNcSeEWvARaQAAIBRFmT1AoFVWVmrevHlyu91yu92aN2+eqqqqjvk1Nputyz+/+c1v2rc5/fTTO33+6quvDvCjCQ17Dh8CyflqnRFqwRVttyknOUajshKVnuAi1AAAQEgJ+z1r11xzjQoLC7Vo0SJJ0ve+9z3NmzdPb7zxxlG/pri4uMPHb7/9tq6//npdccUVHW6/4YYbdP/997d/HBvLIX+StLct1jhfrQNCLXjYkwYAAMJBWMfali1btGjRIi1fvlwzZsyQJD3xxBOaNWuWtm3bplGjRnX5ddnZ2R0+fu2113TGGWdo6NChHW6Pi4vrtG2kq/e0qLTGI4k9a19FqAUHkQYAAMJJWB8GuWzZMrnd7vZQk6SZM2fK7XZr6dKl3bqP0tJSvfnmm7r++us7fe65555Tenq6xo0bpzvvvFO1tbXHvC+Px6OampoOf8LNvkOtS/ZnJLqU4Arr3wV0G6EWeBzuCAAAwlFYv5suKSlRZmZmp9szMzNVUlLSrfv429/+psTERF1++eUdbr/22ms1ZMgQZWdna+PGjZo/f77WrVunxYsXH/W+HnzwQd133309exAhpv1i2OxVk0SoBRp70gAAQDgLyT1rCxYsOOoiIG1/Vq5cKal1sZAjGYbR5e1deeqpp3TttdcqJiamw+033HCDzj77bI0fP15XX321Xn75Zb333ntavXr1Ue9r/vz5qq6ubv+zf//+Hjzq0MDiIl8i1AKHPWkAACAShOSetZtuuum4Ky8OHjxY69evV2lpaafPlZeXKysr67jf55NPPtG2bdv04osvHnfbKVOmyOFwaMeOHZoyZUqX27hcLrlcruPeV6jy+Q0VHGw9DHJwepzJ05iLUAsM9qQBAIBIEpKxlp6ervT09ONuN2vWLFVXV+vzzz/X9OnTJUkrVqxQdXW1Zs+efdyvf/LJJzV16lRNmjTpuNtu2rRJXq9XOTk5x38AYaqoulHNPr9iHFHKSoo5/heEKUKt/xFpAAAgEoXkYZDdNWbMGJ133nm64YYbtHz5ci1fvlw33HCDLrroog4rQY4ePVoLFy7s8LU1NTV66aWX9N3vfrfT/e7atUv333+/Vq5cqb179+qtt97SlVdeqcmTJ+ukk04K+OOyqrbz1QanxUdsnBBq/YvDHQEAQCQL61iTWldsnDBhgubMmaM5c+Zo4sSJevbZZztss23bNlVXV3e47YUXXpBhGPrGN77R6T6dTqfef/99nXvuuRo1apRuvvlmzZkzR++9957sdntAH4+VtV1fLVIXFyHU+g+RBgAAINkMwzDMHiJS1dTUyO12q7q6WklJSabO0tDconc2dj6/ryfu/88mVTZ49f3ThmlkVmI/TRYaCLX+weGOAAAgEnS3A0LynDVYT1VDsyobvLLZpEGpkbW4CKHWd0QaAABAZ8Qa+sXew6tA5rpjFeOInENBCbW+IdIAAACOjlhDv9gbgddXI9R6j0gDAAA4PmIN/aJtJcghEXJ9NUKtd4g0AACA7iPW0GfNLX4VVjVKiow9a4RazxFpAAAAPUesoc8KKxvk8xtKjIlWarzT7HECilDrGSINAACg94g19Nmer1xfzRbG4UKodR+RBgAA0HfEGvqs7Xy1cD4EklDrHiINAACg/xBr6BPDMNqX7R+cHp6xRqgdH5EGAADQ/4g19ElFXbPqPC2yR9k0MCXW7HH6HaF2bEQaAABA4BBr6JO266vlpcTKYY8yeZr+RagdHZEGAAAQeMQa+iRcL4ZNqHWNSAMAAAgeYg198uXFsMMn1gi1zog0AACA4CPW0GtNXp+Kq5skSflhsmeNUOuISAMAADAPsYZe23eoQYak1Hin3LEOs8fpM0LtS0QaAACA+Yg19FrB4fPV8lPjTJ6k7wi1VkQaAACAdRBr6LV9h1qvrzYoxGONUCPSAAAArIhYQ68YhqF9bRfDDuHz1SI91Ig0AAAA6yLW0CuVDV9eDHtAiF4MO5JDjUgDAACwPmINvdK2Vy3XHROSF8OO1FAj0gAAAEIHsYZe2Xs41kJxyf5IDDUiDQAAIPQQa+iVgkOtK0GG2uIikRZqRBoAAEDoItbQYy0+vworGyVJ+WmhE2uRFGpEGgAAQOgj1tBjRdVNavEbinPalZHgMnucbomUUCPSAAAAwgexhh5ruxj2oNQ42UIgeCIh1Ig0AACA8EOsocfaLoadHwLnq4V7qBFpAAAA4YtYQ4+1x5rFV4IM51Aj0gAAAMIfsYYeaWhuUXmtR5K1V4IM11Aj0gAAACIHsYYeKTi8Vy09wal4lzVfPuEYakQaAABA5LHmu21YVvvFsFOteQhkuIUakQYAABC5iDX0SPtKkBa8vlo4hRqRBgAAAGIN3WYYRvthkFZbCTJcQo1IAwAAQBtiDd1WUdes+maf7FE2DUiONXucduEQakQaAAAAjkSsodsKDrUeAjkwJVbR9iiTp2kV6qFGpAEAAOBoiDV0276D1joEMpRDjUgDAADA8RBr6DYrXQw7VEONSAMAAEB3EWvolhafXweqGiWZfzHsUAw1Ig0AAAA9RayhWwqrGuXzG4p32ZUW7zRtjlALNSINAAAAvUWsoVv2feVi2DaT4iiUQo1IAwAAQF8Ra+iWtpUgzVpcJFRCjUgDAABAfyHW0C1te9YGpQU/1kIh1Ig0AAAA9DdiDcdV19Sig/XNkoK/Z83qoUakAQAAIFCINRzXvsOHQGYmuhTrDN5LxsqhRqQBAAAg0Ig1HFdB+/XVgrdXzaqhRqQBAAAgWIg1HNdXV4IMBiuGGpEGAACAYCPWcEx+w2jfsxaMxUWsFmpEGgAAAMxCrOGYKmo9avT65LDblOuODej3slKoEWkAAAAwG7GGY9pf2bpXbUBynOwBjBarhBqRBgAAAKsg1nBMbYdA5qUGbq+aFUKNSAMAAIDVEGs4pv2VjZKkvJTAnK9mdqgR+eOSLQAAGF1JREFUaQAAALAqYg1H5fMbKjx8GOSgAFwM28xQI9IAAABgdcQajqq0pklenyFXdJQyEl39et9mhRqRBgAAgFBBrOGo9redr5YS168hZUaoEWkAAAAINcQajqptJcj+XFwk2KFGpAEAACBUEWs4qi9Xguyf89WCGWpEGgAAAEIdsYYutfj8KqpuktQ/K0EGK9SINAAAAIQLYg1dKq5uks9vKM5pV1q8s0/3FYxQI9IAAAAQbog1dKngK4uL2PoQVoEONSINAAAA4YpYQ5e+XFyk94dABjLUiDQAAACEO2INXWpbtn9QL1eCDFSoEWkAAACIFMQaOmlu8aukpveLiwQi1Ig0AAAARBpiDZ0cqGqU35ASY6LljnX06Gv7O9SINAAAAEQqYg2d7O/l4iL9GWpEGgAAACIdsYZOCirbzlfr/iGQ/RVqRBoAAADQilhDJ+171roZa/0RakQaAAAA0BGxhg6avD6V13okSXkpx18Jsq+hRqQBAAAAXSPW0EFhZYMMSSlxDiXGHHtxkb6EGpEGAAAAHBuxhg72H2qUdPwl+3sbakQaAAAA0D3EGjpoW1zkWOer9SbUiDQAAACgZ4g1dHC8xUV6GmpEGgAAANA7xBra1XtadLC+WVLXi4v0JNSINAAAAKBviDW023/4EMj0BKfinB1fGt0NNSINAAAA6B/EGtq1HQJ55MWwuxNqRBoAAADQv4g1tNtf2XklyOOFGpEGAAAABAaxhnYFRywucqxQI9IAAACAwCLWIEkqr/WoutErm6SBybFHDTUiDQAAAAiOKLMHCLRf/epXmj17tuLi4pScnNytrzEMQwsWLFBubq5iY2N1+umna9OmTR228Xg8+uEPf6j09HTFx8frkksuUWFhYQAeQXBsPFAtScpMitGGoupOoeaMjlJOcoxGZSUqPcFFqAEAAAABFvax1tzcrCuvvFLf//73u/01v/71r/Xoo4/qT3/6k7744gtlZ2frnHPOUW1tbfs2t956qxYuXKgXXnhBn376qerq6nTRRRfJ5/MF4mEE3MaiGklSnDOqQ6j98KzhGpASS6QBAAAAQWYzDMMwe4hgeOaZZ3TrrbeqqqrqmNsZhqHc3FzdeuutuuuuuyS17kXLysrSww8/rP/+7/9WdXW1MjIy9Oyzz+qqq66SJBUVFSkvL09vvfWWzj333G7NVFNTI7fbrerqaiUlJfXp8fXVvCdX6JMdFe0fXzA+WwsuHaf0eAINAAAA6E/d7YCw37PWU3v27FFJSYnmzJnTfpvL5dJpp52mpUuXSpJWrVolr9fbYZvc3FyNHz++fZtQYhiGVhdUtn98+ZQB+sPVk5WZGEOoAQAAACZhgZEjlJSUSJKysrI63J6VlaV9+/a1b+N0OpWSktJpm7av74rH45HH42n/uKampr/G7pNXVh9Qvaf18M2rpg3Ug5dPJNIAAAAAk4XknrUFCxbIZrMd88/KlSv79D1sR1z02TCMTrcd6XjbPPjgg3K73e1/8vLy+jRjfxmVnaDkWIeyk2IINQAAAMAiQnLP2k033aSrr776mNsMHjy4V/ednZ0tqXXvWU5OTvvtZWVl7XvbsrOz1dzcrMrKyg5718rKyjR79uyj3vf8+fN1++23t39cU1NjiWCbMCBZy39ylpz2KEINAAAAsIiQjLX09HSlp6cH5L6HDBmi7OxsLV68WJMnT5bUuqLkRx99pIcffliSNHXqVDkcDi1evFhz586VJBUXF2vjxo369a9/fdT7drlccrlcAZm7r2IcdrNHAAAAAPAVIRlrPVFQUKBDhw6poKBAPp9Pa9eulSQNHz5cCQkJkqTRo0frwQcf1GWXXSabzaZbb71VDzzwgEaMGKERI0bogQceUFxcnK655hpJktvt1vXXX6877rhDaWlpSk1N1Z133qkJEybo7LPPNuuhAgAAAAgjYR9r99xzj/72t7+1f9y2t2zJkiU6/fTTJUnbtm1TdXV1+zY//vGP1djYqP/5n/9RZWWlZsyYoXfffVeJiYnt2/z2t79VdHS05s6dq8bGRp111ll65plnZLezhwoAAABA30XMddasyErXWQMAAAAQHFxnDQAAAABCGLEGAAAAABZErAEAAACABRFrAAAAAGBBxBoAAAAAWBCxBgAAAAAWRKwBAAAAgAURawAAAABgQcQaAAAAAFgQsQYAAAAAFkSsAQAAAIAFEWsAAAAAYEHEGgAAAABYELEGAAAAABZErAEAAACABRFrAAAAAGBBxBoAAAAAWBCxBgAAAAAWRKwBAAAAgAURawAAAABgQcQaAAAAAFgQsQYAAAAAFkSsAQAAAIAFEWsAAAAAYEHEGgAAAABYELEGAAAAABZErAEAAACABRFrAAAAAGBB0WYPEMkMw5Ak1dTUmDwJAAAAgGBpe//f1gNHQ6yZqLa2VpKUl5dn8iQAAAAAgq22tlZut/uon7cZx8s5BIzf71dRUZESExNls9lMnaWmpkZ5eXnav3+/kpKSTJ0l3PDcBhbPb+Dw3AYWz2/g8NwGFs9v4PDcBpaVnl/DMFRbW6vc3FxFRR39zDT2rJkoKipKAwcONHuMDpKSkkx/8YYrntvA4vkNHJ7bwOL5DRye28Di+Q0cntvAssrze6w9am1YYAQAAAAALIhYAwAAAAALItYgSXK5XLr33nvlcrnMHiXs8NwGFs9v4PDcBhbPb+Dw3AYWz2/g8NwGVig+vywwAgAAAAAWxJ41AAAAALAgYg0AAAAALIhYAwAAAAALItYixK9+9SvNnj1bcXFxSk5O7tbXGIahBQsWKDc3V7GxsTr99NO1adOmDtt4PB798Ic/VHp6uuLj43XJJZeosLAwAI/A2iorKzVv3jy53W653W7NmzdPVVVVx/wam83W5Z/f/OY37ducfvrpnT5/9dVXB/jRWEtvnttvf/vbnZ63mTNndtiG126rnj6/Xq9Xd911lyZMmKD4+Hjl5ubqW9/6loqKijpsF4mv3ccff1xDhgxRTEyMpk6dqk8++eSY23/00UeaOnWqYmJiNHToUP3f//1fp21eeeUVjR07Vi6XS2PHjtXChQsDNb7l9eT5ffXVV3XOOecoIyNDSUlJmjVrlt55550O2zzzzDNd/gxuamoK9EOxnJ48tx9++GGXz9vWrVs7bMdr90s9eX67+vfLZrNp3Lhx7dvw2m318ccf6+KLL1Zubq5sNpv+/e9/H/drQvLnroGIcM899xiPPvqocfvttxtut7tbX/PQQw8ZiYmJxiuvvGJs2LDBuOqqq4ycnByjpqamfZsbb7zRGDBggLF48WJj9erVxhlnnGFMmjTJaGlpCdAjsabzzjvPGD9+vLF06VJj6dKlxvjx442LLrromF9TXFzc4c9TTz1l2Gw2Y9euXe3bnHbaacYNN9zQYbuqqqpAPxxL6c1ze9111xnnnXdeh+ft4MGDHbbhtduqp89vVVWVcfbZZxsvvviisXXrVmPZsmXGjBkzjKlTp3bYLtJeuy+88ILhcDiMJ554wti8ebNxyy23GPHx8ca+ffu63H737t1GXFycccsttxibN282nnjiCcPhcBgvv/xy+zZLly417Ha78cADDxhbtmwxHnjgASM6OtpYvnx5sB6WZfT0+b3llluMhx9+2Pj888+N7du3G/PnzzccDoexevXq9m2efvppIykpqdPP4kjT0+d2yZIlhiRj27ZtHZ63r/7s5LX7pZ4+v1VVVR2e1/379xupqanGvffe274Nr91Wb731lvHTn/7UeOWVVwxJxsKFC4+5faj+3CXWIszTTz/drVjz+/1Gdna28dBDD7Xf1tTUZLjdbuP//u//DMNo/YHicDiMF154oX2bAwcOGFFRUcaiRYv6fXar2rx5syGpw1/kZcuWGZKMrVu3dvt+Lr30UuPMM8/scNtpp51m3HLLLf01asjp7XN73XXXGZdeeulRP89rt1V/vXY///xzQ1KHNx+R9tqdPn26ceONN3a4bfTo0cbdd9/d5fY//vGPjdGjR3e47b//+7+NmTNntn88d+5c47zzzuuwzbnnnmtcffXV/TR16Ojp89uVsWPHGvfdd1/7x9399zDc9fS5bYu1ysrKo94nr90v9fW1u3DhQsNmsxl79+5tv43XbmfdibVQ/bnLYZDo0p49e1RSUqI5c+a03+ZyuXTaaadp6dKlkqRVq1bJ6/V22CY3N1fjx49v3yYSLFu2TG63WzNmzGi/bebMmXK73d1+HkpLS/Xmm2/q+uuv7/S55557Tunp6Ro3bpzuvPNO1dbW9tvsVteX5/bDDz9UZmamRo4cqRtuuEFlZWXtn+O126o/XruSVF1dLZvN1ukQ60h57TY3N2vVqlUdXk+SNGfOnKM+j8uWLeu0/bnnnquVK1fK6/Uec5tIeo1KvXt+j+T3+1VbW6vU1NQOt9fV1Sk/P18DBw7URRddpDVr1vTb3KGgL8/t5MmTlZOTo7POOktLlizp8Dleu63647X75JNP6uyzz1Z+fn6H2yP9tdsbofpzN9q07wxLKykpkSRlZWV1uD0rK0v79u1r38bpdColJaXTNm1fHwlKSkqUmZnZ6fbMzMxuPw9/+9vflJiYqMsvv7zD7ddee62GDBmi7Oxsbdy4UfPnz9e6deu0ePHifpnd6nr73J5//vm68sorlZ+frz179ujnP/+5zjzzTK1atUoul4vX7mH98dptamrS3XffrWuuuUZJSUntt0fSa7eiokI+n6/Ln5dHex5LSkq63L6lpUUVFRXKyck56jaR9BqVevf8HumRRx5RfX295s6d237b6NGj9cwzz2jChAmqqanR73//e5100klat26dRowY0a+Pwap689zm5OTor3/9q6ZOnSqPx6Nnn31WZ511lj788EOdeuqpko7++ua126q7z0VxcbHefvttPf/88x1u57XbO6H6c5dYC2ELFizQfffdd8xtvvjiC02bNq3X38Nms3X42DCMTrcdqTvbhILuPr9S5+dJ6tnz8NRTT+naa69VTExMh9tvuOGG9v8eP368RowYoWnTpmn16tWaMmVKt+7bigL93F511VXt/z1+/HhNmzZN+fn5evPNNzsFcU/uN1QE67Xr9Xp19dVXy+/36/HHH+/wuXB97R5LT39edrX9kbf35mdwuOrtc/HPf/5TCxYs0GuvvdbhlxMzZ87ssPDQSSedpClTpuiPf/yj/vCHP/Tf4CGgJ8/tqFGjNGrUqPaPZ82apf379+t///d/22Otp/cZ7nr7XDzzzDNKTk7W1772tQ6389rtvVD8uUushbCbbrrpuKurDR48uFf3nZ2dLan1txA5OTntt5eVlbX/xiE7O1vNzc2qrKzssIeirKxMs2fP7tX3tZLuPr/r169XaWlpp8+Vl5d3+u1MVz755BNt27ZNL7744nG3nTJlihwOh3bs2BHSb3iD9dy2ycnJUX5+vnbs2CGJ167U9+fX6/Vq7ty52rNnjz744IMOe9W6Ei6v3a6kp6fLbrd3+s3rV39eHik7O7vL7aOjo5WWlnbMbXry2g8HvXl+27z44ou6/vrr9dJLL+nss88+5rZRUVE68cQT239ORIK+PLdfNXPmTP3jH/9o/5jXbqu+PL+GYeipp57SvHnz5HQ6j7ltJL52eyNUf+5yzloIS09P1+jRo4/558g9Nd3VdvjSVw9Zam5u1kcffdT+Znbq1KlyOBwdtikuLtbGjRvD4g1vd5/fWbNmqbq6Wp9//nn7165YsULV1dXdeh6efPJJTZ06VZMmTTrutps2bZLX6+0Q0KEoWM9tm4MHD2r//v3tzxuv3b49v22htmPHDr333nvt/8gdS7i8drvidDo1derUTod4Ll68+KjP46xZszpt/+6772ratGlyOBzH3CYcXqM90ZvnV2rdo/btb39bzz//vC688MLjfh/DMLR27dqwfI0eTW+f2yOtWbOmw/PGa7dVX57fjz76SDt37uzyXPYjReJrtzdC9udusFc0gTn27dtnrFmzxrjvvvuMhIQEY82aNcaaNWuM2tra9m1GjRplvPrqq+0fP/TQQ4bb7TZeffVVY8OGDcY3vvGNLpfuHzhwoPHee+8Zq1evNs4888yIXf584sSJxrJly4xly5YZEyZM6LT8+ZHPr2EYRnV1tREXF2f8+c9/7nSfO3fuNO677z7jiy++MPbs2WO8+eabxujRo43JkydH1PPb0+e2trbWuOOOO4ylS5cae/bsMZYsWWLMmjXLGDBgAK/dLvT0+fV6vcYll1xiDBw40Fi7dm2HZaM9Ho9hGJH52m1bnvvJJ580Nm/ebNx6661GfHx8+wpud999tzFv3rz27duWkL7tttuMzZs3G08++WSnJaQ/++wzw263Gw899JCxZcsW46GHHjJ9CWmz9PT5ff75543o6GjjscceO+rlIxYsWGAsWrTI2LVrl7FmzRrjv/7rv4zo6GhjxYoVQX98Zurpc/vb3/7WWLhwobF9+3Zj48aNxt13321IMl555ZX2bXjtfqmnz2+bb37zm8aMGTO6vE9eu61qa2vb389KMh599FFjzZo17SsTh8vPXWItQlx33XWGpE5/lixZ0r6NJOPpp59u/9jv9xv33nuvkZ2dbbhcLuPUU081NmzY0OF+GxsbjZtuuslITU01YmNjjYsuusgoKCgI0qOyjoMHDxrXXnutkZiYaCQmJhrXXnttp2WNj3x+DcMw/vKXvxixsbFdXn+qoKDAOPXUU43U1FTD6XQaw4YNM26++eZO1wsLdz19bhsaGow5c+YYGRkZhsPhMAYNGmRcd911nV6XvHZb9fT53bNnT5c/S7768yRSX7uPPfaYkZ+fbzidTmPKlCnGRx991P656667zjjttNM6bP/hhx8akydPNpxOpzF48OAuf2nz0ksvGaNGjTIcDocxevToDm+II01Pnt/TTjuty9fodddd177NrbfeagwaNMhwOp1GRkaGMWfOHGPp0qVBfETW0ZPn9uGHHzaGDRtmxMTEGCkpKcbJJ59svPnmm53uk9ful3r6s6GqqsqIjY01/vrXv3Z5f7x2W7VdRuJof8/D5eeuzTAOn1kHAAAAALAMzlkDAAAAAAsi1gAAAADAgog1AAAAALAgYg0AAAAALIhYAwAAAAALItYAAAAAwIKINQAAAACwIGINAAAAACyIWAMAAAAACyLWAAAIsk8//VQ2m639z8cff2z2SAAACyLWAAAIsr///e/H/BgAAEmyGYZhmD0EAACRwuPxKDs7W1VVVUpISFBdXZ2SkpJUUlKi2NhYs8cDAFgIe9YAAAii1157TVVVVZKk3//+95KkmpoavfbaayZOBQCwImINAIAg+tvf/iZJGjt2rL7zne9o7NixkjgUEgDQGbEGAECQlJWV6d1335UkffOb35QkXXvttZKkd999V6WlpabNBgCwHmINAIAgee6559TS0iKbzdYeaddee61sNpt8Pp+ee+45kycEAFgJsQYAQJC0HQJ5yimnaNCgQZKk/Px8nXzyyZI4FBIA0BGxBgBAEGzYsEHr1q2T9OUhkG3aPl63bp02bNgQ9NkAANZErAEAEARte9VcLpeuvPLKDp+bO3euXC5Xh+0AACDWAAAIMJ/Pp+eff16SdOGFFyo5ObnD55OTk3XBBRdIkp5//nn5fL5gjwgAsCBiDQCAAHv33XdVXFwsqfMhkG3abi8uLtZ7770XtNkAANZFrAEAEGBtC4ckJyfrwgsv7HKbr+5xY6ERAIAk2QzDMMweAgCAcFVTU6Ps7Gw1NjZ2+2vi4uJUUlKixMTEAE4GALA69qwBABBA//rXv3oUapLU0NCgl19+OUATAQBCBXvWAAAIoFNPPVWffPKJcnJy9Oijjx53+7vuuksFBQU6/fTTtWTJkiBMCACwqmizBwAAIFzt2bNHn376qSTpiiuu0NVXX33cr1m5cqUeeeQRffTRRyooKGi/eDYAIPJwGCQAAAHy7LPPqu0Alq9//evd+pq27QzD0LPPPhuw2QAA1sdhkAAABMiIESO0c+dOZWZmqri4WFFRx/8dqWEYGjRokAoLCzVq1Cht3bo1CJMCAKyIPWsAAATAZ599pp07d0qSLrvssm6FmiTZbDZdfvnlkqRt27ZpxYoVAZsRAGBtxBoAAAHw1WulXXHFFT362q9uzzXXACBycRgkAAAAAFgQe9YAAAAAwIKINQAAAACwIGINAAAAACyIWAMAAAAACyLWAAAAAMCCiDUAAAAAsCBiDQAAAAAsiFgDAAAAAAsi1gAAAADAgog1AAAAALAgYg0AAAAALIhYAwAAAAALItYAAAAAwIKINQAAAACwIGINAAAAACzo/wPPuKccppApHwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "As = AB_visitor.As\n",
    "Bs = AB_visitor.Bs\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "fig.patch.set_facecolor('xkcd:white')\n",
    "ax  = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "# Parallelogram\n",
    "x = [-1, -1, 1, 1]\n",
    "y = [-1,  0, 1, 0]\n",
    "ax.add_patch(patches.Polygon(xy=list(zip(x,y)), facecolor=\"C0\", alpha=.2))\n",
    "\n",
    "# HMM\n",
    "x = [0, 1, 1]\n",
    "y = [0, 1, 0]\n",
    "ax.add_patch(patches.Polygon(xy=list(zip(x,y)), facecolor=\"C0\", alpha=.2))\n",
    "\n",
    "x = [0, -1, -1]\n",
    "y = [0, -1,  0]\n",
    "ax.add_patch(patches.Polygon(xy=list(zip(x,y)), facecolor=\"C0\", alpha=.2))\n",
    "\n",
    "# RNN\n",
    "def f(a):\n",
    "    return a * (2 * a**2 - 1)\n",
    "\n",
    "n = 100\n",
    "x = [-1 + 2 * (i / n) for i in range(n + 1)]\n",
    "y = list(map(f, x))\n",
    "ax.plot(x, y, color = \"C0\")\n",
    "ax.plot(x, x, color = \"C0\")\n",
    "ax.plot(x, [0 for _ in range(len(x))], color = \"C0\")\n",
    "\n",
    "\n",
    "# Results\n",
    "ax.plot(As, Bs, color=\"C1\")\n",
    "\n",
    "ax.plot([As[0]], [Bs[0]],\n",
    "    marker=\"o\", markersize=7, fillstyle=\"none\", label=\"Estimated GUM\", color=\"C1\")\n",
    "\n",
    "ax.plot(\n",
    "    [unknown_gum.A.detach().numpy().ravel().ravel()], \n",
    "    [unknown_gum.B.detach().numpy().ravel().ravel()],\n",
    "    marker=\"x\", markersize=10, label=\"Unknown generator\", color=\"C0\", linestyle=\"None\"\n",
    ")\n",
    "\n",
    "# Show\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"A\", fontsize=20)\n",
    "ax.set_ylabel(\"B\", fontsize=20)\n",
    "ax.set_title(\"Parameters estimation with regard to\\nthe parallelogram (unidimensional case)\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1be40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "GUM",
   "language": "python",
   "name": "gum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
